{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vedpk\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\vedpk\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\vedpk\\Anaconda3\\lib\\site-packages\\sklearn\\learning_curve.py:22: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\vedpk\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import glob, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "import datetime as dt\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, PReLU, BatchNormalization\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/train_proc5.csv\")\n",
    "test = pd.read_csv(\"../input/test_proc5.csv\")\n",
    "submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "air_store = pd.read_csv(\"../input/allstore_info_proc1.csv\").rename(columns={'store_id':'air_store_id'})\n",
    "date_info = pd.read_csv(\"../input/date_info.csv\").rename(columns={\"calendar_date\" : 'visit_date'})\n",
    "train_weather = pd.read_csv(\"../input/train_weather_01.csv\")\n",
    "test_weather = pd.read_csv(\"../input/test_weather_01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['visit_date'] = submission['id'].map(lambda x: str(x).split('_')[2])\n",
    "submission['air_store_id'] = submission['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "submission['visit_date'] = pd.to_datetime(submission['visit_date'])\n",
    "submission['visitors'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weather = pd.concat([train_weather, test_weather])\n",
    "all_weather['visit_date'] = pd.to_datetime(all_weather['visit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['visit_date'] = pd.to_datetime(train['visit_date'])\n",
    "test['visit_date'] = pd.to_datetime(test['visit_date'])\n",
    "#test['visitors'] = -1\n",
    "test['visitors'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_info['visit_date'] = pd.to_datetime(date_info['visit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_info['date_seq_id'] = date_info['visit_date'].dt.date - date_info['visit_date'].dt.date.min() + dt.timedelta(1)\n",
    "date_info['date_seq_id'] = date_info['date_seq_id']/dt.timedelta(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_info['week_seq_id'] = ((date_info['date_seq_id']+4)/7).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_to_drop = list(set(train['air_store_id']) - set(test['air_store_id']))\n",
    "train = train.where(~(train['air_store_id'].isin(stores_to_drop)))\n",
    "train = train.dropna(axis=0,subset=['air_store_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([train,test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train; del test;\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.merge(all_data,air_store,how='left',on='air_store_id',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.merge(all_data,date_info,how='left',on='visit_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['visit_month'] = all_data['visit_date'].dt.month\n",
    "all_data['visit_year'] = all_data['visit_date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['diff_max_lat'] = all_data['latitude'].max() - all_data['latitude']\n",
    "all_data['diff_min_lat'] = all_data['latitude'].min() - all_data['latitude']\n",
    "all_data['diff_max_long'] = all_data['longitude'].max() - all_data['longitude']\n",
    "all_data['diff_min_long'] = all_data['longitude'].min() - all_data['longitude']\n",
    "all_data['lat_plus_long'] = all_data['latitude'] + all_data['longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_to_drop = ['air_b2d8bc9c88b85f96',\n",
    " 'air_cf22e368c1a71d53',\n",
    " 'air_229d7e508d9f1b5e',\n",
    " 'air_d0a7bd3339c3d12a',\n",
    " 'air_cb083b4789a8d3a2',\n",
    " 'air_2703dcb33192b181',\n",
    " 'air_0ead98dd07e7a82a',\n",
    " 'air_d63cfa6d6ab78446']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.where(~(all_data['air_store_id'].isin(stores_to_drop)))\n",
    "all_data = all_data.dropna(axis=0,subset=['air_store_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.merge(all_data,all_weather, on=['air_store_id','visit_date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(282487, 73)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del air_store;del date_info;gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['visit_date'] = pd.to_datetime(all_data['visit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-01-01 00:00:00\n",
      "2017-05-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "#train.sort_values('visit_date').iloc[-1]['visit_date']-dt.timedelta(days=90) #38days validation set\n",
    "print(all_data['visit_date'].min())\n",
    "print(all_data['visit_date'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_data = all_data.sort_values(['air_store_id','visit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = all_data[:train_len]\n",
    "test = all_data[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250468, 73)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reset_index().drop('index',axis=1)\n",
    "test = test.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32019, 73)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2017, 5, 31)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.date(2017, 4, 22) + dt.timedelta(days=39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Asian', 'Bar/Cocktail', 'Cafe/Sweets', 'Creative cuisine',\n",
      "       'Dining bar', 'International cuisine', 'Italian/French', 'Izakaya',\n",
      "       'Japanese food', 'Karaoke/Party', 'Okonomiyaki/Monja/Teppanyaki',\n",
      "       'Other', 'Western food', 'Yakiniku/Korean food'],\n",
      "      dtype='object') \n",
      "\n",
      "\n",
      "Index(['Fukuoka-ken Fukuoka-shi Daimyō',\n",
      "       'Fukuoka-ken Fukuoka-shi Hakata Ekimae',\n",
      "       'Fukuoka-ken Fukuoka-shi Imaizumi', 'Fukuoka-ken Fukuoka-shi Momochi',\n",
      "       'Fukuoka-ken Fukuoka-shi Shiobaru', 'Fukuoka-ken Fukuoka-shi Takatori',\n",
      "       'Fukuoka-ken Fukuoka-shi Tenjin', 'Fukuoka-ken Fukuoka-shi Torikai',\n",
      "       'Fukuoka-ken Itoshima-shi Maebarunishi',\n",
      "       'Fukuoka-ken Kitakyūshū-shi Konyamachi',\n",
      "       ...\n",
      "       'Ōsaka-fu Sakai-shi Minamikawaramachi', 'Ōsaka-fu Suita-shi Izumichō',\n",
      "       'Ōsaka-fu Ōsaka-shi Fuminosato', 'Ōsaka-fu Ōsaka-shi Kyōmachibori',\n",
      "       'Ōsaka-fu Ōsaka-shi Kyūtarōmachi', 'Ōsaka-fu Ōsaka-shi Nakanochō',\n",
      "       'Ōsaka-fu Ōsaka-shi Nanbasennichimae', 'Ōsaka-fu Ōsaka-shi Shinmachi',\n",
      "       'Ōsaka-fu Ōsaka-shi Ōgimachi', 'Ōsaka-fu Ōsaka-shi Ōhiraki'],\n",
      "      dtype='object', length=103) \n",
      "\n",
      "\n",
      "Index(['Fukuoka-ken', 'Hiroshima-ken', 'Hokkaidō', 'Hyōgo-ken', 'Miyagi-ken',\n",
      "       'Niigata-ken', 'Shizuoka-ken', 'Tōkyō-to', 'Ōsaka-fu'],\n",
      "      dtype='object') \n",
      "\n",
      "\n",
      "Index(['Abashiri-shi', 'Adachi-ku', 'Amagasaki-shi', 'Asahikawa-shi',\n",
      "       'Bunkyō-ku', 'Chiyoda-ku', 'Chūō-ku', 'Edogawa-ku', 'Fuchū-shi',\n",
      "       'Fukuoka-shi', 'Fukuyama-shi', 'Hamamatsu-shi', 'Higashiōsaka-shi',\n",
      "       'Himeji-shi', 'Hiroshima-shi', 'Itabashi-ku', 'Itoshima-shi',\n",
      "       'Kakogawa-shi', 'Kashiwazaki-shi', 'Katsushika-ku', 'Katō-gun',\n",
      "       'Kita-ku', 'Kitakyūshū-shi', 'Koganei-shi', 'Kurume-shi', 'Kōbe-shi',\n",
      "       'Kōtō-ku', 'Machida-shi', 'Meguro-ku', 'Minato-ku', 'Musashino-shi',\n",
      "       'Nagaoka-shi', 'Nakano-ku', 'Nerima-ku', 'Neyagawa-shi', 'Niigata-shi',\n",
      "       'Nishinomiya-shi', 'Numazu-shi', 'Sakai-shi', 'Sapporo-shi',\n",
      "       'Sendai-shi', 'Setagaya-ku', 'Shibuya-ku', 'Shinagawa-ku',\n",
      "       'Shinjuku-ku', 'Shizuoka-shi', 'Suginami-ku', 'Suita-shi',\n",
      "       'Tachikawa-shi', 'Taitō-ku', 'Takarazuka-shi', 'Toshima-ku', 'Yame-shi',\n",
      "       'Ōsaka-shi', 'Ōta-ku'],\n",
      "      dtype='object') \n",
      "\n",
      "\n",
      "Index(['Friday', 'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday',\n",
      "       'Wednesday'],\n",
      "      dtype='object') \n",
      "\n",
      "\n",
      "Index(['fukuoka__fukuoka-kana__fukuoka', 'fukuoka__hakata-kana__hakata',\n",
      "       'fukuoka__kuroki-kana__kuroigi', 'fukuoka__kurume-kana__kurume',\n",
      "       'fukuoka__maehara-kana__maebaru', 'fukuoka__yawata-kana__yahata',\n",
      "       'hiroshima__fukuyama-kana__fukuyama',\n",
      "       'hiroshima__hiroshima-kana__hiroshima',\n",
      "       'hokkaido_ishikari__sapporo-katakana__satporo',\n",
      "       'hokkaido_kamikawa__asahikawa-kana__asahikawa',\n",
      "       'hokkaido_okhotsk__abashiri-kana__abashiri',\n",
      "       'hokkaido_tokachi__komaba-kana__NONE', 'hyogo__akashi-kana__akashi',\n",
      "       'hyogo__himeji-kana__gimpe', 'hyogo__kobe-kana__koube',\n",
      "       'hyogo__nishinomiya-kana__nishinomiya',\n",
      "       'kanagawa__hiyoshi-kana__hiyoshi',\n",
      "       'kanagawa__sagamihara-central-kana__sagamiharasuki',\n",
      "       'miyagi__sendai-kana__sendai', 'niigata__kashiwazaki-kana__kashiwazaki',\n",
      "       'niigata__nagaoka-kana__nagaoka', 'niigata__niigata-kana__niigata',\n",
      "       'osaka__hirakata-kana__hirakata', 'osaka__ikomayama-kana__ikomayama',\n",
      "       'osaka__osaka-kana__osaka', 'osaka__sakai-kana__sakai',\n",
      "       'osaka__toyonaka-kana__toyonaka',\n",
      "       'saitama__tokorozawa-kana__tokorozawa',\n",
      "       'shizuoka__hamamatsu-kana__hamamatsu',\n",
      "       'shizuoka__mishima-kana__mishima', 'shizuoka__shizuoka-kana__shizuoka',\n",
      "       'tokyo__edogawa-seaside-kana__edgawawinkai', 'tokyo__fuchu-kana__fuku',\n",
      "       'tokyo__haneda-kana__haneda', 'tokyo__nerima-kana__nerima',\n",
      "       'tokyo__setagaya-kana__setagaya', 'tokyo__tokyo-kana__tonokyo',\n",
      "       'yamaguchi__shimonoseki-kana__shimonoseki'],\n",
      "      dtype='object') \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols_to_encode = ([i for i,j in zip(all_data.dtypes.index,all_data.dtypes.values) if j == 'object'])\n",
    "cols_to_encode.remove('air_store_id')\n",
    "cols_to_encode.remove('visit_date')\n",
    "#lbl = preprocessing.LabelEncoder()\n",
    "for i in cols_to_encode:\n",
    "    #all_data[i] = lbl.fit_transform(all_data[i])\n",
    "    print(pd.get_dummies(all_data[i]).columns, \"\\n\\n\")\n",
    "    all_data = pd.concat([all_data, pd.get_dummies(all_data[i])],axis=1)\n",
    "    if i not in [\"day_of_week\", 'genre_name']:\n",
    "        all_data.drop(i, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['genre_name', 'area_name', 'prefecture', 'city', 'day_of_week', 'station_id']\n"
     ]
    }
   ],
   "source": [
    "print(cols_to_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_shifted_ewm(series, alpha, adjust=True, days=0): #\n",
    "    return series.shift(periods=days).ewm(alpha=alpha, adjust=adjust).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = (train.groupby(['air_store_id', 'day_of_week'])\n",
    "                  .apply(lambda g: calc_shifted_ewm(g['visitors'], 0.1, days=39))\n",
    "                  .reset_index().set_index('level_2').sort_index()\n",
    "                  .rename(columns={'visitors' : 'ewm'})\n",
    "                  .drop(['air_store_id','day_of_week'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.concat([train, tmp], axis=1)[['air_store_id', 'visit_date', 'ewm']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_store_id</th>\n",
       "      <th>visit_date</th>\n",
       "      <th>ewm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2016-07-02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2016-07-04</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2016-07-05</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2016-07-06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           air_store_id visit_date  ewm\n",
       "0  air_00a91d42b08b08d9 2016-07-01  NaN\n",
       "1  air_00a91d42b08b08d9 2016-07-02  NaN\n",
       "2  air_00a91d42b08b08d9 2016-07-04  NaN\n",
       "3  air_00a91d42b08b08d9 2016-07-05  NaN\n",
       "4  air_00a91d42b08b08d9 2016-07-06  NaN"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['visit_date'] = pd.to_datetime(all_data['visit_date'])\n",
    "tmp['visit_date'] = pd.to_datetime(tmp['visit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_data['visit_date'] = all_data['visit_date'].dt.date\n",
    "#tmp['visit_date'] = tmp['visit_date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reset_index().drop('index',axis=1)\n",
    "test = test.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train min: 2016-01-01 00:00:00\n",
      "Train max:2017-04-22 00:00:00\n",
      "Test min: 2017-04-23 00:00:00\n",
      "Test max:2017-05-31 00:00:00\n",
      "tmp min: 2016-01-01 00:00:00\n",
      "tmp max:2017-04-22 00:00:00\n",
      "Difference: 39 days 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print(\"Train min: \" + str(train['visit_date'].min()))\n",
    "print(\"Train max:\" + str(train['visit_date'].max()))\n",
    "print(\"Test min: \" + str(test['visit_date'].min()))\n",
    "print(\"Test max:\" + str(test['visit_date'].max()))\n",
    "print(\"tmp min: \" + str(tmp['visit_date'].min()))\n",
    "print(\"tmp max:\" + str(tmp['visit_date'].max()))\n",
    "print(\"Difference: \" + str(test['visit_date'].max() - train['visit_date'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp['visit_date'] += dt.timedelta(days=39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 282487 entries, 0 to 282486\n",
      "Columns: 295 entries, air_store_id to yamaguchi__shimonoseki-kana__shimonoseki\n",
      "dtypes: datetime64[ns](1), float64(53), int64(12), object(3), uint8(226)\n",
      "memory usage: 211.7+ MB\n"
     ]
    }
   ],
   "source": [
    "all_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250468, 3)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_data = pd.merge(all_data, tmp, on=['air_store_id', 'visit_date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = all_data[:train_len]\n",
    "test = all_data[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reset_index().drop('index',axis=1)\n",
    "test = test.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['visit_date'] = pd.to_datetime(all_data['visit_date'])\n",
    "#all_data['visit_date'] = all_data['visit_date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_data; gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diff_8_7                 13\n",
       "diff_9_8                  6\n",
       "eight_weeks_ago        4377\n",
       "eleven_weeks_ago       4589\n",
       "nine_weeks_ago         4439\n",
       "priorMax               1632\n",
       "priorMean              1632\n",
       "priorMin               1632\n",
       "seven_weeks_ago        4395\n",
       "six_weeks_ago          4349\n",
       "ten_weeks_ago          4545\n",
       "visitors              32019\n",
       "vmax_6weekago_7          13\n",
       "vmax_6weekago_8           6\n",
       "vmean_6weekago_7         13\n",
       "vmean_6weekago_8          6\n",
       "vmedian_6weekago_7       13\n",
       "vmedian_6weekago_8        6\n",
       "vmin_6weekago_7          13\n",
       "vmin_6weekago_8           6\n",
       "avg_temperature        3276\n",
       "high_temperature       3276\n",
       "low_temperature        3276\n",
       "precipitation         12730\n",
       "hours_sunlight         4641\n",
       "dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().sum()[test.isnull().sum()>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['air_store_id', 'diff_10_9', 'diff_11_10', 'diff_12_11', 'diff_8_7',\n",
       "       'diff_9_8', 'eight_weeks_ago', 'eleven_weeks_ago', 'nine_weeks_ago',\n",
       "       'priorMax',\n",
       "       ...\n",
       "       'shizuoka__hamamatsu-kana__hamamatsu',\n",
       "       'shizuoka__mishima-kana__mishima', 'shizuoka__shizuoka-kana__shizuoka',\n",
       "       'tokyo__edogawa-seaside-kana__edgawawinkai', 'tokyo__fuchu-kana__fuku',\n",
       "       'tokyo__haneda-kana__haneda', 'tokyo__nerima-kana__nerima',\n",
       "       'tokyo__setagaya-kana__setagaya', 'tokyo__tokyo-kana__tonokyo',\n",
       "       'yamaguchi__shimonoseki-kana__shimonoseki'],\n",
       "      dtype='object', length=295)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['air_store_id', 'diff_10_9', 'diff_11_10', 'diff_12_11', 'diff_8_7',\n",
       "       'diff_9_8', 'eight_weeks_ago', 'eleven_weeks_ago', 'nine_weeks_ago',\n",
       "       'priorMax',\n",
       "       ...\n",
       "       'shizuoka__hamamatsu-kana__hamamatsu',\n",
       "       'shizuoka__mishima-kana__mishima', 'shizuoka__shizuoka-kana__shizuoka',\n",
       "       'tokyo__edogawa-seaside-kana__edgawawinkai', 'tokyo__fuchu-kana__fuku',\n",
       "       'tokyo__haneda-kana__haneda', 'tokyo__nerima-kana__nerima',\n",
       "       'tokyo__setagaya-kana__setagaya', 'tokyo__tokyo-kana__tonokyo',\n",
       "       'yamaguchi__shimonoseki-kana__shimonoseki'],\n",
       "      dtype='object', length=295)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = train.fillna(0) ##0.5171185851031399\n",
    "#test = test.fillna(0)\n",
    "train = train.fillna(np.nan) ##0.5185937979429444\n",
    "test = test.fillna(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load prepareData.py\n",
    "def prepareData(trainIn, valIn, testIn = None):\n",
    "    stat1 = trainIn.groupby([\"air_store_id\",\"day_of_week\"],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_store_day'})\n",
    "    stat2 = trainIn.groupby([\"air_store_id\",\"day_of_week\"],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_store_day'})\n",
    "        \n",
    "    stat3 = trainIn.groupby([\"area_id\",\"day_of_week\"],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_area'})\n",
    "    stat4 = trainIn.groupby([\"area_id\",\"day_of_week\"],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_area'})\n",
    "\n",
    "    stat5 = trainIn.groupby([\"n200mt_cluster_id\",\"day_of_week\"],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_200mt'})\n",
    "    stat6 = trainIn.groupby([\"n200mt_cluster_id\",\"day_of_week\"],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_200mt'})\n",
    "\n",
    "    stat7 = trainIn.groupby([\"n400mt_cluster_id\",\"day_of_week\"],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_400mt'})\n",
    "    stat8 = trainIn.groupby([\"n400mt_cluster_id\",\"day_of_week\"],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_400mt'})\n",
    "    \n",
    "    stat9 = trainIn.groupby([\"n1000mt_cluster_id\",\"day_of_week\"],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_1000mt'})\n",
    "    stat10 = trainIn.groupby([\"n1000mt_cluster_id\",\"day_of_week\"],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_1000mt'})\n",
    "    \n",
    "    stat11 = trainIn.groupby([\"n200mt_cluster_id\",\"genre_name\"],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_genre_200mt'})\n",
    "    stat12 = trainIn.groupby([\"n200mt_cluster_id\",\"genre_name\"],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_genre_200mt'})\n",
    "    \n",
    "    stat13 = trainIn.groupby([\"n400mt_cluster_id\",\"genre_name\"],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_genre_400mt'})\n",
    "    stat14 = trainIn.groupby([\"n400mt_cluster_id\",\"genre_name\"],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_genre_400mt'})\n",
    "    \n",
    "    stat15 = trainIn.groupby([\"n1000mt_cluster_id\",\"genre_name\"],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_genre_1000mt'})\n",
    "    stat16 = trainIn.groupby([\"n1000mt_cluster_id\",\"genre_name\"],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_genre_1000mt'})\n",
    "    \n",
    "    #####################################\n",
    "    stat17 = trainIn.groupby([\"air_store_id\",\"day_of_week\",'week_seq_id'],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_store_day'})\n",
    "    #stat18 = trainIn.groupby([\"air_store_id\",\"day_of_week\",'week_seq_id'],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_store_day'})\n",
    "    #stat17 = pd.merge(stat17, stat18, on=['air_store_id','day_of_week','week_seq_id'])\n",
    "    #del stat18; gc.collect()\n",
    "    \n",
    "    stat_6wago_day = stat17.copy()\n",
    "    stat_6wago_day.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_6wago_day.loc[:,'week_seq_id'] = stat17.loc[:,'week_seq_id'] + 6\n",
    "    stat_6wago_day = stat_6wago_day.rename(columns={'visitors_sum_store_day' : 'visitors_6wks_wago'})\n",
    "    \n",
    "    stat_7wago_day = stat17.copy()\n",
    "    stat_7wago_day.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_7wago_day.loc[:,'week_seq_id'] = stat17.loc[:,'week_seq_id'] + 7\n",
    "    stat_7wago_day = stat_7wago_day.rename(columns={'visitors_sum_store_day' : 'visitors_7wks_wago'})\n",
    "   \n",
    "    stat_8wago_day = stat17.copy()\n",
    "    stat_8wago_day.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_8wago_day.loc[:,'week_seq_id'] = stat17.loc[:,'week_seq_id'] + 8\n",
    "    stat_8wago_day = stat_8wago_day.rename(columns={'visitors_sum_store_day' : 'visitors_8wks_wago'})\n",
    "\n",
    "    stat_9wago_day = stat17.copy()\n",
    "    stat_9wago_day.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_9wago_day.loc[:,'week_seq_id'] = stat17.loc[:,'week_seq_id'] + 9\n",
    "    stat_9wago_day = stat_9wago_day.rename(columns={'visitors_sum_store_day' : 'visitors_9wks_wago'})\n",
    "    \n",
    "    stat_10wago_day = stat17.copy()\n",
    "    stat_10wago_day.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_10wago_day.loc[:,'week_seq_id'] = stat17.loc[:,'week_seq_id'] + 10\n",
    "    stat_10wago_day = stat_10wago_day.rename(columns={'visitors_sum_store_day' : 'visitors_10wks_wago'})\n",
    "    \n",
    "    #################################\n",
    "    stat18 = trainIn.groupby([\"air_store_id\",\"week_seq_id\"],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_store_week'})\n",
    "    stat19 = trainIn.groupby([\"air_store_id\",\"week_seq_id\"],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_store_week'})                  \n",
    "    stat18 = pd.merge(stat18, stat19, on=['air_store_id','week_seq_id'])\n",
    "    del stat19;\n",
    "    \n",
    "    stat_6wago = stat18.copy()\n",
    "    stat_6wago.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_6wago.loc[:,'week_seq_id'] = stat18.loc[:,'week_seq_id'] + 6\n",
    "    stat_6wago = stat_6wago.rename(columns={'visitors_sum_store_week' : 'visitors_sum_store_6wago'})\n",
    "    stat_6wago = stat_6wago.rename(columns={'visitors_mean_store_week' : 'visitors_mean_store_6wago'})\n",
    "    \n",
    "    stat_7wago = stat18.copy()\n",
    "    stat_7wago.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_7wago.loc[:,'week_seq_id'] = stat18.loc[:,'week_seq_id'] + 7\n",
    "    stat_7wago = stat_7wago.rename(columns={'visitors_sum_store_week' : 'visitors_sum_store_7wago'})\n",
    "    stat_7wago = stat_7wago.rename(columns={'visitors_mean_store_week' : 'visitors_mean_store_8wago'})\n",
    "    \n",
    "    stat_8wago = stat18.copy()\n",
    "    stat_8wago.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_8wago.loc[:,'week_seq_id'] = stat18.loc[:,'week_seq_id'] + 8\n",
    "    stat_8wago = stat_8wago.rename(columns={'visitors_sum_store_week' : 'visitors_sum_store_day_6wago'})\n",
    "    stat_8wago = stat_8wago.rename(columns={'visitors_mean_store_week' : 'visitors_mean_store_day_6wago'}) \n",
    "    \n",
    "    stat_9wago = stat18.copy()\n",
    "    stat_9wago.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_9wago.loc[:,'week_seq_id'] = stat18.loc[:,'week_seq_id'] + 9\n",
    "    stat_9wago = stat_9wago.rename(columns={'visitors_sum_store_week' : 'visitors_sum_store_9wago'})\n",
    "    stat_9wago = stat_9wago.rename(columns={'visitors_mean_store_week' : 'visitors_mean_store_9wago'})\n",
    "    \n",
    "    stat_10wago = stat18.copy()\n",
    "    stat_10wago.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_10wago.loc[:,'week_seq_id'] = stat18.loc[:,'week_seq_id'] + 10\n",
    "    stat_10wago = stat_10wago.rename(columns={'visitors_sum_store_week' : 'visitors_sum_store_10wago'})\n",
    "    stat_10wago = stat_10wago.rename(columns={'visitors_mean_store_week' : 'visitors_mean_store_10wago'}) \n",
    "    \n",
    "    ##############################\n",
    "    trainIn = pd.merge(trainIn, stat1, on = [\"air_store_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat2, on = [\"air_store_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat3, on = [\"area_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat4, on = [\"area_id\", \"day_of_week\"], how='left')\n",
    "    \n",
    "    trainIn = pd.merge(trainIn, stat_6wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_7wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_8wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_9wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_10wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    \n",
    "    trainIn = pd.merge(trainIn, stat_6wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_7wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_8wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_9wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_10wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    \n",
    "    trainIn = pd.merge(trainIn, stat5, on = [\"n200mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat6, on = [\"n200mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat7, on = [\"n400mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat8, on = [\"n400mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat9, on = [\"n1000mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat10, on = [\"n1000mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    \n",
    "    trainIn = pd.merge(trainIn, stat11, on = [\"n200mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat12, on = [\"n200mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat13, on = [\"n400mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat14, on = [\"n400mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat15, on = [\"n1000mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat16, on = [\"n1000mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    \n",
    "    ############################\n",
    "    testIn = pd.merge(testIn, stat1, on = [\"air_store_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat2, on = [\"air_store_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat3, on = [\"area_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat4, on = [\"area_id\", \"day_of_week\"], how='left')\n",
    "\n",
    "    testIn = pd.merge(testIn, stat_6wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_7wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_8wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_9wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_10wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "\n",
    "    testIn = pd.merge(testIn, stat_6wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_7wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_8wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_9wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_10wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "\n",
    "    testIn = pd.merge(testIn, stat5, on = [\"n200mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat6, on = [\"n200mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat7, on = [\"n400mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat8, on = [\"n400mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat9, on = [\"n1000mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat10, on = [\"n1000mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "\n",
    "    testIn = pd.merge(testIn, stat11, on = [\"n200mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat12, on = [\"n200mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat13, on = [\"n400mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat14, on = [\"n400mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat15, on = [\"n1000mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat16, on = [\"n1000mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    \n",
    "    ##################################\n",
    "    valIn = pd.merge(valIn, stat1, on = [\"air_store_id\", \"day_of_week\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat2, on = [\"air_store_id\", \"day_of_week\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat3, on = [\"area_id\", \"day_of_week\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat4, on = [\"area_id\", \"day_of_week\"], how='left')\n",
    "    \n",
    "    valIn = pd.merge(valIn, stat_6wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat_7wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat_8wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat_9wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat_10wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "\n",
    "    valIn = pd.merge(valIn, stat_6wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat_7wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat_8wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat_9wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat_10wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    \n",
    "    valIn = pd.merge(valIn, stat5, on = [\"n200mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat6, on = [\"n200mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat7, on = [\"n400mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat8, on = [\"n400mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat9, on = [\"n1000mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat10, on = [\"n1000mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    \n",
    "    valIn = pd.merge(valIn, stat11, on = [\"n200mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat12, on = [\"n200mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat13, on = [\"n400mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat14, on = [\"n400mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat15, on = [\"n1000mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat16, on = [\"n1000mt_cluster_id\", \"genre_name\"], how='left')\n",
    " \n",
    "    return (trainIn, valIn, testIn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [c for c in train if c not in ['id', 'air_store_id', 'visit_date','visitors','visit_date', 'day_of_week', 'genre_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.sort_values(['air_store_id', 'visit_date']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = X[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.log1p(X['visitors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = test[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "K = 5\n",
    "kf = model_selection.KFold(n_splits = K, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSLE(y, pred):\n",
    "    return metrics.mean_squared_error(np.log1p(y), np.log1p(pred))**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = 0\n",
    "y_train_pred = np.zeros(len(X))\n",
    "#y_train_pred = X['visitors'].copy\n",
    "y_train_pred\n",
    "bestIters = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250468, 290)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250468,)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 250468 entries, 0 to 250467\n",
      "Columns: 295 entries, air_store_id to yamaguchi__shimonoseki-kana__shimonoseki\n",
      "dtypes: datetime64[ns](1), float64(53), int64(12), object(3), uint8(226)\n",
      "memory usage: 187.7+ MB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold  0\n",
      "Train on 200374 samples, validate on 50094 samples\n",
      "Epoch 1/50\n",
      "200192/200374 [============================>.] - ETA: 0s - loss: 0.4794 - mean_squared_error: 0.4794\n",
      "Epoch 00001: val_loss improved from inf to 0.28557, saving model to weights.hdf5\n",
      "200374/200374 [==============================] - 31s 153us/step - loss: 0.4793 - mean_squared_error: 0.4793 - val_loss: 0.2856 - val_mean_squared_error: 0.2856\n",
      "Epoch 2/50\n",
      "200192/200374 [============================>.] - ETA: 0s - loss: 0.3319 - mean_squared_error: 0.3319\n",
      "Epoch 00002: val_loss improved from 0.28557 to 0.28316, saving model to weights.hdf5\n",
      "200374/200374 [==============================] - 29s 144us/step - loss: 0.3319 - mean_squared_error: 0.3319 - val_loss: 0.2832 - val_mean_squared_error: 0.2832\n",
      "Epoch 3/50\n",
      "199936/200374 [============================>.] - ETA: 0s - loss: 0.3283 - mean_squared_error: 0.3283\n",
      "Epoch 00003: val_loss did not improve\n",
      "200374/200374 [==============================] - 29s 143us/step - loss: 0.3282 - mean_squared_error: 0.3282 - val_loss: 0.2895 - val_mean_squared_error: 0.2895\n",
      "Epoch 4/50\n",
      "199936/200374 [============================>.] - ETA: 0s - loss: 0.3250 - mean_squared_error: 0.3250\n",
      "Epoch 00004: val_loss improved from 0.28316 to 0.27804, saving model to weights.hdf5\n",
      "200374/200374 [==============================] - 29s 144us/step - loss: 0.3251 - mean_squared_error: 0.3251 - val_loss: 0.2780 - val_mean_squared_error: 0.2780\n",
      "Epoch 5/50\n",
      "199936/200374 [============================>.] - ETA: 0s - loss: 0.3215 - mean_squared_error: 0.3215\n",
      "Epoch 00005: val_loss improved from 0.27804 to 0.27512, saving model to weights.hdf5\n",
      "200374/200374 [==============================] - 28s 138us/step - loss: 0.3215 - mean_squared_error: 0.3215 - val_loss: 0.2751 - val_mean_squared_error: 0.2751\n",
      "Epoch 6/50\n",
      "199936/200374 [============================>.] - ETA: 0s - loss: 0.3211 - mean_squared_error: 0.3211\n",
      "Epoch 00006: val_loss improved from 0.27512 to 0.27308, saving model to weights.hdf5\n",
      "200374/200374 [==============================] - 28s 142us/step - loss: 0.3211 - mean_squared_error: 0.3211 - val_loss: 0.2731 - val_mean_squared_error: 0.2731\n",
      "Epoch 7/50\n",
      "199936/200374 [============================>.] - ETA: 0s - loss: 0.3166 - mean_squared_error: 0.3166 ETA: \n",
      "Epoch 00007: val_loss did not improve\n",
      "200374/200374 [==============================] - 29s 143us/step - loss: 0.3166 - mean_squared_error: 0.3166 - val_loss: 0.2794 - val_mean_squared_error: 0.2794\n",
      "Epoch 8/50\n",
      "199936/200374 [============================>.] - ETA: 0s - loss: 0.3164 - mean_squared_error: 0.3164\n",
      "Epoch 00008: val_loss did not improve\n",
      "200374/200374 [==============================] - 28s 141us/step - loss: 0.3164 - mean_squared_error: 0.3164 - val_loss: 0.2822 - val_mean_squared_error: 0.2822\n",
      "Epoch 00008: early stopping\n",
      "RMSLE Keras Regressor, validation set, fold  0 :  0.5311813467697177\n",
      "\n",
      "Fold  1\n",
      "Train on 200374 samples, validate on 50094 samples\n",
      "Epoch 1/50\n",
      "200192/200374 [============================>.] - ETA: 0s - loss: 0.4734 - mean_squared_error: 0.4734\n",
      "Epoch 00001: val_loss improved from inf to 0.29950, saving model to weights.hdf5\n",
      "200374/200374 [==============================] - 33s 164us/step - loss: 0.4733 - mean_squared_error: 0.4733 - val_loss: 0.2995 - val_mean_squared_error: 0.2995\n",
      "Epoch 2/50\n",
      "200192/200374 [============================>.] - ETA: 0s - loss: 0.3301 - mean_squared_error: 0.3301\n",
      "Epoch 00002: val_loss improved from 0.29950 to 0.29210, saving model to weights.hdf5\n",
      "200374/200374 [==============================] - 29s 146us/step - loss: 0.3301 - mean_squared_error: 0.3301 - val_loss: 0.2921 - val_mean_squared_error: 0.2921\n",
      "Epoch 3/50\n",
      "199936/200374 [============================>.] - ETA: 0s - loss: 0.3254 - mean_squared_error: 0.3254\n",
      "Epoch 00003: val_loss did not improve\n",
      "200374/200374 [==============================] - 30s 151us/step - loss: 0.3254 - mean_squared_error: 0.3254 - val_loss: 0.2949 - val_mean_squared_error: 0.2949\n",
      "Epoch 4/50\n",
      "200192/200374 [============================>.] - ETA: 0s - loss: 0.3242 - mean_squared_error: 0.3242\n",
      "Epoch 00004: val_loss improved from 0.29210 to 0.28507, saving model to weights.hdf5\n",
      "200374/200374 [==============================] - 29s 146us/step - loss: 0.3242 - mean_squared_error: 0.3242 - val_loss: 0.2851 - val_mean_squared_error: 0.2851\n",
      "Epoch 5/50\n",
      "199936/200374 [============================>.] - ETA: 0s - loss: 0.3198 - mean_squared_error: 0.3198\n",
      "Epoch 00005: val_loss did not improve\n",
      "200374/200374 [==============================] - 28s 142us/step - loss: 0.3198 - mean_squared_error: 0.3198 - val_loss: 0.3071 - val_mean_squared_error: 0.3071\n",
      "Epoch 6/50\n",
      "199936/200374 [============================>.] - ETA: 0s - loss: 0.3187 - mean_squared_error: 0.3187\n",
      "Epoch 00006: val_loss improved from 0.28507 to 0.27885, saving model to weights.hdf5\n",
      "200374/200374 [==============================] - 28s 142us/step - loss: 0.3187 - mean_squared_error: 0.3187 - val_loss: 0.2789 - val_mean_squared_error: 0.2789\n",
      "Epoch 7/50\n",
      "199936/200374 [============================>.] - ETA: 0s - loss: 0.3175 - mean_squared_error: 0.3175\n",
      "Epoch 00007: val_loss improved from 0.27885 to 0.27827, saving model to weights.hdf5\n",
      "200374/200374 [==============================] - 28s 142us/step - loss: 0.3174 - mean_squared_error: 0.3174 - val_loss: 0.2783 - val_mean_squared_error: 0.2783\n",
      "Epoch 8/50\n",
      "199936/200374 [============================>.] - ETA: 0s - loss: 0.3170 - mean_squared_error: 0.3170\n",
      "Epoch 00008: val_loss did not improve\n",
      "200374/200374 [==============================] - 28s 140us/step - loss: 0.3170 - mean_squared_error: 0.3170 - val_loss: 0.2784 - val_mean_squared_error: 0.2784\n",
      "Epoch 9/50\n",
      "200192/200374 [============================>.] - ETA: 0s - loss: 0.3128 - mean_squared_error: 0.3128\n",
      "Epoch 00009: val_loss improved from 0.27827 to 0.27670, saving model to weights.hdf5\n",
      "200374/200374 [==============================] - 29s 145us/step - loss: 0.3128 - mean_squared_error: 0.3128 - val_loss: 0.2767 - val_mean_squared_error: 0.2767\n",
      "Epoch 10/50\n",
      "199936/200374 [============================>.] - ETA: 0s - loss: 0.3126 - mean_squared_error: 0.3126\n",
      "Epoch 00010: val_loss did not improve\n",
      "200374/200374 [==============================] - 28s 142us/step - loss: 0.3126 - mean_squared_error: 0.3126 - val_loss: 0.2771 - val_mean_squared_error: 0.2771\n",
      "Epoch 11/50\n",
      "199936/200374 [============================>.] - ETA: 0s - loss: 0.3098 - mean_squared_error: 0.3098\n",
      "Epoch 00011: val_loss improved from 0.27670 to 0.27518, saving model to weights.hdf5\n",
      "200374/200374 [==============================] - 28s 142us/step - loss: 0.3098 - mean_squared_error: 0.3098 - val_loss: 0.2752 - val_mean_squared_error: 0.2752\n",
      "Epoch 12/50\n",
      "199936/200374 [============================>.] - ETA: 0s - loss: 0.3089 - mean_squared_error: 0.3089\n",
      "Epoch 00012: val_loss did not improve\n",
      "200374/200374 [==============================] - 29s 143us/step - loss: 0.3089 - mean_squared_error: 0.3089 - val_loss: 0.2934 - val_mean_squared_error: 0.2934\n",
      "Epoch 13/50\n",
      "199936/200374 [============================>.] - ETA: 0s - loss: 0.3067 - mean_squared_error: 0.3067\n",
      "Epoch 00013: val_loss did not improve\n",
      "200374/200374 [==============================] - 28s 142us/step - loss: 0.3067 - mean_squared_error: 0.3067 - val_loss: 0.2755 - val_mean_squared_error: 0.2755\n",
      "Epoch 00013: early stopping\n",
      "RMSLE Keras Regressor, validation set, fold  1 :  0.5248763834077842\n",
      "\n",
      "Fold  2\n",
      "Train on 200374 samples, validate on 50094 samples\n",
      "Epoch 1/50\n",
      "199936/200374 [============================>.] - ETA: 0s - loss: 0.4747 - mean_squared_error: 0.4747\n",
      "Epoch 00001: val_loss improved from inf to 0.29755, saving model to weights.hdf5\n",
      "200374/200374 [==============================] - 31s 154us/step - loss: 0.4744 - mean_squared_error: 0.4744 - val_loss: 0.2976 - val_mean_squared_error: 0.2976\n",
      "Epoch 2/50\n",
      "200192/200374 [============================>.] - ETA: 0s - loss: 0.3295 - mean_squared_error: 0.3295\n",
      "Epoch 00002: val_loss improved from 0.29755 to 0.28778, saving model to weights.hdf5\n",
      "200374/200374 [==============================] - 28s 142us/step - loss: 0.3296 - mean_squared_error: 0.3296 - val_loss: 0.2878 - val_mean_squared_error: 0.2878\n",
      "Epoch 3/50\n",
      "199936/200374 [============================>.] - ETA: 0s - loss: 0.3254 - mean_squared_error: 0.3254\n",
      "Epoch 00003: val_loss improved from 0.28778 to 0.28223, saving model to weights.hdf5\n",
      "200374/200374 [==============================] - 29s 144us/step - loss: 0.3255 - mean_squared_error: 0.3255 - val_loss: 0.2822 - val_mean_squared_error: 0.2822\n",
      "Epoch 4/50\n",
      "199936/200374 [============================>.] - ETA: 0s - loss: 0.3240 - mean_squared_error: 0.3240\n",
      "Epoch 00004: val_loss did not improve\n",
      "200374/200374 [==============================] - 29s 144us/step - loss: 0.3240 - mean_squared_error: 0.3240 - val_loss: 0.2861 - val_mean_squared_error: 0.2861\n",
      "Epoch 5/50\n",
      "199936/200374 [============================>.] - ETA: 0s - loss: 0.3202 - mean_squared_error: 0.3202\n",
      "Epoch 00005: val_loss improved from 0.28223 to 0.27660, saving model to weights.hdf5\n",
      "200374/200374 [==============================] - 29s 145us/step - loss: 0.3201 - mean_squared_error: 0.3201 - val_loss: 0.2766 - val_mean_squared_error: 0.2766\n",
      "Epoch 6/50\n",
      "199936/200374 [============================>.] - ETA: 0s - loss: 0.3192 - mean_squared_error: 0.3192\n",
      "Epoch 00006: val_loss did not improve\n",
      "200374/200374 [==============================] - 29s 144us/step - loss: 0.3191 - mean_squared_error: 0.3191 - val_loss: 0.2771 - val_mean_squared_error: 0.2771\n",
      "Epoch 7/50\n",
      "200192/200374 [============================>.] - ETA: 0s - loss: 0.3167 - mean_squared_error: 0.3167\n",
      "Epoch 00007: val_loss improved from 0.27660 to 0.27228, saving model to weights.hdf5\n",
      "200374/200374 [==============================] - 30s 151us/step - loss: 0.3167 - mean_squared_error: 0.3167 - val_loss: 0.2723 - val_mean_squared_error: 0.2723\n",
      "Epoch 8/50\n",
      "199936/200374 [============================>.] - ETA: 0s - loss: 0.3154 - mean_squared_error: 0.3154\n",
      "Epoch 00008: val_loss did not improve\n",
      "200374/200374 [==============================] - 29s 146us/step - loss: 0.3154 - mean_squared_error: 0.3154 - val_loss: 0.2768 - val_mean_squared_error: 0.2768\n",
      "Epoch 9/50\n",
      "199936/200374 [============================>.] - ETA: 0s - loss: 0.3127 - mean_squared_error: 0.3127\n",
      "Epoch 00009: val_loss did not improve\n",
      "200374/200374 [==============================] - 29s 143us/step - loss: 0.3129 - mean_squared_error: 0.3129 - val_loss: 0.2968 - val_mean_squared_error: 0.2968\n",
      "Epoch 00009: early stopping\n",
      "RMSLE Keras Regressor, validation set, fold  2 :  0.5446063306482066\n",
      "\n",
      "Fold  3\n",
      "Train on 200375 samples, validate on 50093 samples\n",
      "Epoch 1/50\n",
      "200192/200375 [============================>.] - ETA: 0s - loss: 0.4779 - mean_squared_error: 0.4779\n",
      "Epoch 00001: val_loss improved from inf to 0.28684, saving model to weights.hdf5\n",
      "200375/200375 [==============================] - 32s 162us/step - loss: 0.4778 - mean_squared_error: 0.4778 - val_loss: 0.2868 - val_mean_squared_error: 0.2868\n",
      "Epoch 2/50\n",
      "199936/200375 [============================>.] - ETA: 0s - loss: 0.3317 - mean_squared_error: 0.3317\n",
      "Epoch 00002: val_loss improved from 0.28684 to 0.28279, saving model to weights.hdf5\n",
      "200375/200375 [==============================] - 29s 147us/step - loss: 0.3317 - mean_squared_error: 0.3317 - val_loss: 0.2828 - val_mean_squared_error: 0.2828\n",
      "Epoch 3/50\n",
      "199936/200375 [============================>.] - ETA: 0s - loss: 0.3274 - mean_squared_error: 0.3274\n",
      "Epoch 00003: val_loss improved from 0.28279 to 0.27677, saving model to weights.hdf5\n",
      "200375/200375 [==============================] - 30s 148us/step - loss: 0.3275 - mean_squared_error: 0.3275 - val_loss: 0.2768 - val_mean_squared_error: 0.2768\n",
      "Epoch 4/50\n",
      "199936/200375 [============================>.] - ETA: 0s - loss: 0.3249 - mean_squared_error: 0.3249\n",
      "Epoch 00004: val_loss did not improve\n",
      "200375/200375 [==============================] - 32s 161us/step - loss: 0.3249 - mean_squared_error: 0.3249 - val_loss: 0.2935 - val_mean_squared_error: 0.2935\n",
      "Epoch 5/50\n",
      "199936/200375 [============================>.] - ETA: 0s - loss: 0.3233 - mean_squared_error: 0.3233\n",
      "Epoch 00005: val_loss did not improve\n",
      "200375/200375 [==============================] - 29s 145us/step - loss: 0.3233 - mean_squared_error: 0.3233 - val_loss: 0.2852 - val_mean_squared_error: 0.2852\n",
      "Epoch 00005: early stopping\n",
      "RMSLE Keras Regressor, validation set, fold  3 :  0.533997708205543\n",
      "\n",
      "Fold  4\n",
      "Train on 200375 samples, validate on 50093 samples\n",
      "Epoch 1/50\n",
      "200192/200375 [============================>.] - ETA: 0s - loss: 0.4788 - mean_squared_error: 0.4788\n",
      "Epoch 00001: val_loss improved from inf to 0.28513, saving model to weights.hdf5\n",
      "200375/200375 [==============================] - 32s 159us/step - loss: 0.4787 - mean_squared_error: 0.4787 - val_loss: 0.2851 - val_mean_squared_error: 0.2851\n",
      "Epoch 2/50\n",
      "199936/200375 [============================>.] - ETA: 0s - loss: 0.3290 - mean_squared_error: 0.3290\n",
      "Epoch 00002: val_loss did not improve\n",
      "200375/200375 [==============================] - 29s 143us/step - loss: 0.3290 - mean_squared_error: 0.3290 - val_loss: 0.2898 - val_mean_squared_error: 0.2898\n",
      "Epoch 3/50\n",
      "199936/200375 [============================>.] - ETA: 0s - loss: 0.3273 - mean_squared_error: 0.3273\n",
      "Epoch 00003: val_loss improved from 0.28513 to 0.27617, saving model to weights.hdf5\n",
      "200375/200375 [==============================] - 29s 143us/step - loss: 0.3272 - mean_squared_error: 0.3272 - val_loss: 0.2762 - val_mean_squared_error: 0.2762\n",
      "Epoch 4/50\n",
      "199936/200375 [============================>.] - ETA: 0s - loss: 0.3237 - mean_squared_error: 0.3237\n",
      "Epoch 00004: val_loss did not improve\n",
      "200375/200375 [==============================] - 29s 143us/step - loss: 0.3236 - mean_squared_error: 0.3236 - val_loss: 0.2798 - val_mean_squared_error: 0.2798\n",
      "Epoch 5/50\n",
      "200192/200375 [============================>.] - ETA: 0s - loss: 0.3215 - mean_squared_error: 0.3215\n",
      "Epoch 00005: val_loss improved from 0.27617 to 0.27287, saving model to weights.hdf5\n",
      "200375/200375 [==============================] - 29s 147us/step - loss: 0.3215 - mean_squared_error: 0.3215 - val_loss: 0.2729 - val_mean_squared_error: 0.2729\n",
      "Epoch 6/50\n",
      "199936/200375 [============================>.] - ETA: 0s - loss: 0.3185 - mean_squared_error: 0.3185\n",
      "Epoch 00006: val_loss improved from 0.27287 to 0.27123, saving model to weights.hdf5\n",
      "200375/200375 [==============================] - 29s 144us/step - loss: 0.3185 - mean_squared_error: 0.3185 - val_loss: 0.2712 - val_mean_squared_error: 0.2712\n",
      "Epoch 7/50\n",
      "199936/200375 [============================>.] - ETA: 0s - loss: 0.3186 - mean_squared_error: 0.3186\n",
      "Epoch 00007: val_loss did not improve\n",
      "200375/200375 [==============================] - 29s 143us/step - loss: 0.3186 - mean_squared_error: 0.3186 - val_loss: 0.2766 - val_mean_squared_error: 0.2766\n",
      "Epoch 8/50\n",
      "199936/200375 [============================>.] - ETA: 0s - loss: 0.3162 - mean_squared_error: 0.3162\n",
      "Epoch 00008: val_loss did not improve\n",
      "200375/200375 [==============================] - 29s 143us/step - loss: 0.3162 - mean_squared_error: 0.3162 - val_loss: 0.2778 - val_mean_squared_error: 0.2778\n",
      "Epoch 00008: early stopping\n",
      "RMSLE Keras Regressor, validation set, fold  4 :  0.5270790660061371\n"
     ]
    }
   ],
   "source": [
    "for i, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "    \n",
    "    train, val, test2 = prepareData(X.iloc[train_index, :].copy(), X.iloc[val_index, :].copy(), test.copy())\n",
    "    train = train.sort_values(['air_store_id', 'visit_date'])\n",
    "    val = val.sort_values(['air_store_id', 'visit_date'])\n",
    "    test2 = test2.sort_values(['air_store_id', 'visit_date'])\n",
    "    train = train.fillna(0)\n",
    "    val = val.fillna(0)\n",
    "    test2 = test.fillna(0)\n",
    "    \n",
    "    X_train, y_train = train[col], np.log1p(train['visitors'])\n",
    "    X_valid, y_valid = val[col], np.log1p(val['visitors'])\n",
    "    print(\"\\nFold \", i)\n",
    "    \n",
    "    sc = preprocessing.StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    test2 = sc.transform(test2[col])\n",
    "    X_valid = sc.fit_transform(X_valid)\n",
    "    #x_val = np.array(x_valid)\n",
    "    #y_val = np.array(y_valid)\n",
    "    \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units = 400 , kernel_initializer = 'normal', input_dim = X_train.shape[1]))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(.4))\n",
    "    model.add(Dense(units = 160 , kernel_initializer = 'normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(.6))\n",
    "    model.add(Dense(units = 64 , kernel_initializer = 'normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(units = 26, kernel_initializer = 'normal'))\n",
    "    model.add(Dense(units = 64 , kernel_initializer = 'normal'))\n",
    "    model.add(Dense(units = 64 , kernel_initializer = 'normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(.6))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(lr=5e-3,decay=1e-4), metrics=['mean_squared_error'])#lr=0.1,decay=1e-4\n",
    "\n",
    "    \n",
    "    wtpath = 'weights.hdf5'\n",
    "    bestepoch = ModelCheckpoint( filepath=wtpath, verbose=1, save_best_only=True )\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=2, verbose=1) \n",
    "    \n",
    "    network_history = (model.fit(X_train, y_train, validation_data = (X_valid, y_valid), epochs=1000, \n",
    "          batch_size=256, verbose=True, callbacks=[bestepoch, early_stop])) \n",
    "    \n",
    "    val_pred = model.predict(X_valid)\n",
    "    val_pred = [item for sublist in val_pred for item in sublist]\n",
    "    val_pred = np.array(val_pred)\n",
    "    val_pred[val_pred < 0] = 0\n",
    "    val_pred = np.expm1(val_pred)\n",
    "    val_pred[val_pred < 1] = 1\n",
    "    y_train_pred[val_index] = np.array(val_pred) \n",
    "    print('RMSLE Keras Regressor, validation set, fold ', i, ': ', RMSLE(val['visitors'], val_pred))\n",
    "\n",
    "    test_pred = model.predict(test2)\n",
    "    test_pred = [item for sublist in test_pred for item in sublist]\n",
    "    test_pred = np.array(test_pred)\n",
    "    test_pred[test_pred < 0] = 0\n",
    "    test_pred = np.expm1(test_pred)\n",
    "    test_pred[test_pred < 1] = 1\n",
    "    y_test_pred += test_pred\n",
    "\n",
    "    del X_train, X_valid, y_train, y_valid, train, test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred /= K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSLE Keras, full validtion, fold  0.5323928754746718\n"
     ]
    }
   ],
   "source": [
    "print('RMSLE Keras, full validtion, fold  ' + str(RMSLE(X['visitors'].values, y_train_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.180293  28.868744  31.92031   ...  3.4842389  4.114339   4.1106715]\n"
     ]
    }
   ],
   "source": [
    "print(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] = test['air_store_id'] + \"_\" + test['visit_date'].dt.date.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['visitors'] = y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('../Submission/submission11_1.csv', float_format='%.6f', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['air_store_id'] = X['air_store_id']\n",
    "submission['visit_date'] =  X['visit_date'].dt.date\n",
    "submission['visitors'] = y_train_pred\n",
    "submission['actual_visitors'] = X['visitors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_store_id</th>\n",
       "      <th>visit_date</th>\n",
       "      <th>visitors</th>\n",
       "      <th>actual_visitors</th>\n",
       "      <th>rmsle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>47.332245</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2016-07-02</td>\n",
       "      <td>21.128527</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2016-07-04</td>\n",
       "      <td>18.165964</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2016-07-05</td>\n",
       "      <td>21.846720</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2016-07-06</td>\n",
       "      <td>28.322580</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           air_store_id  visit_date   visitors  actual_visitors     rmsle\n",
       "0  air_00a91d42b08b08d9  2016-07-01  47.332245             35.0  0.532393\n",
       "1  air_00a91d42b08b08d9  2016-07-02  21.128527              9.0  0.532393\n",
       "2  air_00a91d42b08b08d9  2016-07-04  18.165964             20.0  0.532393\n",
       "3  air_00a91d42b08b08d9  2016-07-05  21.846720             25.0  0.532393\n",
       "4  air_00a91d42b08b08d9  2016-07-06  28.322580             29.0  0.532393"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['rmsle'] = RMSLE(submission['actual_visitors'], submission['visitors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visitors</th>\n",
       "      <th>rmsle</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual_visitors</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>7.301833</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>8.034849</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>8.191721</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>8.882400</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>9.251229</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>9.894290</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.0</th>\n",
       "      <td>10.529503</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.0</th>\n",
       "      <td>11.197293</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.0</th>\n",
       "      <td>11.827628</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <td>12.534514</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11.0</th>\n",
       "      <td>13.232728</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12.0</th>\n",
       "      <td>14.106394</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.0</th>\n",
       "      <td>14.769218</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14.0</th>\n",
       "      <td>15.682753</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15.0</th>\n",
       "      <td>16.319375</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16.0</th>\n",
       "      <td>17.268516</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17.0</th>\n",
       "      <td>17.950332</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18.0</th>\n",
       "      <td>18.804637</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19.0</th>\n",
       "      <td>19.750796</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20.0</th>\n",
       "      <td>20.273955</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21.0</th>\n",
       "      <td>21.216928</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22.0</th>\n",
       "      <td>21.937560</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23.0</th>\n",
       "      <td>22.604230</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24.0</th>\n",
       "      <td>23.456705</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25.0</th>\n",
       "      <td>24.006436</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26.0</th>\n",
       "      <td>24.933154</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27.0</th>\n",
       "      <td>25.381538</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28.0</th>\n",
       "      <td>26.244077</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29.0</th>\n",
       "      <td>27.068722</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30.0</th>\n",
       "      <td>27.659878</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207.0</th>\n",
       "      <td>84.377983</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209.0</th>\n",
       "      <td>19.923498</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211.0</th>\n",
       "      <td>52.971920</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216.0</th>\n",
       "      <td>30.071995</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217.0</th>\n",
       "      <td>65.766945</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218.0</th>\n",
       "      <td>35.221401</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222.0</th>\n",
       "      <td>15.921898</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228.0</th>\n",
       "      <td>31.875908</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229.0</th>\n",
       "      <td>21.550705</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235.0</th>\n",
       "      <td>76.326759</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239.0</th>\n",
       "      <td>45.705877</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244.0</th>\n",
       "      <td>33.944065</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248.0</th>\n",
       "      <td>50.224346</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261.0</th>\n",
       "      <td>17.801615</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262.0</th>\n",
       "      <td>5.488183</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269.0</th>\n",
       "      <td>102.229118</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305.0</th>\n",
       "      <td>36.683914</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311.0</th>\n",
       "      <td>16.107719</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325.0</th>\n",
       "      <td>24.099144</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328.0</th>\n",
       "      <td>64.354401</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335.0</th>\n",
       "      <td>43.460545</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348.0</th>\n",
       "      <td>19.710121</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351.0</th>\n",
       "      <td>13.183074</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372.0</th>\n",
       "      <td>169.997711</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409.0</th>\n",
       "      <td>26.806616</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514.0</th>\n",
       "      <td>30.338745</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627.0</th>\n",
       "      <td>20.825315</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675.0</th>\n",
       "      <td>64.650505</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777.0</th>\n",
       "      <td>25.028374</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877.0</th>\n",
       "      <td>12.029203</td>\n",
       "      <td>0.532393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   visitors     rmsle\n",
       "actual_visitors                      \n",
       "1.0                7.301833  0.532393\n",
       "2.0                8.034849  0.532393\n",
       "3.0                8.191721  0.532393\n",
       "4.0                8.882400  0.532393\n",
       "5.0                9.251229  0.532393\n",
       "6.0                9.894290  0.532393\n",
       "7.0               10.529503  0.532393\n",
       "8.0               11.197293  0.532393\n",
       "9.0               11.827628  0.532393\n",
       "10.0              12.534514  0.532393\n",
       "11.0              13.232728  0.532393\n",
       "12.0              14.106394  0.532393\n",
       "13.0              14.769218  0.532393\n",
       "14.0              15.682753  0.532393\n",
       "15.0              16.319375  0.532393\n",
       "16.0              17.268516  0.532393\n",
       "17.0              17.950332  0.532393\n",
       "18.0              18.804637  0.532393\n",
       "19.0              19.750796  0.532393\n",
       "20.0              20.273955  0.532393\n",
       "21.0              21.216928  0.532393\n",
       "22.0              21.937560  0.532393\n",
       "23.0              22.604230  0.532393\n",
       "24.0              23.456705  0.532393\n",
       "25.0              24.006436  0.532393\n",
       "26.0              24.933154  0.532393\n",
       "27.0              25.381538  0.532393\n",
       "28.0              26.244077  0.532393\n",
       "29.0              27.068722  0.532393\n",
       "30.0              27.659878  0.532393\n",
       "...                     ...       ...\n",
       "207.0             84.377983  0.532393\n",
       "209.0             19.923498  0.532393\n",
       "211.0             52.971920  0.532393\n",
       "216.0             30.071995  0.532393\n",
       "217.0             65.766945  0.532393\n",
       "218.0             35.221401  0.532393\n",
       "222.0             15.921898  0.532393\n",
       "228.0             31.875908  0.532393\n",
       "229.0             21.550705  0.532393\n",
       "235.0             76.326759  0.532393\n",
       "239.0             45.705877  0.532393\n",
       "244.0             33.944065  0.532393\n",
       "248.0             50.224346  0.532393\n",
       "261.0             17.801615  0.532393\n",
       "262.0              5.488183  0.532393\n",
       "269.0            102.229118  0.532393\n",
       "305.0             36.683914  0.532393\n",
       "311.0             16.107719  0.532393\n",
       "325.0             24.099144  0.532393\n",
       "328.0             64.354401  0.532393\n",
       "335.0             43.460545  0.532393\n",
       "348.0             19.710121  0.532393\n",
       "351.0             13.183074  0.532393\n",
       "372.0            169.997711  0.532393\n",
       "409.0             26.806616  0.532393\n",
       "514.0             30.338745  0.532393\n",
       "627.0             20.825315  0.532393\n",
       "675.0             64.650505  0.532393\n",
       "777.0             25.028374  0.532393\n",
       "877.0             12.029203  0.532393\n",
       "\n",
       "[203 rows x 2 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.groupby(['actual_visitors']).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
