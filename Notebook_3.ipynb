{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#start to 2015-11-02 val set\n",
    "#2017-3-15 onwards validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vedpk\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\vedpk\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\vedpk\\Anaconda3\\lib\\site-packages\\sklearn\\learning_curve.py:22: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import glob, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "import datetime as dt\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "air_visit =  pd.read_csv('../Input/air_visit_data.csv')\n",
    "air_store =  pd.read_csv('../Input/air_store_info_proc1.csv')\n",
    "hpg_store = pd.read_csv('../Input/hpg_store_info_proc1.csv')\n",
    "air_res = pd.read_csv('../Input/air_reserve.csv')\n",
    "hpg_res =  pd.read_csv('../Input/hpg_reserve.csv')\n",
    "store_id =  pd.read_csv('../Input/store_id_relation.csv')\n",
    "sample_submission =  pd.read_csv('../Input/sample_submission.csv')\n",
    "date_info =  pd.read_csv('../Input/date_info.csv').rename(columns={'calendar_date':'visit_date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_info['weight'] = ((date_info.index + 1) / len(date_info)) ** 5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hpg_res = pd.merge(hpg_res, store_id, how='inner', on=['hpg_store_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "air_res['visit_datetime'] = pd.to_datetime(air_res['visit_datetime'])\n",
    "air_res['visit_datetime'] = air_res['visit_datetime'].dt.date\n",
    "air_res['reserve_datetime'] = pd.to_datetime(air_res['reserve_datetime'])\n",
    "air_res['reserve_datetime'] = air_res['reserve_datetime'].dt.date\n",
    "air_res['reserve_datetime_diff'] = air_res.apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n",
    "tmp1 = air_res.groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs1', 'reserve_visitors':'rv1'})\n",
    "tmp2 = air_res.groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs2', 'reserve_visitors':'rv2'})\n",
    "air_res = pd.merge(tmp1, tmp2, how='inner', on=['air_store_id','visit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hpg_res['visit_datetime'] = pd.to_datetime(hpg_res['visit_datetime'])\n",
    "hpg_res['visit_datetime'] = hpg_res['visit_datetime'].dt.date\n",
    "hpg_res['reserve_datetime'] = pd.to_datetime(hpg_res['reserve_datetime'])\n",
    "hpg_res['reserve_datetime'] = hpg_res['reserve_datetime'].dt.date\n",
    "hpg_res['reserve_datetime_diff'] = hpg_res.apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n",
    "tmp1 = hpg_res.groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs1', 'reserve_visitors':'rv1'})\n",
    "tmp2 = hpg_res.groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs2', 'reserve_visitors':'rv2'})\n",
    "hpg_res = pd.merge(tmp1, tmp2, how='inner', on=['air_store_id','visit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "air_visit['visit_date'] = pd.to_datetime(air_visit['visit_date'])\n",
    "air_visit['dow'] = air_visit['visit_date'].dt.dayofweek\n",
    "air_visit['year'] = air_visit['visit_date'].dt.year\n",
    "air_visit['month'] = air_visit['visit_date'].dt.month\n",
    "air_visit['visit_date'] = air_visit['visit_date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_submission['visit_date'] = sample_submission['id'].map(lambda x: str(x).split('_')[2])\n",
    "sample_submission['air_store_id'] = sample_submission['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "sample_submission['visit_date'] = pd.to_datetime(sample_submission['visit_date'])\n",
    "sample_submission['dow'] = sample_submission['visit_date'].dt.dayofweek\n",
    "sample_submission['year'] = sample_submission['visit_date'].dt.year\n",
    "sample_submission['month'] = sample_submission['visit_date'].dt.month\n",
    "sample_submission['visit_date'] = sample_submission['visit_date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_stores = sample_submission['air_store_id'].unique()\n",
    "stores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i]*len(unique_stores)}) for i in range(7)], axis=0, ignore_index=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(477)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air_visit.sort_values('visit_date').iloc[-1]['visit_date']-air_visit.sort_values('visit_date').iloc[0]['visit_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2016, 3, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air_visit.sort_values('visit_date').iloc[0]['visit_date']+dt.timedelta(days=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2017, 3, 15)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air_visit.sort_values('visit_date').iloc[-1]['visit_date']-dt.timedelta(days=38) #38days validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val = air_visit[air_visit['visit_date'] < dt.date(2017, 3, 15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_store_id</th>\n",
       "      <th>visit_date</th>\n",
       "      <th>visitors</th>\n",
       "      <th>dow</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>2016-01-13</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>2016-01-14</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>2016-01-15</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>2016-01-16</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>2016-01-18</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           air_store_id  visit_date  visitors  dow  year  month\n",
       "0  air_ba937bf13d40fb24  2016-01-13        25    2  2016      1\n",
       "1  air_ba937bf13d40fb24  2016-01-14        32    3  2016      1\n",
       "2  air_ba937bf13d40fb24  2016-01-15        29    4  2016      1\n",
       "3  air_ba937bf13d40fb24  2016-01-16        22    5  2016      1\n",
       "4  air_ba937bf13d40fb24  2016-01-18         6    0  2016      1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "air_visit = air_visit[air_visit['visit_date'] > dt.date(2016, 3, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sure it can be compressed...\n",
    "tmp = air_visit.groupby(['air_store_id','dow'], as_index=False)['visitors'].min().rename(columns={'visitors':'min_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n",
    "tmp = air_visit.groupby(['air_store_id','dow'], as_index=False)['visitors'].mean().rename(columns={'visitors':'mean_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = air_visit.groupby(['air_store_id','dow'], as_index=False)['visitors'].median().rename(columns={'visitors':'median_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = air_visit.groupby(['air_store_id','dow'], as_index=False)['visitors'].max().rename(columns={'visitors':'max_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = air_visit.groupby(['air_store_id','dow'], as_index=False)['visitors'].count().rename(columns={'visitors':'count_observations'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stores = pd.merge(stores, air_store, how='left', on=['air_store_id']) \n",
    "\n",
    "stores['air_genre_name'] = stores['air_genre_name'].map(lambda x: str(str(x).replace('/',' ')))\n",
    "stores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-',' ')))\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for i in range(10):\n",
    "    stores['air_genre_name'+str(i)] = lbl.fit_transform(stores['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "    stores['air_area_name'+str(i)] = lbl.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "stores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\n",
    "stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_info['visit_date'] = pd.to_datetime(date_info['visit_date'])\n",
    "date_info['day_of_week'] = lbl.fit_transform(date_info['day_of_week'])\n",
    "date_info['visit_date'] = date_info['visit_date'].dt.date\n",
    "train = pd.merge(air_visit, date_info, how='left', on=['visit_date']) \n",
    "test = pd.merge(sample_submission, date_info, how='left', on=['visit_date']) \n",
    "\n",
    "val = pd.merge(val, date_info, how='left', on=['visit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.merge(train, stores, how='left', on=['air_store_id','dow']) \n",
    "test = pd.merge(test, stores, how='left', on=['air_store_id','dow'])\n",
    "\n",
    "val = pd.merge(val, stores, how='left', on=['air_store_id','dow'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.merge(train, air_res, how='left', on=['air_store_id','visit_date']) \n",
    "test = pd.merge(test, air_res, how='left', on=['air_store_id','visit_date'])\n",
    "\n",
    "train = pd.merge(train, hpg_res, how='left', on=['air_store_id','visit_date']) \n",
    "test = pd.merge(test, hpg_res, how='left', on=['air_store_id','visit_date'])\n",
    "\n",
    "val = pd.merge(val, air_res, how='left', on=['air_store_id','visit_date']) \n",
    "val = pd.merge(val, hpg_res, how='left', on=['air_store_id','visit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['id'] = train.apply(lambda r: '_'.join([str(r['air_store_id']), str(r['visit_date'])]), axis=1)\n",
    "val['id'] = val.apply(lambda r: '_'.join([str(r['air_store_id']), str(r['visit_date'])]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['total_reserv_sum'] = train['rv1_x'] + train['rv1_y']\n",
    "train['total_reserv_mean'] = (train['rv2_x'] + train['rv2_y']) / 2\n",
    "train['total_reserv_dt_diff_mean'] = (train['rs2_x'] + train['rs2_y']) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['total_reserv_sum'] = test['rv1_x'] + test['rv1_y']\n",
    "test['total_reserv_mean'] = (test['rv2_x'] + test['rv2_y']) / 2\n",
    "test['total_reserv_dt_diff_mean'] = (test['rs2_x'] + test['rs2_y']) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val['total_reserv_sum'] = val['rv1_x'] + val['rv1_y']\n",
    "val['total_reserv_mean'] = (val['rv2_x'] + val['rv2_y']) / 2\n",
    "val['total_reserv_dt_diff_mean'] = (val['rs2_x'] + val['rs2_y']) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train['date_int'] = train['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "#test['date_int'] = test['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "train['var_max_lat'] = train['latitude'].max() - train['latitude']\n",
    "train['var_max_long'] = train['longitude'].max() - train['longitude']\n",
    "test['var_max_lat'] = test['latitude'].max() - test['latitude']\n",
    "test['var_max_long'] = test['longitude'].max() - test['longitude']\n",
    "\n",
    "#val['date_int'] = val['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "val['var_max_lat'] = val['latitude'].max() - val['latitude']\n",
    "val['var_max_long'] = val['longitude'].max() - val['longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['lon_plus_lat'] = train['longitude'] + train['latitude'] \n",
    "test['lon_plus_lat'] = test['longitude'] + test['latitude']\n",
    "\n",
    "val['lon_plus_lat'] = val['longitude'] + val['latitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lbl = preprocessing.LabelEncoder()\n",
    "train['air_store_id2'] = lbl.fit_transform(train['air_store_id'])\n",
    "test['air_store_id2'] = lbl.transform(test['air_store_id'])\n",
    "\n",
    "val['air_store_id2'] = lbl.transform(val['air_store_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col = [c for c in train if c not in ['id', 'air_store_id', 'visit_date','visitors','prefecture','city','rv1_x','rv1_y','rv2_x','rv2_y']]\n",
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)\n",
    "\n",
    "val = val.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['air_cb083b4789a8d3a2',\n",
       " 'air_d63cfa6d6ab78446',\n",
       " 'air_cf22e368c1a71d53',\n",
       " 'air_d0a7bd3339c3d12a',\n",
       " 'air_0ead98dd07e7a82a',\n",
       " 'air_2703dcb33192b181',\n",
       " 'air_229d7e508d9f1b5e',\n",
       " 'air_b2d8bc9c88b85f96']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dropping stores not in test\n",
    "\n",
    "to_drop = list(set(train['air_store_id']) - set(test['air_store_id']))\n",
    "to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.where(~(train['air_store_id'].isin(to_drop)))\n",
    "train = train.dropna(axis=0,subset=['air_store_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val = val.where(~(val['air_store_id'].isin(to_drop)))\n",
    "val = val.dropna(axis=0,subset=['air_store_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "821"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train['air_store_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#X = train[train['visit_date'] < dt.date(2017, 2, 21)]\n",
    "X = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = np.log1p(X['visitors'])\n",
    "X_train = X[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_val = np.log1p(val['visitors'])\n",
    "X_val = val[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_train = xgb.DMatrix(X_train,y_train)\n",
    "d_valid = xgb.DMatrix(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RMSLE(y, pred):\n",
    "    return metrics.mean_squared_error(y, pred)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = ensemble.GradientBoostingRegressor(learning_rate=0.2, random_state=3, n_estimators=200, subsample=0.8, \n",
    "                      max_depth =10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2 = neighbors.KNeighborsRegressor(n_jobs=-1, n_neighbors=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model3 = xgb.XGBRegressor(learning_rate=0.2, random_state=3, n_estimators=200, subsample=0.8, \n",
    "                     colsample_bytree=0.8, max_depth =10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_train = xgb.DMatrix(X_train,y_train)\n",
    "d_valid = xgb.DMatrix(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['objective'] = 'reg:linear'\n",
    "params['booster'] = 'gbtree'\n",
    "params['eval_metric'] = 'rmse'\n",
    "params['eta'] = 0.1\n",
    "params['max_depth'] = 4\n",
    "params['silent'] = 1\n",
    "params['subsample'] = 0.8\n",
    "params['colsample_bytree'] = 0.8\n",
    "params['tree_method'] = \"exact\"\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.2, loss='ls', max_depth=10, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=200, presort='auto', random_state=3,\n",
       "             subsample=0.8, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(train[col], np.log1p(train['visitors'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "          metric_params=None, n_jobs=-1, n_neighbors=4, p=2,\n",
       "          weights='uniform')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(train[col], np.log1p(train['visitors'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8, gamma=0, learning_rate=0.2, max_delta_step=0,\n",
       "       max_depth=10, min_child_weight=1, missing=None, n_estimators=200,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=3,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=0.8)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(X_train[col], np.log1p(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:2.20736\tvalid-rmse:2.20433\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 100 rounds.\n",
      "[10]\ttrain-rmse:0.909145\tvalid-rmse:0.923466\n",
      "[20]\ttrain-rmse:0.574003\tvalid-rmse:0.600156\n",
      "[30]\ttrain-rmse:0.51536\tvalid-rmse:0.545571\n",
      "[40]\ttrain-rmse:0.505893\tvalid-rmse:0.537777\n",
      "[50]\ttrain-rmse:0.503147\tvalid-rmse:0.536969\n",
      "[60]\ttrain-rmse:0.501799\tvalid-rmse:0.537648\n",
      "[70]\ttrain-rmse:0.50068\tvalid-rmse:0.536886\n",
      "[80]\ttrain-rmse:0.499709\tvalid-rmse:0.536308\n",
      "[90]\ttrain-rmse:0.499028\tvalid-rmse:0.537346\n",
      "[100]\ttrain-rmse:0.498378\tvalid-rmse:0.537127\n",
      "[110]\ttrain-rmse:0.497832\tvalid-rmse:0.536708\n",
      "[120]\ttrain-rmse:0.497309\tvalid-rmse:0.536467\n",
      "[130]\ttrain-rmse:0.496665\tvalid-rmse:0.535979\n",
      "[140]\ttrain-rmse:0.49618\tvalid-rmse:0.535345\n",
      "[150]\ttrain-rmse:0.495801\tvalid-rmse:0.535447\n",
      "[160]\ttrain-rmse:0.495412\tvalid-rmse:0.535225\n",
      "[170]\ttrain-rmse:0.494998\tvalid-rmse:0.534951\n",
      "[180]\ttrain-rmse:0.494433\tvalid-rmse:0.533686\n",
      "[190]\ttrain-rmse:0.494044\tvalid-rmse:0.533431\n",
      "[200]\ttrain-rmse:0.493574\tvalid-rmse:0.533067\n",
      "[210]\ttrain-rmse:0.493198\tvalid-rmse:0.532856\n",
      "[220]\ttrain-rmse:0.492786\tvalid-rmse:0.532643\n",
      "[230]\ttrain-rmse:0.492513\tvalid-rmse:0.532374\n",
      "[240]\ttrain-rmse:0.492123\tvalid-rmse:0.53187\n",
      "[250]\ttrain-rmse:0.491695\tvalid-rmse:0.53139\n",
      "[260]\ttrain-rmse:0.4914\tvalid-rmse:0.531328\n",
      "[270]\ttrain-rmse:0.491059\tvalid-rmse:0.530654\n",
      "[280]\ttrain-rmse:0.490768\tvalid-rmse:0.530416\n",
      "[290]\ttrain-rmse:0.490464\tvalid-rmse:0.530283\n",
      "[300]\ttrain-rmse:0.490137\tvalid-rmse:0.529861\n",
      "[310]\ttrain-rmse:0.489681\tvalid-rmse:0.529436\n",
      "[320]\ttrain-rmse:0.489466\tvalid-rmse:0.529398\n",
      "[330]\ttrain-rmse:0.4891\tvalid-rmse:0.529126\n",
      "[340]\ttrain-rmse:0.488861\tvalid-rmse:0.529129\n",
      "[350]\ttrain-rmse:0.48864\tvalid-rmse:0.528993\n",
      "[360]\ttrain-rmse:0.488333\tvalid-rmse:0.528749\n",
      "[370]\ttrain-rmse:0.488026\tvalid-rmse:0.528483\n",
      "[380]\ttrain-rmse:0.48765\tvalid-rmse:0.528318\n",
      "[390]\ttrain-rmse:0.487402\tvalid-rmse:0.528125\n",
      "[400]\ttrain-rmse:0.486995\tvalid-rmse:0.527503\n",
      "[410]\ttrain-rmse:0.486644\tvalid-rmse:0.52744\n",
      "[420]\ttrain-rmse:0.486458\tvalid-rmse:0.527279\n",
      "[430]\ttrain-rmse:0.486277\tvalid-rmse:0.527503\n",
      "[440]\ttrain-rmse:0.486035\tvalid-rmse:0.527602\n",
      "[450]\ttrain-rmse:0.485779\tvalid-rmse:0.528119\n",
      "[460]\ttrain-rmse:0.48547\tvalid-rmse:0.527904\n",
      "[470]\ttrain-rmse:0.485219\tvalid-rmse:0.527633\n",
      "[480]\ttrain-rmse:0.485025\tvalid-rmse:0.527559\n",
      "[490]\ttrain-rmse:0.484706\tvalid-rmse:0.527034\n",
      "[500]\ttrain-rmse:0.484423\tvalid-rmse:0.526609\n",
      "[510]\ttrain-rmse:0.484229\tvalid-rmse:0.526437\n",
      "[520]\ttrain-rmse:0.483926\tvalid-rmse:0.526185\n",
      "[530]\ttrain-rmse:0.483683\tvalid-rmse:0.52671\n",
      "[540]\ttrain-rmse:0.483442\tvalid-rmse:0.526378\n",
      "[550]\ttrain-rmse:0.483184\tvalid-rmse:0.526036\n",
      "[560]\ttrain-rmse:0.482993\tvalid-rmse:0.525941\n",
      "[570]\ttrain-rmse:0.482714\tvalid-rmse:0.525848\n",
      "[580]\ttrain-rmse:0.482503\tvalid-rmse:0.525871\n",
      "[590]\ttrain-rmse:0.482338\tvalid-rmse:0.52555\n",
      "[600]\ttrain-rmse:0.48206\tvalid-rmse:0.525054\n",
      "[610]\ttrain-rmse:0.481885\tvalid-rmse:0.525681\n",
      "[620]\ttrain-rmse:0.481735\tvalid-rmse:0.522835\n",
      "[630]\ttrain-rmse:0.48155\tvalid-rmse:0.522534\n",
      "[640]\ttrain-rmse:0.481294\tvalid-rmse:0.522188\n",
      "[650]\ttrain-rmse:0.481095\tvalid-rmse:0.521302\n",
      "[660]\ttrain-rmse:0.480923\tvalid-rmse:0.520988\n",
      "[670]\ttrain-rmse:0.480711\tvalid-rmse:0.520722\n",
      "[680]\ttrain-rmse:0.480517\tvalid-rmse:0.521719\n",
      "[690]\ttrain-rmse:0.480297\tvalid-rmse:0.52165\n",
      "[700]\ttrain-rmse:0.4801\tvalid-rmse:0.521738\n",
      "[710]\ttrain-rmse:0.479931\tvalid-rmse:0.521374\n",
      "[720]\ttrain-rmse:0.479741\tvalid-rmse:0.520487\n",
      "[730]\ttrain-rmse:0.479547\tvalid-rmse:0.521009\n",
      "[740]\ttrain-rmse:0.479386\tvalid-rmse:0.520696\n",
      "[750]\ttrain-rmse:0.479208\tvalid-rmse:0.520728\n",
      "[760]\ttrain-rmse:0.47904\tvalid-rmse:0.520529\n",
      "[770]\ttrain-rmse:0.478896\tvalid-rmse:0.520129\n",
      "[780]\ttrain-rmse:0.478738\tvalid-rmse:0.519664\n",
      "[790]\ttrain-rmse:0.478548\tvalid-rmse:0.52218\n",
      "[800]\ttrain-rmse:0.478377\tvalid-rmse:0.521801\n",
      "[810]\ttrain-rmse:0.478236\tvalid-rmse:0.521673\n",
      "[820]\ttrain-rmse:0.478044\tvalid-rmse:0.521483\n",
      "[830]\ttrain-rmse:0.477851\tvalid-rmse:0.521114\n",
      "[840]\ttrain-rmse:0.477711\tvalid-rmse:0.521125\n",
      "[850]\ttrain-rmse:0.477517\tvalid-rmse:0.521153\n",
      "[860]\ttrain-rmse:0.477363\tvalid-rmse:0.521159\n",
      "[870]\ttrain-rmse:0.477229\tvalid-rmse:0.522468\n",
      "[880]\ttrain-rmse:0.477034\tvalid-rmse:0.522511\n",
      "Stopping. Best iteration:\n",
      "[780]\ttrain-rmse:0.478738\tvalid-rmse:0.519664\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model4 = (xgb.train(params,d_train,num_boost_round=2000,evals=watchlist,\n",
    "                   early_stopping_rounds=100,verbose_eval=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds1 = model1.predict(val[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds2 = model2.predict(val[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds3 = model3.predict(val[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred4 = model4.predict(xgb.DMatrix(val[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE GradientBoostingRegressor:  0.366855472476\n",
      "RMSE KNeighborsRegressor:  0.433797705915\n",
      "RMSE XGBRegressor:  1.62164716171\n",
      "RMSE XGBTrain:  0.482479349734\n"
     ]
    }
   ],
   "source": [
    "print('RMSE GradientBoostingRegressor: ', RMSLE(np.log1p(val['visitors'].values), preds1))\n",
    "print('RMSE KNeighborsRegressor: ', RMSLE(np.log1p(val['visitors'].values), preds2))\n",
    "print('RMSE XGBRegressor: ', RMSLE(np.log1p(val['visitors'].values), preds3))\n",
    "print('RMSE XGBTrain: ', RMSLE(np.log1p(val['visitors'].values), pred4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "preds1 = model1.predict(test[col])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "preds2 = model2.predict(test[col])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "preds3 = model3.predict(test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred4 = model4.predict(test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test['visitors'] = (preds2+preds3) / 2 #(preds1+preds2+preds3) / 3\n",
    "test['visitors'] = (preds1+preds2+preds3+preds4) / 4 \n",
    "test['visitors'] = np.expm1(test['visitors']).clip(lower=0.)\n",
    "sub1 = test[['id','visitors']].copy()\n",
    "#del train; del data;"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "dfs = { re.search('/([^/\\.]*)\\\\.csv', fn).group(1):\n",
    "    pd.read_csv(fn)for fn in glob.glob('../Input/*.csv')}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "for k, v in dfs.items(): \n",
    "    k = k.split(\"\\\\\")[1]\n",
    "    locals()[k] = v"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "wkend_holidays = date_info.apply(\n",
    "    (lambda x:(x.day_of_week=='Sunday' or x.day_of_week=='Saturday') and x.holiday_flg==1), axis=1)\n",
    "date_info.loc[wkend_holidays, 'holiday_flg'] = 0\n",
    "date_info['weight'] = ((date_info.index + 1) / len(date_info)) ** 5  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "visit_data = air_visit_data.merge(date_info, left_on='visit_date', right_on='calendar_date', how='left')\n",
    "visit_data.drop('calendar_date', axis=1, inplace=True)\n",
    "visit_data['visitors'] = visit_data.visitors.map(pd.np.log1p)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "wmean = lambda x:( (x.weight * x.visitors).sum() / x.weight.sum() )\n",
    "visitors = visit_data.groupby(['air_store_id', 'day_of_week', 'holiday_flg']).apply(wmean).reset_index()\n",
    "visitors.rename(columns={0:'visitors'}, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "sample_submission['air_store_id'] = sample_submission.id.map(lambda x: '_'.join(x.split('_')[:-1]))\n",
    "sample_submission['calendar_date'] = sample_submission.id.map(lambda x: x.split('_')[2])\n",
    "sample_submission.drop('visitors', axis=1, inplace=True)\n",
    "sample_submission = sample_submission.merge(date_info, on='calendar_date', how='left')\n",
    "sample_submission = sample_submission.merge(visitors, on=[\n",
    "    'air_store_id', 'day_of_week', 'holiday_flg'], how='left')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "missings = sample_submission.visitors.isnull()\n",
    "sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n",
    "    visitors[visitors.holiday_flg==0], on=('air_store_id', 'day_of_week'), \n",
    "    how='left')['visitors_y'].values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "missings = sample_submission.visitors.isnull()\n",
    "sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n",
    "    visitors[['air_store_id', 'visitors']].groupby('air_store_id').mean().reset_index(), \n",
    "    on='air_store_id', how='left')['visitors_y'].values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "sample_submission['visitors'] = sample_submission.visitors.map(pd.np.expm1)\n",
    "sub2 = sample_submission[['id', 'visitors']].copy()\n",
    "sub_merge = pd.merge(sub1, sub2, on='id', how='inner')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "sub_merge['visitors'] = (sub_merge['visitors_x'] + sub_merge['visitors_y']* 1.1)/2\n",
    "sub_merge[['id', 'visitors']].to_csv('../Submission/submission3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_merge['visitors'] = (sub_merge['visitors'])/2\n",
    "sub_merge[['id', 'visitors']].to_csv('../Submission/submission3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
