{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vedpk\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\vedpk\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\vedpk\\Anaconda3\\lib\\site-packages\\sklearn\\learning_curve.py:22: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\vedpk\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import glob, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "import datetime as dt\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, PReLU, BatchNormalization\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/train_proc5.csv\")\n",
    "test = pd.read_csv(\"../input/test_proc5.csv\")\n",
    "submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "air_store = pd.read_csv(\"../input/allstore_info_proc1.csv\").rename(columns={'store_id':'air_store_id'})\n",
    "date_info = pd.read_csv(\"../input/date_info.csv\").rename(columns={\"calendar_date\" : 'visit_date'})\n",
    "train_weather = pd.read_csv(\"../input/train_weather_01.csv\")\n",
    "test_weather = pd.read_csv(\"../input/test_weather_01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw1_start = dt.date(2016,4,29)\n",
    "gw1_end = dt.date(2016,5,5)\n",
    "gw2_start = dt.date(2017,4,29)\n",
    "gw2_end = dt.date(2017,5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['visit_date'] = submission['id'].map(lambda x: str(x).split('_')[2])\n",
    "submission['air_store_id'] = submission['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "submission['visit_date'] = pd.to_datetime(submission['visit_date'])\n",
    "submission['visitors'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weather = pd.concat([train_weather, test_weather])\n",
    "all_weather['visit_date'] = pd.to_datetime(all_weather['visit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['visit_date'] = pd.to_datetime(train['visit_date'])\n",
    "test['visit_date'] = pd.to_datetime(test['visit_date'])\n",
    "test['visitors'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_info['visit_date'] = pd.to_datetime(date_info['visit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_info['date_seq_id'] = date_info['visit_date'].dt.date - date_info['visit_date'].dt.date.min() + dt.timedelta(1)\n",
    "date_info['date_seq_id'] = date_info['date_seq_id']/dt.timedelta(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_info['week_seq_id'] = ((date_info['date_seq_id']+4)/7).astype(np.int64)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "date_info['holiday_flg'] = date_info[['day_of_week','holiday_flg']].apply(lambda x: 1 if x['holiday_flg'] == 1 or \n",
    "    x['day_of_week'] == 'Friday' or x['day_of_week'] == 'Saturday' or x['day_of_week'] == 'Sunday' else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_info['golden_week'] = 0\n",
    "date_info['after_golden_week'] = 0\n",
    "date_info.loc[(date_info['visit_date'] >= gw1_start) & (date_info['visit_date'] <= gw1_end), 'golden_week'] = 1\n",
    "date_info.loc[(date_info['visit_date'] >= dt.date(2016,5,6)) & (date_info['visit_date'] <= dt.date(2016,5,12)), 'after_golden_week'] = 1\n",
    "\n",
    "date_info.loc[(date_info['visit_date'] >= gw2_start) & (date_info['visit_date'] <= gw2_end), 'golden_week'] = 1\n",
    "date_info.loc[(date_info['visit_date'] >= dt.date(2017,5,6)) & (date_info['visit_date'] <= dt.date(2017,5,12)), 'after_golden_week'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_to_drop = list(set(train['air_store_id']) - set(test['air_store_id']))\n",
    "train = train.where(~(train['air_store_id'].isin(stores_to_drop)))\n",
    "train = train.dropna(axis=0,subset=['air_store_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([train,test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train; del test;\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.merge(all_data,air_store,how='left',on='air_store_id',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.merge(all_data,date_info,how='left',on='visit_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['visit_day'] = all_data['visit_date'].dt.day\n",
    "all_data['visit_month'] = all_data['visit_date'].dt.month\n",
    "all_data['visit_year'] = all_data['visit_date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['diff_max_lat'] = all_data['latitude'].max() - all_data['latitude']\n",
    "all_data['diff_min_lat'] = all_data['latitude'].min() - all_data['latitude']\n",
    "all_data['diff_max_long'] = all_data['longitude'].max() - all_data['longitude']\n",
    "all_data['diff_min_long'] = all_data['longitude'].min() - all_data['longitude']\n",
    "all_data['lat_plus_long'] = all_data['latitude'] + all_data['longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_to_drop = ['air_b2d8bc9c88b85f96',\n",
    " 'air_cf22e368c1a71d53',\n",
    " 'air_229d7e508d9f1b5e',\n",
    " 'air_d0a7bd3339c3d12a',\n",
    " 'air_cb083b4789a8d3a2',\n",
    " 'air_2703dcb33192b181',\n",
    " 'air_0ead98dd07e7a82a',\n",
    " 'air_d63cfa6d6ab78446']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.where(~(all_data['air_store_id'].isin(stores_to_drop)))\n",
    "all_data = all_data.dropna(axis=0,subset=['air_store_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_data = pd.merge(all_data,all_weather, on=['air_store_id','visit_date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(282487, 70)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "del air_store;del date_info;gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['visit_date'] = pd.to_datetime(all_data['visit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_week_multiplier = 1.1688571428571\n",
    "after_golden_week_multiplier = 0.85"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "all_data['multiplier'] = 1\n",
    "all_data.loc[(all_data['visit_date'] >= gw1_start) & (all_data['visit_date'] <= gw1_end), 'multiplier'] = golden_week_multiplier\n",
    "all_data.loc[(all_data['visit_date'] >= dt.date(2016,5,6)) & (all_data['visit_date'] <= dt.date(2016,5,12)), 'multiplier'] = after_golden_week_multiplier\n",
    "\n",
    "all_data.loc[(all_data['visit_date'] >= gw2_start) & (all_data['visit_date'] <= gw2_end), 'multiplier'] = golden_week_multiplier\n",
    "all_data.loc[(all_data['visit_date'] >= dt.date(2017,5,6)) & (all_data['visit_date'] <= dt.date(2017,5,12)), 'multiplier'] = after_golden_week_multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 282487 entries, 0 to 282486\n",
      "Data columns (total 70 columns):\n",
      "air_store_id            282487 non-null object\n",
      "diff_10_9               282487 non-null float64\n",
      "diff_11_10              282487 non-null float64\n",
      "diff_12_11              282487 non-null float64\n",
      "diff_8_7                282474 non-null float64\n",
      "diff_9_8                282481 non-null float64\n",
      "eight_weeks_ago         221561 non-null float64\n",
      "eleven_weeks_ago        207193 non-null float64\n",
      "nine_weeks_ago          216946 non-null float64\n",
      "priorMax                249026 non-null float64\n",
      "priorMean               249026 non-null float64\n",
      "priorMin                249026 non-null float64\n",
      "priorSum                282487 non-null float64\n",
      "seven_weeks_ago         226112 non-null float64\n",
      "six_weeks_ago           230884 non-null float64\n",
      "ten_weeks_ago           211815 non-null float64\n",
      "visit_date              282487 non-null datetime64[ns]\n",
      "visitors                250468 non-null float64\n",
      "vmax_6weekago_10        282487 non-null float64\n",
      "vmax_6weekago_11        282487 non-null float64\n",
      "vmax_6weekago_12        282487 non-null float64\n",
      "vmax_6weekago_7         282474 non-null float64\n",
      "vmax_6weekago_8         282481 non-null float64\n",
      "vmax_6weekago_9         282487 non-null float64\n",
      "vmean_6weekago_10       282487 non-null float64\n",
      "vmean_6weekago_11       282487 non-null float64\n",
      "vmean_6weekago_12       282487 non-null float64\n",
      "vmean_6weekago_7        282474 non-null float64\n",
      "vmean_6weekago_8        282481 non-null float64\n",
      "vmean_6weekago_9        282487 non-null float64\n",
      "vmedian_6weekago_10     282487 non-null float64\n",
      "vmedian_6weekago_11     282487 non-null float64\n",
      "vmedian_6weekago_12     282487 non-null float64\n",
      "vmedian_6weekago_7      282474 non-null float64\n",
      "vmedian_6weekago_8      282481 non-null float64\n",
      "vmedian_6weekago_9      282487 non-null float64\n",
      "vmin_6weekago_10        282487 non-null float64\n",
      "vmin_6weekago_11        282487 non-null float64\n",
      "vmin_6weekago_12        282487 non-null float64\n",
      "vmin_6weekago_7         282474 non-null float64\n",
      "vmin_6weekago_8         282481 non-null float64\n",
      "vmin_6weekago_9         282487 non-null float64\n",
      "latitude                282487 non-null float64\n",
      "longitude               282487 non-null float64\n",
      "genre_name              282487 non-null object\n",
      "area_name               282487 non-null object\n",
      "prefecture              282487 non-null object\n",
      "city                    282487 non-null object\n",
      "area_id                 282487 non-null int64\n",
      "n200mt_cluster_id       282487 non-null int64\n",
      "n400mt_cluster_id       282487 non-null int64\n",
      "n1000mt_cluster_id      282487 non-null int64\n",
      "num_area_neighbors      282487 non-null int64\n",
      "num_200mt_neighbors     282487 non-null int64\n",
      "num_400mt_neighbors     282487 non-null int64\n",
      "num_1000mt_neighbors    282487 non-null int64\n",
      "day_of_week             282487 non-null object\n",
      "holiday_flg             282487 non-null int64\n",
      "date_seq_id             282487 non-null float64\n",
      "week_seq_id             282487 non-null int64\n",
      "golden_week             282487 non-null int64\n",
      "after_golden_week       282487 non-null int64\n",
      "visit_day               282487 non-null int64\n",
      "visit_month             282487 non-null int64\n",
      "visit_year              282487 non-null int64\n",
      "diff_max_lat            282487 non-null float64\n",
      "diff_min_lat            282487 non-null float64\n",
      "diff_max_long           282487 non-null float64\n",
      "diff_min_long           282487 non-null float64\n",
      "lat_plus_long           282487 non-null float64\n",
      "dtypes: datetime64[ns](1), float64(48), int64(15), object(6)\n",
      "memory usage: 153.0+ MB\n"
     ]
    }
   ],
   "source": [
    "all_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_data = all_data.sort_values(['air_store_id','visit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = all_data[:train_len]\n",
    "test = all_data[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reset_index().drop('index',axis=1)\n",
    "test = test.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250468, 70)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32019, 70)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_encode = ([i for i,j in zip(all_data.dtypes.index,all_data.dtypes.values) if j == 'object'])\n",
    "cols_to_encode.remove('air_store_id')\n",
    "#cols_to_encode.extend(['area_id','n200mt_cluster_id','n400mt_cluster_id','n1000mt_cluster_id'])\n",
    "#cols_to_encode.remove('visit_date')\n",
    "\n",
    "for i in cols_to_encode:\n",
    "    all_data = pd.concat([all_data, pd.get_dummies(all_data[i])],axis=1)\n",
    "    if i not in [\"day_of_week\", 'genre_name', 'area_id','n200mt_cluster_id','n400mt_cluster_id','n1000mt_cluster_id']:\n",
    "        all_data.drop(i, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['genre_name', 'area_name', 'prefecture', 'city', 'day_of_week']\n"
     ]
    }
   ],
   "source": [
    "print(cols_to_encode)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def calc_shifted_ewm(series, alpha, adjust=True, days=0): #\n",
    "    return series.shift(periods=days).ewm(alpha=alpha, adjust=adjust).mean()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "tmp = (train.groupby(['air_store_id', 'day_of_week'])\n",
    "                  .apply(lambda g: calc_shifted_ewm(g['visitors'], 0.1, days=39))\n",
    "                  .reset_index().set_index('level_2').sort_index()\n",
    "                  .rename(columns={'visitors' : 'ewm'})\n",
    "                  .drop(['air_store_id','day_of_week'], axis=1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "tmp = pd.concat([train, tmp], axis=1)[['air_store_id', 'visit_date', 'ewm']]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "all_data['visit_date'] = pd.to_datetime(all_data['visit_date'])\n",
    "tmp['visit_date'] = pd.to_datetime(tmp['visit_date'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tmp['visit_date'] += dt.timedelta(days=39)\n",
    "all_data = pd.merge(all_data, tmp, on=['air_store_id', 'visit_date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reset_index().drop('index',axis=1)\n",
    "test = test.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train min: 2016-01-01 00:00:00\n",
      "Train max:2017-04-22 00:00:00\n",
      "Test min: 2017-04-23 00:00:00\n",
      "Test max:2017-05-31 00:00:00\n",
      "Difference: 39 days 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print(\"Train min: \" + str(train['visit_date'].min()))\n",
    "print(\"Train max:\" + str(train['visit_date'].max()))\n",
    "print(\"Test min: \" + str(test['visit_date'].min()))\n",
    "print(\"Test max:\" + str(test['visit_date'].max()))\n",
    "\n",
    "print(\"Difference: \" + str(test['visit_date'].max() - train['visit_date'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = all_data[:train_len]\n",
    "test = all_data[train_len:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train = train.reset_index().drop('index',axis=1)\n",
    "test = test.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['visit_date'] = pd.to_datetime(all_data['visit_date'])\n",
    "#all_data['visit_date'] = all_data['visit_date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_data; gc.collect();"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "train = train.fillna(np.nan)\n",
    "test = test.fillna(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = train.groupby(by=['air_store_id'])['visit_date'].min().reset_index().rename(columns={'visit_date' : 'days_since_first_obs'})\n",
    "train = pd.merge(train, tmp, on=['air_store_id'], how='left')\n",
    "train['days_since_first_obs'] = (train['visit_date'] - train['days_since_first_obs']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['days_since_first_obs'] > 38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load prepareData.py\n",
    "def prepareData(trainIn, valIn, testIn = None):\n",
    "    stat1 = trainIn.groupby([\"air_store_id\",\"day_of_week\"])['visitors'].sum().reset_index().rename(columns={'visitors':'visitors_sum_store_day'})\n",
    "    stat2 = trainIn.groupby([\"air_store_id\",\"day_of_week\"])['visitors'].mean().reset_index().rename(columns={'visitors':'visitors_mean_store_day'})\n",
    "        \n",
    "    stat3 = trainIn.groupby([\"area_id\",\"day_of_week\"])['visitors'].sum().reset_index().rename(columns={'visitors':'visitors_sum_area'})\n",
    "    stat4 = trainIn.groupby([\"area_id\",\"day_of_week\"])['visitors'].mean().reset_index().rename(columns={'visitors':'visitors_mean_area'})\n",
    "\n",
    "    stat5 = trainIn.groupby([\"n200mt_cluster_id\",\"day_of_week\"])['visitors'].sum().reset_index().rename(columns={'visitors':'visitors_sum_200mt'})\n",
    "    stat6 = trainIn.groupby([\"n200mt_cluster_id\",\"day_of_week\"])['visitors'].mean().reset_index().rename(columns={'visitors':'visitors_mean_200mt'})\n",
    "\n",
    "    stat7 = trainIn.groupby([\"n400mt_cluster_id\",\"day_of_week\"])['visitors'].sum().reset_index().rename(columns={'visitors':'visitors_sum_400mt'})\n",
    "    stat8 = trainIn.groupby([\"n400mt_cluster_id\",\"day_of_week\"])['visitors'].mean().reset_index().rename(columns={'visitors':'visitors_mean_400mt'})\n",
    "    \n",
    "    stat9 = trainIn.groupby([\"n1000mt_cluster_id\",\"day_of_week\"])['visitors'].sum().reset_index().rename(columns={'visitors':'visitors_sum_1000mt'})\n",
    "    stat10 = trainIn.groupby([\"n1000mt_cluster_id\",\"day_of_week\"])['visitors'].mean().reset_index().rename(columns={'visitors':'visitors_mean_1000mt'})\n",
    "    \n",
    "    stat11 = trainIn.groupby([\"n200mt_cluster_id\",\"genre_name\"])['visitors'].sum().reset_index().rename(columns={'visitors':'visitors_sum_genre_200mt'})\n",
    "    stat12 = trainIn.groupby([\"n200mt_cluster_id\",\"genre_name\"])['visitors'].mean().reset_index().rename(columns={'visitors':'visitors_mean_genre_200mt'})\n",
    "    \n",
    "    stat13 = trainIn.groupby([\"n400mt_cluster_id\",\"genre_name\"])['visitors'].sum().reset_index().rename(columns={'visitors':'visitors_sum_genre_400mt'})\n",
    "    stat14 = trainIn.groupby([\"n400mt_cluster_id\",\"genre_name\"])['visitors'].mean().reset_index().rename(columns={'visitors':'visitors_mean_genre_400mt'})\n",
    "    \n",
    "    stat15 = trainIn.groupby([\"n1000mt_cluster_id\",\"genre_name\"])['visitors'].sum().reset_index().rename(columns={'visitors':'visitors_sum_genre_1000mt'})\n",
    "    stat16 = trainIn.groupby([\"n1000mt_cluster_id\",\"genre_name\"])['visitors'].mean().reset_index().rename(columns={'visitors':'visitors_mean_genre_1000mt'})\n",
    "    \n",
    "    #####################################\n",
    "    stat17 = trainIn.groupby([\"air_store_id\",\"day_of_week\",'week_seq_id'])['visitors'].sum().reset_index().rename(columns={'visitors':'visitors_sum_store_day'})\n",
    "    #stat18 = trainIn.groupby([\"air_store_id\",\"day_of_week\",'week_seq_id'])['visitors'].mean().reset_index().rename(columns={'visitors':'visitors_mean_store_day'})\n",
    "    #stat17 = pd.merge(stat17, stat18, on=['air_store_id','day_of_week','week_seq_id'])\n",
    "    #del stat18; gc.collect()\n",
    "    \n",
    "    stat_6wago_day = stat17.copy()\n",
    "    stat_6wago_day.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_6wago_day.loc[:,'week_seq_id'] = stat17.loc[:,'week_seq_id'] + 6\n",
    "    stat_6wago_day = stat_6wago_day.rename(columns={'visitors_sum_store_day' : 'visitors_6wks_wago'})\n",
    "    \n",
    "    stat_7wago_day = stat17.copy()\n",
    "    stat_7wago_day.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_7wago_day.loc[:,'week_seq_id'] = stat17.loc[:,'week_seq_id'] + 7\n",
    "    stat_7wago_day = stat_7wago_day.rename(columns={'visitors_sum_store_day' : 'visitors_7wks_wago'})\n",
    "   \n",
    "    stat_8wago_day = stat17.copy()\n",
    "    stat_8wago_day.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_8wago_day.loc[:,'week_seq_id'] = stat17.loc[:,'week_seq_id'] + 8\n",
    "    stat_8wago_day = stat_8wago_day.rename(columns={'visitors_sum_store_day' : 'visitors_8wks_wago'})\n",
    "\n",
    "    stat_9wago_day = stat17.copy()\n",
    "    stat_9wago_day.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_9wago_day.loc[:,'week_seq_id'] = stat17.loc[:,'week_seq_id'] + 9\n",
    "    stat_9wago_day = stat_9wago_day.rename(columns={'visitors_sum_store_day' : 'visitors_9wks_wago'})\n",
    "    \n",
    "    stat_10wago_day = stat17.copy()\n",
    "    stat_10wago_day.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_10wago_day.loc[:,'week_seq_id'] = stat17.loc[:,'week_seq_id'] + 10\n",
    "    stat_10wago_day = stat_10wago_day.rename(columns={'visitors_sum_store_day' : 'visitors_10wks_wago'})\n",
    "    \n",
    "    #################################\n",
    "    stat18 = trainIn.groupby([\"air_store_id\",\"week_seq_id\"])['visitors'].sum().reset_index().rename(columns={'visitors':'visitors_sum_store_week'})\n",
    "    stat19 = trainIn.groupby([\"air_store_id\",\"week_seq_id\"])['visitors'].mean().reset_index().rename(columns={'visitors':'visitors_mean_store_week'})                  \n",
    "    stat18 = pd.merge(stat18, stat19, on=['air_store_id','week_seq_id'])\n",
    "    del stat19;\n",
    "    \n",
    "    stat_6wago = stat18.copy()\n",
    "    stat_6wago.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_6wago.loc[:,'week_seq_id'] = stat18.loc[:,'week_seq_id'] + 6\n",
    "    stat_6wago = stat_6wago.rename(columns={'visitors_sum_store_week' : 'visitors_sum_store_6wago'})\n",
    "    stat_6wago = stat_6wago.rename(columns={'visitors_mean_store_week' : 'visitors_mean_store_6wago'})\n",
    "    \n",
    "    stat_7wago = stat18.copy()\n",
    "    stat_7wago.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_7wago.loc[:,'week_seq_id'] = stat18.loc[:,'week_seq_id'] + 7\n",
    "    stat_7wago = stat_7wago.rename(columns={'visitors_sum_store_week' : 'visitors_sum_store_7wago'})\n",
    "    stat_7wago = stat_7wago.rename(columns={'visitors_mean_store_week' : 'visitors_mean_store_8wago'})\n",
    "    \n",
    "    stat_8wago = stat18.copy()\n",
    "    stat_8wago.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_8wago.loc[:,'week_seq_id'] = stat18.loc[:,'week_seq_id'] + 8\n",
    "    stat_8wago = stat_8wago.rename(columns={'visitors_sum_store_week' : 'visitors_sum_store_day_6wago'})\n",
    "    stat_8wago = stat_8wago.rename(columns={'visitors_mean_store_week' : 'visitors_mean_store_day_6wago'}) \n",
    "    \n",
    "    stat_9wago = stat18.copy()\n",
    "    stat_9wago.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_9wago.loc[:,'week_seq_id'] = stat18.loc[:,'week_seq_id'] + 9\n",
    "    stat_9wago = stat_9wago.rename(columns={'visitors_sum_store_week' : 'visitors_sum_store_9wago'})\n",
    "    stat_9wago = stat_9wago.rename(columns={'visitors_mean_store_week' : 'visitors_mean_store_9wago'})\n",
    "    \n",
    "    stat_10wago = stat18.copy()\n",
    "    stat_10wago.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_10wago.loc[:,'week_seq_id'] = stat18.loc[:,'week_seq_id'] + 10\n",
    "    stat_10wago = stat_10wago.rename(columns={'visitors_sum_store_week' : 'visitors_sum_store_10wago'})\n",
    "    stat_10wago = stat_10wago.rename(columns={'visitors_mean_store_week' : 'visitors_mean_store_10wago'}) \n",
    "    \n",
    "    ##############################\n",
    "    trainIn = pd.merge(trainIn, stat1, on = [\"air_store_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat2, on = [\"air_store_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat3, on = [\"area_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat4, on = [\"area_id\", \"day_of_week\"], how='left')\n",
    "    \n",
    "    trainIn = pd.merge(trainIn, stat_6wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_7wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_8wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_9wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_10wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    \n",
    "    trainIn = pd.merge(trainIn, stat_6wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_7wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_8wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_9wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_10wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    \n",
    "    trainIn = pd.merge(trainIn, stat5, on = [\"n200mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat6, on = [\"n200mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat7, on = [\"n400mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat8, on = [\"n400mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat9, on = [\"n1000mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat10, on = [\"n1000mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    \n",
    "    trainIn = pd.merge(trainIn, stat11, on = [\"n200mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat12, on = [\"n200mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat13, on = [\"n400mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat14, on = [\"n400mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat15, on = [\"n1000mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat16, on = [\"n1000mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    \n",
    "    ############################\n",
    "    testIn = pd.merge(testIn, stat1, on = [\"air_store_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat2, on = [\"air_store_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat3, on = [\"area_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat4, on = [\"area_id\", \"day_of_week\"], how='left')\n",
    "\n",
    "    testIn = pd.merge(testIn, stat_6wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_7wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_8wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_9wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_10wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "\n",
    "    testIn = pd.merge(testIn, stat_6wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_7wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_8wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_9wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_10wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "\n",
    "    testIn = pd.merge(testIn, stat5, on = [\"n200mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat6, on = [\"n200mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat7, on = [\"n400mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat8, on = [\"n400mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat9, on = [\"n1000mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat10, on = [\"n1000mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "\n",
    "    testIn = pd.merge(testIn, stat11, on = [\"n200mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat12, on = [\"n200mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat13, on = [\"n400mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat14, on = [\"n400mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat15, on = [\"n1000mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat16, on = [\"n1000mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    \n",
    "    ##################################\n",
    "    valIn = pd.merge(valIn, stat1, on = [\"air_store_id\", \"day_of_week\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat2, on = [\"air_store_id\", \"day_of_week\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat3, on = [\"area_id\", \"day_of_week\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat4, on = [\"area_id\", \"day_of_week\"], how='left')\n",
    "    \n",
    "    valIn = pd.merge(valIn, stat_6wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat_7wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat_8wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat_9wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat_10wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "\n",
    "    valIn = pd.merge(valIn, stat_6wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat_7wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat_8wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat_9wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat_10wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    \n",
    "    valIn = pd.merge(valIn, stat5, on = [\"n200mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat6, on = [\"n200mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat7, on = [\"n400mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat8, on = [\"n400mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat9, on = [\"n1000mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat10, on = [\"n1000mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    \n",
    "    valIn = pd.merge(valIn, stat11, on = [\"n200mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat12, on = [\"n200mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat13, on = [\"n400mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat14, on = [\"n400mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat15, on = [\"n1000mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat16, on = [\"n1000mt_cluster_id\", \"genre_name\"], how='left')\n",
    " \n",
    "    return (trainIn, valIn, testIn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [c for c in train if c not in ['id', 'air_store_id', 'visit_date','visitors','days_since_first_obs', 'day_of_week', 'genre_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = train.sort_values(['air_store_id', 'visit_date']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = X[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.log1p(X['visitors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = test[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSLE(y, pred):\n",
    "    return metrics.mean_squared_error(np.log1p(y), np.log1p(pred))**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(37)\n",
    "#tf.set_random_seed(344)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_loops = 3\n",
    "K = 10\n",
    "kf = model_selection.KFold(n_splits = K, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = 0\n",
    "y_train_pred = np.zeros(len(X))\n",
    "#y_train_pred = X['visitors'].copy\n",
    "y_train_pred\n",
    "bestIters = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for a in range(num_loops):\n",
    "        for i, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "        train, val, test2 = prepareData(X.iloc[train_index, :].copy(), X.iloc[val_index, :].copy(), test.copy())\n",
    "        train = train.sort_values(['air_store_id', 'visit_date'])\n",
    "        val = val.sort_values(['air_store_id', 'visit_date'])\n",
    "        test2 = test2.sort_values(['air_store_id', 'visit_date'])\n",
    "        train = train.fillna(0)\n",
    "        val = val.fillna(0)\n",
    "        test2 = test.fillna(0)\n",
    "\n",
    "        X_train, y_train = train[col], np.log1p(train['visitors'])\n",
    "        X_valid, y_valid = val[col], np.log1p(val['visitors'])\n",
    "        print(\"\\nFold \", i)\n",
    "\n",
    "        sc = preprocessing.StandardScaler()\n",
    "        X_train = sc.fit_transform(X_train)\n",
    "        test2 = sc.transform(test2[col])\n",
    "        X_valid = sc.fit_transform(X_valid)\n",
    "        #x_val = np.array(x_valid)\n",
    "        #y_val = np.array(y_valid)\n",
    "\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units = 160 , kernel_initializer = 'normal', input_dim = X_train.shape[1]))\n",
    "        model.add(PReLU())\n",
    "        model.add(Dropout(.2))\n",
    "        model.add(Dense(units = 100 , kernel_initializer = 'normal'))\n",
    "        model.add(PReLU())\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(.2))\n",
    "        model.add(Dense(units = 64 , kernel_initializer = 'normal'))\n",
    "        model.add(PReLU())\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(.1))\n",
    "        model.add(Dense(units = 26, kernel_initializer = 'normal'))\n",
    "        model.add(PReLU())\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(.1))\n",
    "        model.add(Dense(1, kernel_initializer='normal'))\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(lr=1e-2,decay=1e-4), metrics=['mean_squared_error'])#lr=0.1,decay=1e-4\n",
    "\n",
    "        wtpath = 'weights.hdf5'\n",
    "        bestepoch = ModelCheckpoint( filepath=wtpath, verbose=1, save_best_only=True )\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=2, verbose=1) \n",
    "\n",
    "        network_history = (model.fit(X_train, y_train, validation_data = (X_valid, y_valid), epochs=500, \n",
    "              batch_size=256, verbose=True, callbacks=[bestepoch, early_stop])) \n",
    "\n",
    "        val_pred = model.predict(X_valid)\n",
    "        val_pred = [item for sublist in val_pred for item in sublist]\n",
    "        val_pred = np.array(val_pred)\n",
    "        val_pred[val_pred < 0] = 0\n",
    "        val_pred = np.expm1(val_pred)\n",
    "        val_pred[val_pred < 1] = 1\n",
    "        y_train_pred[val_index] += np.array(val_pred) \n",
    "        print('RMSLE Keras Regressor, validation set, fold ', i, ': ', RMSLE(val['visitors'], val_pred))\n",
    "\n",
    "        test_pred = model.predict(test2)\n",
    "        test_pred = [item for sublist in test_pred for item in sublist]\n",
    "        test_pred += np.array(test_pred)\n",
    "        test_pred[test_pred < 0] = 0\n",
    "        test_pred = np.expm1(test_pred)\n",
    "        test_pred[test_pred < 1] = 1\n",
    "        y_test_pred += test_pred\n",
    "\n",
    "        del X_train, X_valid, y_train, y_valid, train, test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred /= (K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('RMSLE Keras, full validtion, fold  ' + str(RMSLE(X['visitors'].values, y_train_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] = test['air_store_id'] + \"_\" + test['visit_date'].dt.date.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['visitors'] = y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission.to_csv('../submissions/submission11_3.csv', float_format='%.6f', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valdf = pd.DataFrame()\n",
    "valdf['air_store_id'] = X['air_store_id']\n",
    "valdf['visit_date'] = X['visit_date']\n",
    "valdf['visitors'] = X['visitors']\n",
    "valdf['prediction'] = y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valdf.to_csv('../submissions/val11_3.csv', float_format='%.6f', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
