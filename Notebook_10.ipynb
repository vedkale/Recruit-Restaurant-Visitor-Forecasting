{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vedpk\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\vedpk\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\vedpk\\Anaconda3\\lib\\site-packages\\sklearn\\learning_curve.py:22: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import glob, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "import datetime as dt\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/train_proc5.csv\")\n",
    "test = pd.read_csv(\"../input/test_proc5.csv\")\n",
    "submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "air_store = pd.read_csv(\"../input/allstore_info_proc1.csv\").rename(columns={'store_id':'air_store_id'})\n",
    "date_info = pd.read_csv(\"../input/date_info.csv\").rename(columns={\"calendar_date\" : 'visit_date'})\n",
    "train_weather = pd.read_csv(\"../input/train_weather_01.csv\")\n",
    "test_weather = pd.read_csv(\"../input/test_weather_01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['visit_date'] = submission['id'].map(lambda x: str(x).split('_')[2])\n",
    "submission['air_store_id'] = submission['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "submission['visit_date'] = pd.to_datetime(submission['visit_date'])\n",
    "submission['visitors'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weather = pd.concat([train_weather, test_weather])\n",
    "all_weather['visit_date'] = pd.to_datetime(all_weather['visit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['visit_date'] = pd.to_datetime(train['visit_date'])\n",
    "test['visit_date'] = pd.to_datetime(test['visit_date'])\n",
    "#test['visitors'] = -1\n",
    "test['visitors'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_info['visit_date'] = pd.to_datetime(date_info['visit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_info['date_seq_id'] = date_info['visit_date'].dt.date - date_info['visit_date'].dt.date.min() + dt.timedelta(1)\n",
    "date_info['date_seq_id'] = date_info['date_seq_id']/dt.timedelta(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_info['week_seq_id'] = ((date_info['date_seq_id']+4)/7).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_to_drop = list(set(train['air_store_id']) - set(test['air_store_id']))\n",
    "train = train.where(~(train['air_store_id'].isin(stores_to_drop)))\n",
    "train = train.dropna(axis=0,subset=['air_store_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([train,test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train; del test;\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.merge(all_data,air_store,how='left',on='air_store_id',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.merge(all_data,date_info,how='left',on='visit_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['visit_month'] = all_data['visit_date'].dt.month\n",
    "all_data['visit_year'] = all_data['visit_date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['diff_max_lat'] = all_data['latitude'].max() - all_data['latitude']\n",
    "all_data['diff_min_lat'] = all_data['latitude'].min() - all_data['latitude']\n",
    "all_data['diff_max_long'] = all_data['longitude'].max() - all_data['longitude']\n",
    "all_data['diff_min_long'] = all_data['longitude'].min() - all_data['longitude']\n",
    "all_data['lat_plus_long'] = all_data['latitude'] + all_data['longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_to_drop = ['air_b2d8bc9c88b85f96',\n",
    " 'air_cf22e368c1a71d53',\n",
    " 'air_229d7e508d9f1b5e',\n",
    " 'air_d0a7bd3339c3d12a',\n",
    " 'air_cb083b4789a8d3a2',\n",
    " 'air_2703dcb33192b181',\n",
    " 'air_0ead98dd07e7a82a',\n",
    " 'air_d63cfa6d6ab78446']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.where(~(all_data['air_store_id'].isin(stores_to_drop)))\n",
    "all_data = all_data.dropna(axis=0,subset=['air_store_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.merge(all_data,all_weather, on=['air_store_id','visit_date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(282487, 73)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del air_store;del date_info;gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['visit_date'] = pd.to_datetime(all_data['visit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['visit_date'] = all_data['visit_date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-01-01\n",
      "2017-05-31\n"
     ]
    }
   ],
   "source": [
    "#train.sort_values('visit_date').iloc[-1]['visit_date']-dt.timedelta(days=90) #38days validation set\n",
    "print(all_data['visit_date'].min())\n",
    "print(all_data['visit_date'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_data = all_data.sort_values(['air_store_id','visit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = all_data[:train_len]\n",
    "test = all_data[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250468, 73)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reset_index().drop('index',axis=1)\n",
    "test = test.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32019, 73)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2017, 5, 31)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.date(2017, 4, 22) + dt.timedelta(days=39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "air_store_id\n",
       "air_7bacc4d36fb094c9     23\n",
       "air_457efe8c3a30ea17     23\n",
       "air_256be208a979e023     24\n",
       "air_c26f027b5bc1f081     25\n",
       "air_901925b628677c2e     25\n",
       "air_3a8a3f8fb5cd7f88     25\n",
       "air_c8265ecc116f2284     25\n",
       "air_35c4732dcbfe31be     25\n",
       "air_646b93e336f0dded     25\n",
       "air_1e665503b8474c55     26\n",
       "air_fff68b929994bfbd     26\n",
       "air_9f277fb7a2c1d842     26\n",
       "air_9e920b758503ef54     27\n",
       "air_2a184c1745274b2b     29\n",
       "air_9a6f6e7f623003d2     29\n",
       "air_382f5ace4e2247b8     30\n",
       "air_22682e965418936f     30\n",
       "air_9d3482b4680aee88     30\n",
       "air_bed603c423b7d9d4     30\n",
       "air_fdcfef8bd859f650     30\n",
       "air_eb20a89bba7dd3d0     31\n",
       "air_931a8a4321b6e7d1     31\n",
       "air_f1094dbf2aef85d9     31\n",
       "air_f85e21e543cf44f2     31\n",
       "air_9dd7d38b0f1760c4     32\n",
       "air_fa4ffc9057812fa2     32\n",
       "air_70e9e8cd55879414     32\n",
       "air_4c665a2bfff0da3b     32\n",
       "air_d54d6fcb116fbed3     32\n",
       "air_caf996ac27206301     34\n",
       "air_aed3a8b49abe4a48     34\n",
       "air_93dd7070c9bf5453     34\n",
       "air_c9f6de13be8b8f25     35\n",
       "air_b8925441167c3152     37\n",
       "air_57013002b912772b     39\n",
       "air_c759b6abeb552160     43\n",
       "air_60aa54ecbc602348     51\n",
       "air_48ffd31594bc3263     51\n",
       "air_f0c7272956e62f12     54\n",
       "air_1f7f8fa557bc0d55     56\n",
       "air_d44d210d2994f01b     63\n",
       "air_25e9888d30b386df     63\n",
       "air_5b704df317ed1962     81\n",
       "air_bbe1c1a47e09f161    122\n",
       "air_a21ffca0bea1661a    138\n",
       "Name: visitors, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['visitors'] == 1].groupby('air_store_id').count()['visitors'].sort_values()[550:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['']\n",
    "cols_to_encode = ([i for i,j in zip(all_data.dtypes.index,all_data.dtypes.values) if j == 'object'])\n",
    "\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for i in cols_to_encode:\n",
    "    all_data[i] = lbl.fit_transform(all_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['air_store_id',\n",
       " 'visit_date',\n",
       " 'genre_name',\n",
       " 'area_name',\n",
       " 'prefecture',\n",
       " 'city',\n",
       " 'day_of_week',\n",
       " 'station_id']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_shifted_ewm(series, alpha, adjust=True, days=0): #\n",
    "    return series.shift(periods=days).ewm(alpha=alpha, adjust=adjust).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = (train.groupby(['air_store_id', 'day_of_week'])\n",
    "                  .apply(lambda g: calc_shifted_ewm(g['visitors'], 0.1, days=39))\n",
    "                  .reset_index().set_index('level_2').sort_index()\n",
    "                  .rename(columns={'visitors' : 'ewm'})\n",
    "                  .drop(['air_store_id','day_of_week'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.concat([train, tmp], axis=1)[['air_store_id', 'visit_date', 'ewm']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_store_id</th>\n",
       "      <th>visit_date</th>\n",
       "      <th>ewm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2016-07-02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2016-07-04</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2016-07-05</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2016-07-06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           air_store_id  visit_date  ewm\n",
       "0  air_00a91d42b08b08d9  2016-07-01  NaN\n",
       "1  air_00a91d42b08b08d9  2016-07-02  NaN\n",
       "2  air_00a91d42b08b08d9  2016-07-04  NaN\n",
       "3  air_00a91d42b08b08d9  2016-07-05  NaN\n",
       "4  air_00a91d42b08b08d9  2016-07-06  NaN"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['visit_date'] = pd.to_datetime(all_data['visit_date'])\n",
    "tmp['visit_date'] = pd.to_datetime(tmp['visit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_data['visit_date'] = all_data['visit_date'].dt.date\n",
    "#tmp['visit_date'] = tmp['visit_date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reset_index().drop('index',axis=1)\n",
    "test = test.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train min: 2016-01-01\n",
      "Train max:2017-04-22\n",
      "Test min: 2017-04-23\n",
      "Test max:2017-05-31\n",
      "tmp min: 2016-01-01 00:00:00\n",
      "tmp max:2017-04-22 00:00:00\n",
      "Difference: 39 days, 0:00:00\n"
     ]
    }
   ],
   "source": [
    "print(\"Train min: \" + str(train['visit_date'].min()))\n",
    "print(\"Train max:\" + str(train['visit_date'].max()))\n",
    "print(\"Test min: \" + str(test['visit_date'].min()))\n",
    "print(\"Test max:\" + str(test['visit_date'].max()))\n",
    "print(\"tmp min: \" + str(tmp['visit_date'].min()))\n",
    "print(\"tmp max:\" + str(tmp['visit_date'].max()))\n",
    "print(\"Difference: \" + str(test['visit_date'].max() - train['visit_date'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp['visit_date'] += dt.timedelta(days=39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 282487 entries, 0 to 282486\n",
      "Data columns (total 73 columns):\n",
      "air_store_id            282487 non-null int64\n",
      "diff_10_9               282487 non-null float64\n",
      "diff_11_10              282487 non-null float64\n",
      "diff_12_11              282487 non-null float64\n",
      "diff_8_7                282474 non-null float64\n",
      "diff_9_8                282481 non-null float64\n",
      "eight_weeks_ago         221561 non-null float64\n",
      "eleven_weeks_ago        207193 non-null float64\n",
      "nine_weeks_ago          216946 non-null float64\n",
      "priorMax                249026 non-null float64\n",
      "priorMean               249026 non-null float64\n",
      "priorMin                249026 non-null float64\n",
      "priorSum                282487 non-null float64\n",
      "seven_weeks_ago         226112 non-null float64\n",
      "six_weeks_ago           230884 non-null float64\n",
      "ten_weeks_ago           211815 non-null float64\n",
      "visit_date              282487 non-null datetime64[ns]\n",
      "visitors                250468 non-null float64\n",
      "vmax_6weekago_10        282487 non-null float64\n",
      "vmax_6weekago_11        282487 non-null float64\n",
      "vmax_6weekago_12        282487 non-null float64\n",
      "vmax_6weekago_7         282474 non-null float64\n",
      "vmax_6weekago_8         282481 non-null float64\n",
      "vmax_6weekago_9         282487 non-null float64\n",
      "vmean_6weekago_10       282487 non-null float64\n",
      "vmean_6weekago_11       282487 non-null float64\n",
      "vmean_6weekago_12       282487 non-null float64\n",
      "vmean_6weekago_7        282474 non-null float64\n",
      "vmean_6weekago_8        282481 non-null float64\n",
      "vmean_6weekago_9        282487 non-null float64\n",
      "vmedian_6weekago_10     282487 non-null float64\n",
      "vmedian_6weekago_11     282487 non-null float64\n",
      "vmedian_6weekago_12     282487 non-null float64\n",
      "vmedian_6weekago_7      282474 non-null float64\n",
      "vmedian_6weekago_8      282481 non-null float64\n",
      "vmedian_6weekago_9      282487 non-null float64\n",
      "vmin_6weekago_10        282487 non-null float64\n",
      "vmin_6weekago_11        282487 non-null float64\n",
      "vmin_6weekago_12        282487 non-null float64\n",
      "vmin_6weekago_7         282474 non-null float64\n",
      "vmin_6weekago_8         282481 non-null float64\n",
      "vmin_6weekago_9         282487 non-null float64\n",
      "latitude                282487 non-null float64\n",
      "longitude               282487 non-null float64\n",
      "genre_name              282487 non-null int64\n",
      "area_name               282487 non-null int64\n",
      "prefecture              282487 non-null int64\n",
      "city                    282487 non-null int64\n",
      "area_id                 282487 non-null int64\n",
      "n200mt_cluster_id       282487 non-null int64\n",
      "n400mt_cluster_id       282487 non-null int64\n",
      "n1000mt_cluster_id      282487 non-null int64\n",
      "num_area_neighbors      282487 non-null int64\n",
      "num_200mt_neighbors     282487 non-null int64\n",
      "num_400mt_neighbors     282487 non-null int64\n",
      "num_1000mt_neighbors    282487 non-null int64\n",
      "day_of_week             282487 non-null int64\n",
      "holiday_flg             282487 non-null int64\n",
      "date_seq_id             282487 non-null float64\n",
      "week_seq_id             282487 non-null int64\n",
      "visit_month             282487 non-null int64\n",
      "visit_year              282487 non-null int64\n",
      "diff_max_lat            282487 non-null float64\n",
      "diff_min_lat            282487 non-null float64\n",
      "diff_max_long           282487 non-null float64\n",
      "diff_min_long           282487 non-null float64\n",
      "lat_plus_long           282487 non-null float64\n",
      "station_id              282487 non-null int64\n",
      "avg_temperature         254589 non-null float64\n",
      "high_temperature        254585 non-null float64\n",
      "low_temperature         254585 non-null float64\n",
      "precipitation           193859 non-null float64\n",
      "hours_sunlight          242400 non-null float64\n",
      "dtypes: datetime64[ns](1), float64(53), int64(19)\n",
      "memory usage: 159.5 MB\n"
     ]
    }
   ],
   "source": [
    "all_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250468, 3)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.merge(all_data, tmp, on=['air_store_id', 'visit_date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = all_data[:train_len]\n",
    "test = all_data[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reset_index().drop('index',axis=1)\n",
    "test = test.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['visit_date'] = pd.to_datetime(all_data['visit_date'])\n",
    "#all_data['visit_date'] = all_data['visit_date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_data; gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diff_8_7                 13\n",
       "diff_9_8                  6\n",
       "eight_weeks_ago        4377\n",
       "eleven_weeks_ago       4589\n",
       "nine_weeks_ago         4439\n",
       "priorMax               1632\n",
       "priorMean              1632\n",
       "priorMin               1632\n",
       "seven_weeks_ago        4395\n",
       "six_weeks_ago          4349\n",
       "ten_weeks_ago          4545\n",
       "visitors              32019\n",
       "vmax_6weekago_7          13\n",
       "vmax_6weekago_8           6\n",
       "vmean_6weekago_7         13\n",
       "vmean_6weekago_8          6\n",
       "vmedian_6weekago_7       13\n",
       "vmedian_6weekago_8        6\n",
       "vmin_6weekago_7          13\n",
       "vmin_6weekago_8           6\n",
       "avg_temperature        3276\n",
       "high_temperature       3276\n",
       "low_temperature        3276\n",
       "precipitation         12730\n",
       "hours_sunlight         4641\n",
       "ewm                   32019\n",
       "dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().sum()[test.isnull().sum()>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['air_store_id', 'diff_10_9', 'diff_11_10', 'diff_12_11', 'diff_8_7',\n",
       "       'diff_9_8', 'eight_weeks_ago', 'eleven_weeks_ago', 'nine_weeks_ago',\n",
       "       'priorMax', 'priorMean', 'priorMin', 'priorSum', 'seven_weeks_ago',\n",
       "       'six_weeks_ago', 'ten_weeks_ago', 'visit_date', 'visitors',\n",
       "       'vmax_6weekago_10', 'vmax_6weekago_11', 'vmax_6weekago_12',\n",
       "       'vmax_6weekago_7', 'vmax_6weekago_8', 'vmax_6weekago_9',\n",
       "       'vmean_6weekago_10', 'vmean_6weekago_11', 'vmean_6weekago_12',\n",
       "       'vmean_6weekago_7', 'vmean_6weekago_8', 'vmean_6weekago_9',\n",
       "       'vmedian_6weekago_10', 'vmedian_6weekago_11', 'vmedian_6weekago_12',\n",
       "       'vmedian_6weekago_7', 'vmedian_6weekago_8', 'vmedian_6weekago_9',\n",
       "       'vmin_6weekago_10', 'vmin_6weekago_11', 'vmin_6weekago_12',\n",
       "       'vmin_6weekago_7', 'vmin_6weekago_8', 'vmin_6weekago_9', 'latitude',\n",
       "       'longitude', 'genre_name', 'area_name', 'prefecture', 'city', 'area_id',\n",
       "       'n200mt_cluster_id', 'n400mt_cluster_id', 'n1000mt_cluster_id',\n",
       "       'num_area_neighbors', 'num_200mt_neighbors', 'num_400mt_neighbors',\n",
       "       'num_1000mt_neighbors', 'day_of_week', 'holiday_flg', 'date_seq_id',\n",
       "       'week_seq_id', 'visit_month', 'visit_year', 'diff_max_lat',\n",
       "       'diff_min_lat', 'diff_max_long', 'diff_min_long', 'lat_plus_long',\n",
       "       'station_id', 'avg_temperature', 'high_temperature', 'low_temperature',\n",
       "       'precipitation', 'hours_sunlight', 'ewm'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['air_store_id', 'diff_10_9', 'diff_11_10', 'diff_12_11', 'diff_8_7',\n",
       "       'diff_9_8', 'eight_weeks_ago', 'eleven_weeks_ago', 'nine_weeks_ago',\n",
       "       'priorMax', 'priorMean', 'priorMin', 'priorSum', 'seven_weeks_ago',\n",
       "       'six_weeks_ago', 'ten_weeks_ago', 'visit_date', 'visitors',\n",
       "       'vmax_6weekago_10', 'vmax_6weekago_11', 'vmax_6weekago_12',\n",
       "       'vmax_6weekago_7', 'vmax_6weekago_8', 'vmax_6weekago_9',\n",
       "       'vmean_6weekago_10', 'vmean_6weekago_11', 'vmean_6weekago_12',\n",
       "       'vmean_6weekago_7', 'vmean_6weekago_8', 'vmean_6weekago_9',\n",
       "       'vmedian_6weekago_10', 'vmedian_6weekago_11', 'vmedian_6weekago_12',\n",
       "       'vmedian_6weekago_7', 'vmedian_6weekago_8', 'vmedian_6weekago_9',\n",
       "       'vmin_6weekago_10', 'vmin_6weekago_11', 'vmin_6weekago_12',\n",
       "       'vmin_6weekago_7', 'vmin_6weekago_8', 'vmin_6weekago_9', 'latitude',\n",
       "       'longitude', 'genre_name', 'area_name', 'prefecture', 'city', 'area_id',\n",
       "       'n200mt_cluster_id', 'n400mt_cluster_id', 'n1000mt_cluster_id',\n",
       "       'num_area_neighbors', 'num_200mt_neighbors', 'num_400mt_neighbors',\n",
       "       'num_1000mt_neighbors', 'day_of_week', 'holiday_flg', 'date_seq_id',\n",
       "       'week_seq_id', 'visit_month', 'visit_year', 'diff_max_lat',\n",
       "       'diff_min_lat', 'diff_max_long', 'diff_min_long', 'lat_plus_long',\n",
       "       'station_id', 'avg_temperature', 'high_temperature', 'low_temperature',\n",
       "       'precipitation', 'hours_sunlight', 'ewm'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = train.fillna(0) ##0.5171185851031399\n",
    "#test = test.fillna(0)\n",
    "train = train.fillna(np.nan) ##0.5185937979429444\n",
    "test = test.fillna(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load prepareData.py\n",
    "def prepareData(trainIn, valIn, testIn = None):\n",
    "    stat1 = trainIn.groupby([\"air_store_id\",\"day_of_week\"],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_store_day'})\n",
    "    stat2 = trainIn.groupby([\"air_store_id\",\"day_of_week\"],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_store_day'})\n",
    "        \n",
    "    stat3 = trainIn.groupby([\"area_id\",\"day_of_week\"],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_area'})\n",
    "    stat4 = trainIn.groupby([\"area_id\",\"day_of_week\"],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_area'})\n",
    "\n",
    "    stat5 = trainIn.groupby([\"n200mt_cluster_id\",\"day_of_week\"],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_200mt'})\n",
    "    stat6 = trainIn.groupby([\"n200mt_cluster_id\",\"day_of_week\"],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_200mt'})\n",
    "\n",
    "    stat7 = trainIn.groupby([\"n400mt_cluster_id\",\"day_of_week\"],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_400mt'})\n",
    "    stat8 = trainIn.groupby([\"n400mt_cluster_id\",\"day_of_week\"],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_400mt'})\n",
    "    \n",
    "    stat9 = trainIn.groupby([\"n1000mt_cluster_id\",\"day_of_week\"],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_1000mt'})\n",
    "    stat10 = trainIn.groupby([\"n1000mt_cluster_id\",\"day_of_week\"],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_1000mt'})\n",
    "    \n",
    "    stat11 = trainIn.groupby([\"n200mt_cluster_id\",\"genre_name\"],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_genre_200mt'})\n",
    "    stat12 = trainIn.groupby([\"n200mt_cluster_id\",\"genre_name\"],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_genre_200mt'})\n",
    "    \n",
    "    stat13 = trainIn.groupby([\"n400mt_cluster_id\",\"genre_name\"],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_genre_400mt'})\n",
    "    stat14 = trainIn.groupby([\"n400mt_cluster_id\",\"genre_name\"],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_genre_400mt'})\n",
    "    \n",
    "    stat15 = trainIn.groupby([\"n1000mt_cluster_id\",\"genre_name\"],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_genre_1000mt'})\n",
    "    stat16 = trainIn.groupby([\"n1000mt_cluster_id\",\"genre_name\"],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_genre_1000mt'})\n",
    "    \n",
    "    #####################################\n",
    "    stat17 = trainIn.groupby([\"air_store_id\",\"day_of_week\",'week_seq_id'],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_store_day'})\n",
    "    #stat18 = trainIn.groupby([\"air_store_id\",\"day_of_week\",'week_seq_id'],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_store_day'})\n",
    "    #stat17 = pd.merge(stat17, stat18, on=['air_store_id','day_of_week','week_seq_id'])\n",
    "    #del stat18; gc.collect()\n",
    "    \n",
    "    stat_6wago_day = stat17.copy()\n",
    "    stat_6wago_day.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_6wago_day.loc[:,'week_seq_id'] = stat17.loc[:,'week_seq_id'] + 6\n",
    "    stat_6wago_day = stat_6wago_day.rename(columns={'visitors_sum_store_day' : 'visitors_6wks_wago'})\n",
    "    \n",
    "    stat_7wago_day = stat17.copy()\n",
    "    stat_7wago_day.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_7wago_day.loc[:,'week_seq_id'] = stat17.loc[:,'week_seq_id'] + 7\n",
    "    stat_7wago_day = stat_7wago_day.rename(columns={'visitors_sum_store_day' : 'visitors_7wks_wago'})\n",
    "   \n",
    "    stat_8wago_day = stat17.copy()\n",
    "    stat_8wago_day.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_8wago_day.loc[:,'week_seq_id'] = stat17.loc[:,'week_seq_id'] + 8\n",
    "    stat_8wago_day = stat_8wago_day.rename(columns={'visitors_sum_store_day' : 'visitors_8wks_wago'})\n",
    "\n",
    "    stat_9wago_day = stat17.copy()\n",
    "    stat_9wago_day.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_9wago_day.loc[:,'week_seq_id'] = stat17.loc[:,'week_seq_id'] + 9\n",
    "    stat_9wago_day = stat_9wago_day.rename(columns={'visitors_sum_store_day' : 'visitors_9wks_wago'})\n",
    "    \n",
    "    stat_10wago_day = stat17.copy()\n",
    "    stat_10wago_day.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_10wago_day.loc[:,'week_seq_id'] = stat17.loc[:,'week_seq_id'] + 10\n",
    "    stat_10wago_day = stat_10wago_day.rename(columns={'visitors_sum_store_day' : 'visitors_10wks_wago'})\n",
    "    \n",
    "    #################################\n",
    "    stat18 = trainIn.groupby([\"air_store_id\",\"week_seq_id\"],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_store_week'})\n",
    "    stat19 = trainIn.groupby([\"air_store_id\",\"week_seq_id\"],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_store_week'})                  \n",
    "    stat18 = pd.merge(stat18, stat19, on=['air_store_id','week_seq_id'])\n",
    "    del stat19;\n",
    "    \n",
    "    stat_6wago = stat18.copy()\n",
    "    stat_6wago.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_6wago.loc[:,'week_seq_id'] = stat18.loc[:,'week_seq_id'] + 6\n",
    "    stat_6wago = stat_6wago.rename(columns={'visitors_sum_store_week' : 'visitors_sum_store_6wago'})\n",
    "    stat_6wago = stat_6wago.rename(columns={'visitors_mean_store_week' : 'visitors_mean_store_6wago'})\n",
    "    \n",
    "    stat_7wago = stat18.copy()\n",
    "    stat_7wago.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_7wago.loc[:,'week_seq_id'] = stat18.loc[:,'week_seq_id'] + 7\n",
    "    stat_7wago = stat_7wago.rename(columns={'visitors_sum_store_week' : 'visitors_sum_store_7wago'})\n",
    "    stat_7wago = stat_7wago.rename(columns={'visitors_mean_store_week' : 'visitors_mean_store_8wago'})\n",
    "    \n",
    "    stat_8wago = stat18.copy()\n",
    "    stat_8wago.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_8wago.loc[:,'week_seq_id'] = stat18.loc[:,'week_seq_id'] + 8\n",
    "    stat_8wago = stat_8wago.rename(columns={'visitors_sum_store_week' : 'visitors_sum_store_day_6wago'})\n",
    "    stat_8wago = stat_8wago.rename(columns={'visitors_mean_store_week' : 'visitors_mean_store_day_6wago'}) \n",
    "    \n",
    "    stat_9wago = stat18.copy()\n",
    "    stat_9wago.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_9wago.loc[:,'week_seq_id'] = stat18.loc[:,'week_seq_id'] + 9\n",
    "    stat_9wago = stat_9wago.rename(columns={'visitors_sum_store_week' : 'visitors_sum_store_9wago'})\n",
    "    stat_9wago = stat_9wago.rename(columns={'visitors_mean_store_week' : 'visitors_mean_store_9wago'})\n",
    "    \n",
    "    stat_10wago = stat18.copy()\n",
    "    stat_10wago.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_10wago.loc[:,'week_seq_id'] = stat18.loc[:,'week_seq_id'] + 10\n",
    "    stat_10wago = stat_10wago.rename(columns={'visitors_sum_store_week' : 'visitors_sum_store_10wago'})\n",
    "    stat_10wago = stat_10wago.rename(columns={'visitors_mean_store_week' : 'visitors_mean_store_10wago'}) \n",
    "    \n",
    "    ##############################\n",
    "    trainIn = pd.merge(trainIn, stat1, on = [\"air_store_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat2, on = [\"air_store_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat3, on = [\"area_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat4, on = [\"area_id\", \"day_of_week\"], how='left')\n",
    "    \n",
    "    trainIn = pd.merge(trainIn, stat_6wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_7wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_8wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_9wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_10wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    \n",
    "    trainIn = pd.merge(trainIn, stat_6wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_7wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_8wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_9wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_10wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    \n",
    "    trainIn = pd.merge(trainIn, stat5, on = [\"n200mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat6, on = [\"n200mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat7, on = [\"n400mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat8, on = [\"n400mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat9, on = [\"n1000mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat10, on = [\"n1000mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    \n",
    "    trainIn = pd.merge(trainIn, stat11, on = [\"n200mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat12, on = [\"n200mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat13, on = [\"n400mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat14, on = [\"n400mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat15, on = [\"n1000mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat16, on = [\"n1000mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    \n",
    "    ############################\n",
    "    testIn = pd.merge(testIn, stat1, on = [\"air_store_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat2, on = [\"air_store_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat3, on = [\"area_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat4, on = [\"area_id\", \"day_of_week\"], how='left')\n",
    "\n",
    "    testIn = pd.merge(testIn, stat_6wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_7wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_8wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_9wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_10wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "\n",
    "    testIn = pd.merge(testIn, stat_6wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_7wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_8wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_9wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_10wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "\n",
    "    testIn = pd.merge(testIn, stat5, on = [\"n200mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat6, on = [\"n200mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat7, on = [\"n400mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat8, on = [\"n400mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat9, on = [\"n1000mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat10, on = [\"n1000mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "\n",
    "    testIn = pd.merge(testIn, stat11, on = [\"n200mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat12, on = [\"n200mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat13, on = [\"n400mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat14, on = [\"n400mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat15, on = [\"n1000mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat16, on = [\"n1000mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    \n",
    "    ##################################\n",
    "    valIn = pd.merge(valIn, stat1, on = [\"air_store_id\", \"day_of_week\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat2, on = [\"air_store_id\", \"day_of_week\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat3, on = [\"area_id\", \"day_of_week\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat4, on = [\"area_id\", \"day_of_week\"], how='left')\n",
    "    \n",
    "    valIn = pd.merge(valIn, stat_6wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat_7wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat_8wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat_9wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat_10wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "\n",
    "    valIn = pd.merge(valIn, stat_6wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat_7wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat_8wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat_9wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat_10wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    \n",
    "    valIn = pd.merge(valIn, stat5, on = [\"n200mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat6, on = [\"n200mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat7, on = [\"n400mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat8, on = [\"n400mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat9, on = [\"n1000mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat10, on = [\"n1000mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    \n",
    "    valIn = pd.merge(valIn, stat11, on = [\"n200mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat12, on = [\"n200mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat13, on = [\"n400mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat14, on = [\"n400mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat15, on = [\"n1000mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat16, on = [\"n1000mt_cluster_id\", \"genre_name\"], how='left')\n",
    " \n",
    "    return (trainIn, valIn, testIn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [c for c in train if c not in ['id', 'air_store_id', 'visit_date','visitors','prefecture','city']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#X = train[train['visit_date'] < dt.date(2017, 2, 21)]\n",
    "#X = train[col].copy()\n",
    "#y = np.log1p(train['visitors']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.sort_values(['air_store_id', 'visit_date']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = test[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "kf = model_selection.KFold(n_splits = K, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSLE(y, pred):\n",
    "    return metrics.mean_squared_error(np.log1p(y), np.log1p(pred))**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = 0\n",
    "y_train_pred = np.zeros(len(X))\n",
    "#y_train_pred = X['visitors'].copy\n",
    "y_train_pred\n",
    "bestIters = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250468, 74)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250468,)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['num_leaves'] = 70,\n",
    "params['metric'] = 'rmse'\n",
    "params['max_depth'] = 8,\n",
    "#params['min_data_in_leaf'] = 20,\n",
    "#params['min_sum_hessian_in_leaf'] = 1e-3,\n",
    "params['learning_rate'] = 0.02,\n",
    "params['num_threads'] = -1, \n",
    "params['feature_fraction'] = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold  0\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\tvalid_0's rmse: 0.532803\n",
      "[200]\tvalid_0's rmse: 0.516248\n",
      "[300]\tvalid_0's rmse: 0.511126\n",
      "[400]\tvalid_0's rmse: 0.508768\n",
      "[500]\tvalid_0's rmse: 0.507496\n",
      "[600]\tvalid_0's rmse: 0.506409\n",
      "[700]\tvalid_0's rmse: 0.505751\n",
      "[800]\tvalid_0's rmse: 0.505293\n",
      "[900]\tvalid_0's rmse: 0.504872\n",
      "[1000]\tvalid_0's rmse: 0.504507\n",
      "[1100]\tvalid_0's rmse: 0.504166\n",
      "[1200]\tvalid_0's rmse: 0.503845\n",
      "[1300]\tvalid_0's rmse: 0.503552\n",
      "[1400]\tvalid_0's rmse: 0.503334\n",
      "[1500]\tvalid_0's rmse: 0.503091\n",
      "[1600]\tvalid_0's rmse: 0.502931\n",
      "[1700]\tvalid_0's rmse: 0.502789\n",
      "[1800]\tvalid_0's rmse: 0.502619\n",
      "[1900]\tvalid_0's rmse: 0.502412\n",
      "[2000]\tvalid_0's rmse: 0.502269\n",
      "[2100]\tvalid_0's rmse: 0.502182\n",
      "[2200]\tvalid_0's rmse: 0.502084\n",
      "[2300]\tvalid_0's rmse: 0.50198\n",
      "[2400]\tvalid_0's rmse: 0.501866\n",
      "[2500]\tvalid_0's rmse: 0.501775\n",
      "[2600]\tvalid_0's rmse: 0.501695\n",
      "[2700]\tvalid_0's rmse: 0.501645\n",
      "[2800]\tvalid_0's rmse: 0.501587\n",
      "Early stopping, best iteration is:\n",
      "[2808]\tvalid_0's rmse: 0.50158\n",
      "RMSLE LGB Regressor, validation set, fold  0 :  0.5015798896893562\n",
      "\n",
      "Fold  1\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\tvalid_0's rmse: 0.535706\n",
      "[200]\tvalid_0's rmse: 0.519336\n",
      "[300]\tvalid_0's rmse: 0.514003\n",
      "[400]\tvalid_0's rmse: 0.511726\n",
      "[500]\tvalid_0's rmse: 0.510373\n",
      "[600]\tvalid_0's rmse: 0.509282\n",
      "[700]\tvalid_0's rmse: 0.508507\n",
      "[800]\tvalid_0's rmse: 0.507892\n",
      "[900]\tvalid_0's rmse: 0.507412\n",
      "[1000]\tvalid_0's rmse: 0.507046\n",
      "[1100]\tvalid_0's rmse: 0.506707\n",
      "[1200]\tvalid_0's rmse: 0.506454\n",
      "[1300]\tvalid_0's rmse: 0.506215\n",
      "[1400]\tvalid_0's rmse: 0.506015\n",
      "[1500]\tvalid_0's rmse: 0.50584\n",
      "[1600]\tvalid_0's rmse: 0.50564\n",
      "[1700]\tvalid_0's rmse: 0.505491\n",
      "[1800]\tvalid_0's rmse: 0.505328\n",
      "[1900]\tvalid_0's rmse: 0.505183\n",
      "[2000]\tvalid_0's rmse: 0.505037\n",
      "[2100]\tvalid_0's rmse: 0.504978\n",
      "[2200]\tvalid_0's rmse: 0.504914\n",
      "Early stopping, best iteration is:\n",
      "[2211]\tvalid_0's rmse: 0.5049\n",
      "RMSLE LGB Regressor, validation set, fold  1 :  0.504897117591847\n",
      "\n",
      "Fold  2\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\tvalid_0's rmse: 0.5323\n",
      "[200]\tvalid_0's rmse: 0.516043\n",
      "[300]\tvalid_0's rmse: 0.511099\n",
      "[400]\tvalid_0's rmse: 0.508677\n",
      "[500]\tvalid_0's rmse: 0.507247\n",
      "[600]\tvalid_0's rmse: 0.506259\n",
      "[700]\tvalid_0's rmse: 0.505554\n",
      "[800]\tvalid_0's rmse: 0.505054\n",
      "[900]\tvalid_0's rmse: 0.504635\n",
      "[1000]\tvalid_0's rmse: 0.504244\n",
      "[1100]\tvalid_0's rmse: 0.50399\n",
      "[1200]\tvalid_0's rmse: 0.503726\n",
      "[1300]\tvalid_0's rmse: 0.503527\n",
      "[1400]\tvalid_0's rmse: 0.503298\n",
      "[1500]\tvalid_0's rmse: 0.503116\n",
      "[1600]\tvalid_0's rmse: 0.502973\n",
      "[1700]\tvalid_0's rmse: 0.502836\n",
      "[1800]\tvalid_0's rmse: 0.502648\n",
      "[1900]\tvalid_0's rmse: 0.502456\n",
      "[2000]\tvalid_0's rmse: 0.502387\n",
      "[2100]\tvalid_0's rmse: 0.5023\n",
      "[2200]\tvalid_0's rmse: 0.502205\n",
      "Early stopping, best iteration is:\n",
      "[2190]\tvalid_0's rmse: 0.502201\n",
      "RMSLE LGB Regressor, validation set, fold  2 :  0.5022007611888284\n",
      "\n",
      "Fold  3\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\tvalid_0's rmse: 0.537808\n",
      "[200]\tvalid_0's rmse: 0.521143\n",
      "[300]\tvalid_0's rmse: 0.515681\n",
      "[400]\tvalid_0's rmse: 0.51332\n",
      "[500]\tvalid_0's rmse: 0.511884\n",
      "[600]\tvalid_0's rmse: 0.511025\n",
      "[700]\tvalid_0's rmse: 0.510339\n",
      "[800]\tvalid_0's rmse: 0.509805\n",
      "[900]\tvalid_0's rmse: 0.509354\n",
      "[1000]\tvalid_0's rmse: 0.509044\n",
      "[1100]\tvalid_0's rmse: 0.508604\n",
      "[1200]\tvalid_0's rmse: 0.508354\n",
      "[1300]\tvalid_0's rmse: 0.508127\n",
      "[1400]\tvalid_0's rmse: 0.507926\n",
      "[1500]\tvalid_0's rmse: 0.507666\n",
      "[1600]\tvalid_0's rmse: 0.507478\n",
      "[1700]\tvalid_0's rmse: 0.507317\n",
      "[1800]\tvalid_0's rmse: 0.507244\n",
      "[1900]\tvalid_0's rmse: 0.507048\n",
      "[2000]\tvalid_0's rmse: 0.506934\n",
      "[2100]\tvalid_0's rmse: 0.506783\n",
      "[2200]\tvalid_0's rmse: 0.506693\n",
      "[2300]\tvalid_0's rmse: 0.506581\n",
      "[2400]\tvalid_0's rmse: 0.506457\n",
      "[2500]\tvalid_0's rmse: 0.506371\n",
      "[2600]\tvalid_0's rmse: 0.506286\n",
      "[2700]\tvalid_0's rmse: 0.506141\n",
      "[2800]\tvalid_0's rmse: 0.506013\n",
      "[2900]\tvalid_0's rmse: 0.505971\n",
      "[3000]\tvalid_0's rmse: 0.505905\n",
      "[3100]\tvalid_0's rmse: 0.505858\n",
      "[3200]\tvalid_0's rmse: 0.505801\n",
      "Early stopping, best iteration is:\n",
      "[3195]\tvalid_0's rmse: 0.505797\n",
      "RMSLE LGB Regressor, validation set, fold  3 :  0.5057943766272428\n",
      "\n",
      "Fold  4\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\tvalid_0's rmse: 0.5311\n",
      "[200]\tvalid_0's rmse: 0.515234\n",
      "[300]\tvalid_0's rmse: 0.509812\n",
      "[400]\tvalid_0's rmse: 0.507328\n",
      "[500]\tvalid_0's rmse: 0.505801\n",
      "[600]\tvalid_0's rmse: 0.504763\n",
      "[700]\tvalid_0's rmse: 0.504039\n",
      "[800]\tvalid_0's rmse: 0.503529\n",
      "[900]\tvalid_0's rmse: 0.503185\n",
      "[1000]\tvalid_0's rmse: 0.502653\n",
      "[1100]\tvalid_0's rmse: 0.502258\n",
      "[1200]\tvalid_0's rmse: 0.501955\n",
      "[1300]\tvalid_0's rmse: 0.50164\n",
      "[1400]\tvalid_0's rmse: 0.501351\n",
      "[1500]\tvalid_0's rmse: 0.50107\n",
      "[1600]\tvalid_0's rmse: 0.500896\n",
      "[1700]\tvalid_0's rmse: 0.500777\n",
      "[1800]\tvalid_0's rmse: 0.5006\n",
      "[1900]\tvalid_0's rmse: 0.500485\n",
      "[2000]\tvalid_0's rmse: 0.500343\n",
      "[2100]\tvalid_0's rmse: 0.500235\n",
      "[2200]\tvalid_0's rmse: 0.500131\n",
      "[2300]\tvalid_0's rmse: 0.500087\n",
      "[2400]\tvalid_0's rmse: 0.50004\n",
      "[2500]\tvalid_0's rmse: 0.499928\n",
      "[2600]\tvalid_0's rmse: 0.499873\n",
      "Early stopping, best iteration is:\n",
      "[2577]\tvalid_0's rmse: 0.499851\n",
      "RMSLE LGB Regressor, validation set, fold  4 :  0.4998499628995148\n"
     ]
    }
   ],
   "source": [
    "for i, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "    params['feature_fraction_seed'] = 17*i+2,\n",
    "    params['bagging_fraction'] = 0.7,\n",
    "    params['bagging_seed'] = 17*i+2\n",
    "    \n",
    "    train, val, test2 = prepareData(X.iloc[train_index, :].copy(), X.iloc[val_index, :].copy(), test.copy())\n",
    "    train = train.sort_values(['air_store_id', 'visit_date'])\n",
    "    val = val.sort_values(['air_store_id', 'visit_date'])\n",
    "    test2 = test2.sort_values(['air_store_id', 'visit_date'])\n",
    "    train = train.fillna(0)\n",
    "    val = val.fillna(0)\n",
    "    test2 = test.fillna(0)\n",
    "    \n",
    "    X_train, y_train = train[col], np.log1p(train['visitors'])\n",
    "    X_valid, y_valid = val[col], np.log1p(val['visitors'])\n",
    "    print(\"\\nFold \", i)\n",
    "    \n",
    "    d_train = lgb.Dataset(X_train[col], y_train)\n",
    "    d_valid = lgb.Dataset(X_valid[col], y_valid)\n",
    "    watchlist = [d_valid]\n",
    "    \n",
    "    model = lgb.train(params, train_set=d_train, num_boost_round=10000, valid_sets=watchlist,\n",
    "                      verbose_eval=100,early_stopping_rounds=50)\n",
    "    bestIter = model.best_iteration\n",
    "    bestIters.append(bestIter)\n",
    "    \n",
    "    val_pred = model.predict(X_valid,num_iteration =bestIter)\n",
    "    val_pred[val_pred < 0] = 0\n",
    "    val_pred = np.expm1(val_pred)\n",
    "    val_pred[val_pred < 1] = 1\n",
    "    y_train_pred[val_index] = np.array(val_pred) \n",
    "    print('RMSLE LGB Regressor, validation set, fold ', i, ': ', RMSLE(val['visitors'], val_pred))\n",
    "\n",
    "    test_pred = model.predict(test2[col],num_iteration =bestIter)\n",
    "    test_pred[test_pred < 0] = 0\n",
    "    test_pred = np.expm1(test_pred)\n",
    "    test_pred[test_pred < 1] = 1\n",
    "    y_test_pred += test_pred\n",
    "\n",
    "    del X_train, X_valid, y_train, y_valid, train, test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred /= K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSLE LGB Regressor, full validtion, fold  0.502869173444738\n"
     ]
    }
   ],
   "source": [
    "print('RMSLE LGB Regressor, full validtion, fold  ' + str(RMSLE(X['visitors'].values, y_train_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] = str(test['air_store_id']) + \"_\" + str(test['visit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['visitors'] = y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('../Submission/submission10.csv', float_format='%.6f', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "gbmMetrics = pd.concat([pd.Series(model.feature_name(), name='featureName'), pd.Series(model.feature_importance(), name='importance')], axis=1)\n",
    "gbmMetrics.sort_values(by='importance', inplace=True, ascending=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(dpi=300)\n",
    "g = sns.barplot(x=\"importance\", y=\"featureName\", data=gbmMetrics)\n",
    "g.figure.set_size_inches(20, 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
