{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "import datetime as dt\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../Input/train_proc2.csv\")\n",
    "test = pd.read_csv(\"../Input/test_proc2.csv\")\n",
    "submission = pd.read_csv(\"../Input/sample_submission.csv\")\n",
    "air_store = pd.read_csv(\"../Input/allstore_info_proc1.csv\").rename(columns={'store_id':'air_store_id'})\n",
    "date_info = pd.read_csv(\"../Input/date_info.csv\").rename(columns={\"calendar_date\" : 'visit_date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['visit_date'] = submission['id'].map(lambda x: str(x).split('_')[2])\n",
    "submission['air_store_id'] = submission['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "submission['visit_date'] = pd.to_datetime(submission['visit_date'])\n",
    "submission['visitors'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['visit_date'] = pd.to_datetime(train['visit_date'])\n",
    "test['visit_date'] = pd.to_datetime(test['visit_date'])\n",
    "test['visitors'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_info['visit_date'] = pd.to_datetime(date_info['visit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_info['date_seq_id'] = date_info['visit_date'].dt.date - date_info['visit_date'].dt.date.min() + dt.timedelta(1)\n",
    "date_info['date_seq_id'] = date_info['date_seq_id']/dt.timedelta(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_info['week_seq_id'] = ((date_info['date_seq_id']+4)/7).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train = train[train['visit_date'] >= dt.date(2016,3,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_to_drop = list(set(train['air_store_id']) - set(test['air_store_id']))\n",
    "train = train.where(~(train['air_store_id'].isin(stores_to_drop)))\n",
    "train = train.dropna(axis=0,subset=['air_store_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([train,test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train; del test;\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.merge(all_data,air_store,how='left',on='air_store_id',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.merge(all_data,date_info,how='left',on='visit_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['visit_month'] = all_data['visit_date'].dt.month\n",
    "all_data['visit_month'] = all_data['visit_date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dont know why this would help ??\n",
    "all_data['diff_max_lat'] = all_data['latitude'].max() - all_data['latitude']\n",
    "all_data['diff_min_lat'] = all_data['latitude'].min() - all_data['latitude']\n",
    "all_data['diff_max_long'] = all_data['longitude'].max() - all_data['longitude']\n",
    "all_data['diff_min_long'] = all_data['longitude'].min() - all_data['longitude']\n",
    "all_data['lat_plus_long'] = all_data['latitude'] + all_data['longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_to_drop = ['air_b2d8bc9c88b85f96',\n",
    " 'air_cf22e368c1a71d53',\n",
    " 'air_229d7e508d9f1b5e',\n",
    " 'air_d0a7bd3339c3d12a',\n",
    " 'air_cb083b4789a8d3a2',\n",
    " 'air_2703dcb33192b181',\n",
    " 'air_0ead98dd07e7a82a',\n",
    " 'air_d63cfa6d6ab78446']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.where(~(all_data['air_store_id'].isin(stores_to_drop)))\n",
    "all_data = all_data.dropna(axis=0,subset=['air_store_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_encode = ([i for i,j in zip(all_data.dtypes.index,all_data.dtypes.values)\n",
    "                 if j == 'object'])\n",
    "#cols_to_encode.remove('visit_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl = preprocessing.LabelEncoder()\n",
    "for i in cols_to_encode:\n",
    "    all_data[i] = lbl.fit_transform(all_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(282487, 107)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = all_data[:train_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250468, 107)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "del air_store;del date_info;del all_data; gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['visit_date'] = pd.to_datetime(train['visit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['visit_date'] = train['visit_date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2017, 1, 22)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sort_values('visit_date').iloc[-1]['visit_date']-dt.timedelta(days=90) #38days validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = train[train['visit_date'] > dt.date(2016, 12, 23)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['visit_date'] <= dt.date(2016, 12, 23)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load prepareData.py\n",
    "def prepareData(trainIn, testIn):\n",
    "    stat1 = trainIn.groupby([\"air_store_id\",\"day_of_week\"],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_store_day'})\n",
    "    stat2 = trainIn.groupby([\"air_store_id\",\"day_of_week\"],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_store_day'})\n",
    "        \n",
    "    stat3 = trainIn.groupby([\"area_id\",\"day_of_week\"],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_area'})\n",
    "    stat4 = trainIn.groupby([\"area_id\",\"day_of_week\"],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_area'})\n",
    "\n",
    "    stat5 = trainIn.groupby([\"n200mt_cluster_id\",\"day_of_week\"],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_200mt'})\n",
    "    stat6 = trainIn.groupby([\"n200mt_cluster_id\",\"day_of_week\"],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_200mt'})\n",
    "\n",
    "    stat7 = trainIn.groupby([\"n400mt_cluster_id\",\"day_of_week\"],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_400mt'})\n",
    "    stat8 = trainIn.groupby([\"n400mt_cluster_id\",\"day_of_week\"],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_400mt'})\n",
    "    \n",
    "    stat9 = trainIn.groupby([\"n1000mt_cluster_id\",\"day_of_week\"],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_1000mt'})\n",
    "    stat10 = trainIn.groupby([\"n1000mt_cluster_id\",\"day_of_week\"],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_1000mt'})\n",
    "    \n",
    "    stat11 = trainIn.groupby([\"n200mt_cluster_id\",\"genre_name\"],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_genre_200mt'})\n",
    "    stat12 = trainIn.groupby([\"n200mt_cluster_id\",\"genre_name\"],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_genre_200mt'})\n",
    "    \n",
    "    stat13 = trainIn.groupby([\"n400mt_cluster_id\",\"genre_name\"],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_genre_400mt'})\n",
    "    stat14 = trainIn.groupby([\"n400mt_cluster_id\",\"genre_name\"],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_genre_400mt'})\n",
    "    \n",
    "    stat15 = trainIn.groupby([\"n1000mt_cluster_id\",\"genre_name\"],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_genre_1000mt'})\n",
    "    stat16 = trainIn.groupby([\"n1000mt_cluster_id\",\"genre_name\"],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_genre_1000mt'})\n",
    "    #####################################\n",
    "    stat17 = trainIn.groupby([\"air_store_id\",\"day_of_week\",'week_seq_id'],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_store_day'})\n",
    "    #stat18 = trainIn.groupby([\"air_store_id\",\"day_of_week\",'week_seq_id'],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_store_day'})\n",
    "    #stat17 = pd.merge(stat17, stat18, on=['air_store_id','day_of_week','week_seq_id'])\n",
    "    #del stat18; gc.collect()\n",
    "    \n",
    "    stat_6wago_day = stat17.copy()\n",
    "    stat_6wago_day.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_6wago_day.loc[:,'week_seq_id'] = stat17.loc[:,'week_seq_id'] + 6\n",
    "    stat_6wago_day = stat_6wago_day.rename(columns={'visitors_sum_store_day' : 'visitors_6wks_wago'})\n",
    "    \n",
    "    stat_7wago_day = stat17.copy()\n",
    "    stat_7wago_day.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_7wago_day.loc[:,'week_seq_id'] = stat17.loc[:,'week_seq_id'] + 7\n",
    "    stat_7wago_day = stat_7wago_day.rename(columns={'visitors_sum_store_day' : 'visitors_7wks_wago'})\n",
    "   \n",
    "    stat_8wago_day = stat17.copy()\n",
    "    stat_8wago_day.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_8wago_day.loc[:,'week_seq_id'] = stat17.loc[:,'week_seq_id'] + 8\n",
    "    stat_8wago_day = stat_8wago_day.rename(columns={'visitors_sum_store_day' : 'visitors_8wks_wago'})\n",
    "\n",
    "    stat_9wago_day = stat17.copy()\n",
    "    stat_9wago_day.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_9wago_day.loc[:,'week_seq_id'] = stat17.loc[:,'week_seq_id'] + 9\n",
    "    stat_9wago_day = stat_9wago_day.rename(columns={'visitors_sum_store_day' : 'visitors_9wks_wago'})\n",
    "    \n",
    "    stat_10wago_day = stat17.copy()\n",
    "    stat_10wago_day.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_10wago_day.loc[:,'week_seq_id'] = stat17.loc[:,'week_seq_id'] + 10\n",
    "    stat_10wago_day = stat_10wago_day.rename(columns={'visitors_sum_store_day' : 'visitors_10wks_wago'})\n",
    "    \n",
    "    #################################\n",
    "    stat18 = trainIn.groupby([\"air_store_id\",\"week_seq_id\"],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_store_week'})\n",
    "    stat19 = trainIn.groupby([\"air_store_id\",\"week_seq_id\"],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_store_week'})                  \n",
    "    stat18 = pd.merge(stat18, stat19, on=['air_store_id','week_seq_id'])\n",
    "    del stat19;\n",
    "    \n",
    "    stat_6wago = stat18.copy()\n",
    "    stat_6wago.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_6wago.loc[:,'week_seq_id'] = stat18.loc[:,'week_seq_id'] + 6\n",
    "    stat_6wago = stat_6wago.rename(columns={'visitors_sum_store_week' : 'visitors_sum_store_6wago'})\n",
    "    stat_6wago = stat_6wago.rename(columns={'visitors_mean_store_week' : 'visitors_mean_store_6wago'})\n",
    "    \n",
    "    stat_7wago = stat18.copy()\n",
    "    stat_7wago.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_7wago.loc[:,'week_seq_id'] = stat18.loc[:,'week_seq_id'] + 7\n",
    "    stat_7wago = stat_7wago.rename(columns={'visitors_sum_store_week' : 'visitors_sum_store_7wago'})\n",
    "    stat_7wago = stat_7wago.rename(columns={'visitors_mean_store_week' : 'visitors_mean_store_8wago'})\n",
    "    \n",
    "    stat_8wago = stat18.copy()\n",
    "    stat_8wago.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_8wago.loc[:,'week_seq_id'] = stat18.loc[:,'week_seq_id'] + 8\n",
    "    stat_8wago = stat_8wago.rename(columns={'visitors_sum_store_week' : 'visitors_sum_store_day_6wago'})\n",
    "    stat_8wago = stat_8wago.rename(columns={'visitors_mean_store_week' : 'visitors_mean_store_day_6wago'}) \n",
    "    \n",
    "    stat_9wago = stat18.copy()\n",
    "    stat_9wago.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_9wago.loc[:,'week_seq_id'] = stat18.loc[:,'week_seq_id'] + 9\n",
    "    stat_9wago = stat_9wago.rename(columns={'visitors_sum_store_week' : 'visitors_sum_store_9wago'})\n",
    "    stat_9wago = stat_9wago.rename(columns={'visitors_mean_store_week' : 'visitors_mean_store_9wago'})\n",
    "    \n",
    "    stat_10wago = stat18.copy()\n",
    "    stat_10wago.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_10wago.loc[:,'week_seq_id'] = stat18.loc[:,'week_seq_id'] + 10\n",
    "    stat_10wago = stat_10wago.rename(columns={'visitors_sum_store_week' : 'visitors_sum_store_10wago'})\n",
    "    stat_10wago = stat_10wago.rename(columns={'visitors_mean_store_week' : 'visitors_mean_store_10wago'}) \n",
    "    ##############################\n",
    "    \n",
    "    trainIn = pd.merge(trainIn, stat1, on = [\"air_store_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat2, on = [\"air_store_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat3, on = [\"area_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat4, on = [\"area_id\", \"day_of_week\"], how='left')\n",
    "    \n",
    "    trainIn = pd.merge(trainIn, stat_6wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_7wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_8wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_9wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_10wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    \n",
    "    trainIn = pd.merge(trainIn, stat_6wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_7wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_8wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_9wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_10wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    \n",
    "    trainIn = pd.merge(trainIn, stat5, on = [\"n200mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat6, on = [\"n200mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat7, on = [\"n400mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat8, on = [\"n400mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat9, on = [\"n1000mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat10, on = [\"n1000mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    \n",
    "    trainIn = pd.merge(trainIn, stat11, on = [\"n200mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat12, on = [\"n200mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat13, on = [\"n400mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat14, on = [\"n400mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat15, on = [\"n1000mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat16, on = [\"n1000mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    \n",
    "    ############################\n",
    "    testIn = pd.merge(testIn, stat1, on = [\"air_store_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat2, on = [\"air_store_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat3, on = [\"area_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat4, on = [\"area_id\", \"day_of_week\"], how='left')\n",
    "    \n",
    "    testIn = pd.merge(testIn, stat_6wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_7wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_8wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_9wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_10wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "\n",
    "    testIn = pd.merge(testIn, stat_6wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_7wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_8wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_9wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_10wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    \n",
    "    testIn = pd.merge(testIn, stat5, on = [\"n200mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat6, on = [\"n200mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat7, on = [\"n400mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat8, on = [\"n400mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat9, on = [\"n1000mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat10, on = [\"n1000mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    \n",
    "    testIn = pd.merge(testIn, stat11, on = [\"n200mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat12, on = [\"n200mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat13, on = [\"n400mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat14, on = [\"n400mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat15, on = [\"n1000mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat16, on = [\"n1000mt_cluster_id\", \"genre_name\"], how='left')\n",
    " \n",
    "    return (trainIn, testIn)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stores = pd.merge(stores, air_store, how='left', on=['air_store_id']) \n",
    "\n",
    "stores['air_genre_name'] = stores['air_genre_name'].map(lambda x: str(str(x).replace('/',' ')))\n",
    "stores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-',' ')))\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for i in range(10):\n",
    "    stores['air_genre_name'+str(i)] = lbl.fit_transform(stores['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "    stores['air_area_name'+str(i)] = lbl.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "stores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\n",
    "stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [c for c in train if c not in ['id', 'air_store_id', 'visit_date','visitors','prefecture','city']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['objective'] = 'reg:linear'\n",
    "params['booster'] = 'gbtree'\n",
    "params['eval_metric'] = 'rmse'\n",
    "#params['eta'] = 0.1\n",
    "params['max_depth'] = 4\n",
    "params['silent'] = 1\n",
    "params['subsample'] = 0.8\n",
    "params['colsample_bytree'] = 0.8\n",
    "params['tree_method'] = \"exact\"\n",
    "\n",
    "#watchlist = [(d_train, 'train'), (d_valid, 'valid')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#X = train[train['visit_date'] < dt.date(2017, 2, 21)]\n",
    "#X = train[col].copy()\n",
    "y = np.log1p(train['visitors']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[col]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "d_train = xgb.DMatrix(X_train,y_train)\n",
    "d_valid = xgb.DMatrix(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "kf = model_selection.KFold(n_splits = K, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSLE(y, pred):\n",
    "    return metrics.mean_squared_error(y, pred)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold  0\n",
      "[0]\ttrain-rmse:1.75378\tvalid-rmse:1.74924\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 1000 rounds.\n",
      "[100]\ttrain-rmse:0.507492\tvalid-rmse:0.526114\n",
      "[200]\ttrain-rmse:0.4949\tvalid-rmse:0.523447\n",
      "[300]\ttrain-rmse:0.485461\tvalid-rmse:0.522683\n",
      "[400]\ttrain-rmse:0.476658\tvalid-rmse:0.522776\n",
      "[500]\ttrain-rmse:0.468939\tvalid-rmse:0.523061\n",
      "[600]\ttrain-rmse:0.461676\tvalid-rmse:0.523687\n",
      "[700]\ttrain-rmse:0.455041\tvalid-rmse:0.524862\n",
      "[800]\ttrain-rmse:0.448543\tvalid-rmse:0.525064\n",
      "[900]\ttrain-rmse:0.442195\tvalid-rmse:0.525733\n",
      "[1000]\ttrain-rmse:0.436329\tvalid-rmse:0.526236\n",
      "[1100]\ttrain-rmse:0.430443\tvalid-rmse:0.527303\n",
      "[1200]\ttrain-rmse:0.424942\tvalid-rmse:0.528283\n",
      "[1300]\ttrain-rmse:0.41955\tvalid-rmse:0.528788\n",
      "Stopping. Best iteration:\n",
      "[382]\ttrain-rmse:0.478159\tvalid-rmse:0.52255\n",
      "\n",
      "RMSLE XGB Regressor, validation set, fold  0 :  0.5291542570499939\n",
      "Prediction length on test set, XGB Regressor, fold  0 :  82302\n",
      "\n",
      "Fold  1\n",
      "[0]\ttrain-rmse:1.75342\tvalid-rmse:1.75158\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 1000 rounds.\n",
      "[100]\ttrain-rmse:0.507859\tvalid-rmse:0.527969\n",
      "[200]\ttrain-rmse:0.495789\tvalid-rmse:0.526268\n",
      "[300]\ttrain-rmse:0.485159\tvalid-rmse:0.524431\n",
      "[400]\ttrain-rmse:0.476815\tvalid-rmse:0.5245\n",
      "[500]\ttrain-rmse:0.468872\tvalid-rmse:0.524768\n",
      "[600]\ttrain-rmse:0.461585\tvalid-rmse:0.525163\n",
      "[700]\ttrain-rmse:0.454557\tvalid-rmse:0.526059\n",
      "[800]\ttrain-rmse:0.448337\tvalid-rmse:0.527188\n",
      "[900]\ttrain-rmse:0.442052\tvalid-rmse:0.527509\n",
      "[1000]\ttrain-rmse:0.436136\tvalid-rmse:0.52814\n",
      "[1100]\ttrain-rmse:0.430596\tvalid-rmse:0.529245\n",
      "[1200]\ttrain-rmse:0.425458\tvalid-rmse:0.529899\n",
      "[1300]\ttrain-rmse:0.420046\tvalid-rmse:0.53053\n",
      "Stopping. Best iteration:\n",
      "[332]\ttrain-rmse:0.482371\tvalid-rmse:0.524181\n",
      "\n",
      "RMSLE XGB Regressor, validation set, fold  1 :  0.5306280520658123\n",
      "Prediction length on test set, XGB Regressor, fold  1 :  82302\n",
      "\n",
      "Fold  2\n",
      "[0]\ttrain-rmse:1.75377\tvalid-rmse:1.75015\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 1000 rounds.\n",
      "[100]\ttrain-rmse:0.509768\tvalid-rmse:0.519325\n",
      "[200]\ttrain-rmse:0.497043\tvalid-rmse:0.516909\n",
      "[300]\ttrain-rmse:0.487435\tvalid-rmse:0.516105\n",
      "[400]\ttrain-rmse:0.478945\tvalid-rmse:0.51663\n",
      "[500]\ttrain-rmse:0.470696\tvalid-rmse:0.516442\n",
      "[600]\ttrain-rmse:0.463364\tvalid-rmse:0.51713\n",
      "[700]\ttrain-rmse:0.456576\tvalid-rmse:0.517881\n",
      "[800]\ttrain-rmse:0.450008\tvalid-rmse:0.518667\n",
      "[900]\ttrain-rmse:0.443869\tvalid-rmse:0.519481\n",
      "[1000]\ttrain-rmse:0.437939\tvalid-rmse:0.519607\n",
      "[1100]\ttrain-rmse:0.432495\tvalid-rmse:0.519925\n",
      "[1200]\ttrain-rmse:0.426912\tvalid-rmse:0.521128\n",
      "[1300]\ttrain-rmse:0.421577\tvalid-rmse:0.521681\n",
      "Stopping. Best iteration:\n",
      "[316]\ttrain-rmse:0.486083\tvalid-rmse:0.516053\n",
      "\n",
      "RMSLE XGB Regressor, validation set, fold  2 :  0.5219366447969225\n",
      "Prediction length on test set, XGB Regressor, fold  2 :  82302\n",
      "\n",
      "Fold  3\n",
      "[0]\ttrain-rmse:1.75365\tvalid-rmse:1.75134\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 1000 rounds.\n",
      "[100]\ttrain-rmse:0.509654\tvalid-rmse:0.518996\n",
      "[200]\ttrain-rmse:0.496354\tvalid-rmse:0.51642\n",
      "[300]\ttrain-rmse:0.486943\tvalid-rmse:0.516166\n",
      "[400]\ttrain-rmse:0.478165\tvalid-rmse:0.516896\n",
      "[500]\ttrain-rmse:0.470334\tvalid-rmse:0.516559\n",
      "[600]\ttrain-rmse:0.462751\tvalid-rmse:0.517542\n",
      "[700]\ttrain-rmse:0.456013\tvalid-rmse:0.518006\n",
      "[800]\ttrain-rmse:0.449781\tvalid-rmse:0.518326\n",
      "[900]\ttrain-rmse:0.443554\tvalid-rmse:0.518958\n",
      "[1000]\ttrain-rmse:0.437574\tvalid-rmse:0.519699\n",
      "[1100]\ttrain-rmse:0.431829\tvalid-rmse:0.520826\n",
      "[1200]\ttrain-rmse:0.426563\tvalid-rmse:0.52152\n",
      "Stopping. Best iteration:\n",
      "[263]\ttrain-rmse:0.490081\tvalid-rmse:0.515755\n",
      "\n",
      "RMSLE XGB Regressor, validation set, fold  3 :  0.5220122098747472\n",
      "Prediction length on test set, XGB Regressor, fold  3 :  82302\n",
      "\n",
      "Fold  4\n",
      "[0]\ttrain-rmse:1.75271\tvalid-rmse:1.76137\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 1000 rounds.\n",
      "[100]\ttrain-rmse:0.509523\tvalid-rmse:0.51745\n",
      "[200]\ttrain-rmse:0.496632\tvalid-rmse:0.514971\n",
      "[300]\ttrain-rmse:0.486794\tvalid-rmse:0.514767\n",
      "[400]\ttrain-rmse:0.478396\tvalid-rmse:0.515239\n",
      "[500]\ttrain-rmse:0.47015\tvalid-rmse:0.514843\n",
      "[600]\ttrain-rmse:0.462524\tvalid-rmse:0.5157\n",
      "[700]\ttrain-rmse:0.45572\tvalid-rmse:0.516386\n",
      "[800]\ttrain-rmse:0.449135\tvalid-rmse:0.517596\n",
      "[900]\ttrain-rmse:0.442983\tvalid-rmse:0.518667\n",
      "[1000]\ttrain-rmse:0.437057\tvalid-rmse:0.519605\n",
      "[1100]\ttrain-rmse:0.431775\tvalid-rmse:0.520181\n",
      "[1200]\ttrain-rmse:0.425855\tvalid-rmse:0.520622\n",
      "Stopping. Best iteration:\n",
      "[249]\ttrain-rmse:0.491384\tvalid-rmse:0.514185\n",
      "\n",
      "RMSLE XGB Regressor, validation set, fold  4 :  0.5207035653140136\n",
      "Prediction length on test set, XGB Regressor, fold  4 :  82302\n",
      "\n",
      "Fold  5\n",
      "[0]\ttrain-rmse:1.75327\tvalid-rmse:1.75176\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 1000 rounds.\n",
      "[100]\ttrain-rmse:0.508625\tvalid-rmse:0.525291\n",
      "[200]\ttrain-rmse:0.496087\tvalid-rmse:0.522895\n",
      "[300]\ttrain-rmse:0.485987\tvalid-rmse:0.522606\n",
      "[400]\ttrain-rmse:0.477296\tvalid-rmse:0.523353\n",
      "[500]\ttrain-rmse:0.469838\tvalid-rmse:0.524325\n",
      "[600]\ttrain-rmse:0.46222\tvalid-rmse:0.523606\n",
      "[700]\ttrain-rmse:0.455241\tvalid-rmse:0.523576\n",
      "[800]\ttrain-rmse:0.449178\tvalid-rmse:0.524228\n",
      "[900]\ttrain-rmse:0.442723\tvalid-rmse:0.52489\n",
      "[1000]\ttrain-rmse:0.436613\tvalid-rmse:0.5254\n",
      "[1100]\ttrain-rmse:0.430796\tvalid-rmse:0.525809\n",
      "[1200]\ttrain-rmse:0.42551\tvalid-rmse:0.526066\n",
      "[1300]\ttrain-rmse:0.420242\tvalid-rmse:0.52671\n",
      "Stopping. Best iteration:\n",
      "[338]\ttrain-rmse:0.482655\tvalid-rmse:0.522283\n",
      "\n",
      "RMSLE XGB Regressor, validation set, fold  5 :  0.5267306849238313\n",
      "Prediction length on test set, XGB Regressor, fold  5 :  82302\n",
      "\n",
      "Fold  6\n",
      "[0]\ttrain-rmse:1.75345\tvalid-rmse:1.7539\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 1000 rounds.\n",
      "[100]\ttrain-rmse:0.509126\tvalid-rmse:0.518605\n",
      "[200]\ttrain-rmse:0.496682\tvalid-rmse:0.51624\n",
      "[300]\ttrain-rmse:0.486412\tvalid-rmse:0.516083\n",
      "[400]\ttrain-rmse:0.477831\tvalid-rmse:0.514881\n",
      "[500]\ttrain-rmse:0.469973\tvalid-rmse:0.515483\n",
      "[600]\ttrain-rmse:0.462449\tvalid-rmse:0.516561\n",
      "[700]\ttrain-rmse:0.455528\tvalid-rmse:0.517321\n",
      "[800]\ttrain-rmse:0.449128\tvalid-rmse:0.517874\n",
      "[900]\ttrain-rmse:0.443299\tvalid-rmse:0.518801\n",
      "[1000]\ttrain-rmse:0.43742\tvalid-rmse:0.52001\n",
      "[1100]\ttrain-rmse:0.431673\tvalid-rmse:0.52078\n",
      "[1200]\ttrain-rmse:0.426203\tvalid-rmse:0.521276\n",
      "[1300]\ttrain-rmse:0.421055\tvalid-rmse:0.521902\n",
      "[1400]\ttrain-rmse:0.415635\tvalid-rmse:0.522244\n",
      "Stopping. Best iteration:\n",
      "[401]\ttrain-rmse:0.477733\tvalid-rmse:0.514789\n",
      "\n",
      "RMSLE XGB Regressor, validation set, fold  6 :  0.5222461230824218\n",
      "Prediction length on test set, XGB Regressor, fold  6 :  82302\n",
      "\n",
      "Fold  7\n",
      "[0]\ttrain-rmse:1.75312\tvalid-rmse:1.75847\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 1000 rounds.\n",
      "[100]\ttrain-rmse:0.510352\tvalid-rmse:0.515068\n",
      "[200]\ttrain-rmse:0.49781\tvalid-rmse:0.511807\n",
      "[300]\ttrain-rmse:0.48749\tvalid-rmse:0.509591\n",
      "[400]\ttrain-rmse:0.479086\tvalid-rmse:0.509212\n",
      "[500]\ttrain-rmse:0.471697\tvalid-rmse:0.509793\n",
      "[600]\ttrain-rmse:0.46443\tvalid-rmse:0.509961\n",
      "[700]\ttrain-rmse:0.457825\tvalid-rmse:0.510175\n",
      "[800]\ttrain-rmse:0.451314\tvalid-rmse:0.510835\n",
      "[900]\ttrain-rmse:0.445173\tvalid-rmse:0.511546\n",
      "[1000]\ttrain-rmse:0.438812\tvalid-rmse:0.512348\n",
      "[1100]\ttrain-rmse:0.432748\tvalid-rmse:0.512833\n",
      "[1200]\ttrain-rmse:0.427071\tvalid-rmse:0.513599\n",
      "[1300]\ttrain-rmse:0.421483\tvalid-rmse:0.513632\n",
      "[1400]\ttrain-rmse:0.416316\tvalid-rmse:0.514493\n",
      "Stopping. Best iteration:\n",
      "[421]\ttrain-rmse:0.477455\tvalid-rmse:0.509071\n",
      "\n",
      "RMSLE XGB Regressor, validation set, fold  7 :  0.5146664376723636\n",
      "Prediction length on test set, XGB Regressor, fold  7 :  82302\n",
      "\n",
      "Fold  8\n",
      "[0]\ttrain-rmse:1.75327\tvalid-rmse:1.75712\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 1000 rounds.\n",
      "[100]\ttrain-rmse:0.508412\tvalid-rmse:0.522373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttrain-rmse:0.495656\tvalid-rmse:0.519684\n",
      "[300]\ttrain-rmse:0.485585\tvalid-rmse:0.519499\n",
      "[400]\ttrain-rmse:0.477277\tvalid-rmse:0.520176\n",
      "[500]\ttrain-rmse:0.469191\tvalid-rmse:0.52117\n",
      "[600]\ttrain-rmse:0.461612\tvalid-rmse:0.522096\n",
      "[700]\ttrain-rmse:0.454766\tvalid-rmse:0.522695\n",
      "[800]\ttrain-rmse:0.448711\tvalid-rmse:0.523204\n",
      "[900]\ttrain-rmse:0.442682\tvalid-rmse:0.524105\n",
      "[1000]\ttrain-rmse:0.436742\tvalid-rmse:0.525413\n",
      "[1100]\ttrain-rmse:0.431146\tvalid-rmse:0.525927\n",
      "[1200]\ttrain-rmse:0.425563\tvalid-rmse:0.526562\n",
      "[1300]\ttrain-rmse:0.420269\tvalid-rmse:0.526665\n",
      "Stopping. Best iteration:\n",
      "[302]\ttrain-rmse:0.485358\tvalid-rmse:0.519395\n",
      "\n",
      "RMSLE XGB Regressor, validation set, fold  8 :  0.5267762266391837\n",
      "Prediction length on test set, XGB Regressor, fold  8 :  82302\n",
      "\n",
      "Fold  9\n",
      "[0]\ttrain-rmse:1.754\tvalid-rmse:1.74987\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 1000 rounds.\n",
      "[100]\ttrain-rmse:0.508642\tvalid-rmse:0.523802\n",
      "[200]\ttrain-rmse:0.496218\tvalid-rmse:0.521462\n",
      "[300]\ttrain-rmse:0.486159\tvalid-rmse:0.520857\n",
      "[400]\ttrain-rmse:0.477652\tvalid-rmse:0.520922\n",
      "[500]\ttrain-rmse:0.469674\tvalid-rmse:0.520979\n",
      "[600]\ttrain-rmse:0.46255\tvalid-rmse:0.521264\n",
      "[700]\ttrain-rmse:0.455848\tvalid-rmse:0.52224\n",
      "[800]\ttrain-rmse:0.448812\tvalid-rmse:0.521845\n",
      "[900]\ttrain-rmse:0.442527\tvalid-rmse:0.523135\n",
      "[1000]\ttrain-rmse:0.436597\tvalid-rmse:0.523678\n",
      "[1100]\ttrain-rmse:0.430931\tvalid-rmse:0.523946\n",
      "[1200]\ttrain-rmse:0.42564\tvalid-rmse:0.524634\n",
      "Stopping. Best iteration:\n",
      "[222]\ttrain-rmse:0.493631\tvalid-rmse:0.520489\n",
      "\n",
      "RMSLE XGB Regressor, validation set, fold  9 :  0.5244377883219812\n",
      "Prediction length on test set, XGB Regressor, fold  9 :  82302\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = 0\n",
    "#K-Fold Validation for xgboost\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    train, val = prepareData(X.iloc[train_index, :].copy(), X.iloc[test_index, :].copy())\n",
    "    # Create data for this fold\n",
    "    #y_train, y_valid = y.iloc[train_index].copy(), y.iloc[test_index]\n",
    "    #X_train, X_valid = .iloc[train_index, :].copy(), X.iloc[test_index, :].copy()\n",
    "    \n",
    "    X_train, y_train = train[col], np.log1p(train['visitors'])\n",
    "    X_valid, y_valid = val[col], np.log1p(val['visitors'])\n",
    "    print(\"\\nFold \", i)\n",
    "    \n",
    "    d_train = xgb.DMatrix(X_train[col],y_train)\n",
    "    d_valid = xgb.DMatrix(X_valid[col], y_valid)\n",
    "    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "    model = (xgb.train(params,d_train,num_boost_round=5000,evals=watchlist,\n",
    "                   early_stopping_rounds=1000,verbose_eval=100))\n",
    "    pred = model.predict(xgb.DMatrix(X_valid))\n",
    "    print('RMSLE XGB Regressor, validation set, fold ', i, ': ', RMSLE(y_valid, pred))\n",
    "\n",
    "    pred = model.predict(xgb.DMatrix(test[col]))\n",
    "    print('Prediction length on test set, XGB Regressor, fold ', i, ': ', len(pred))\n",
    "    y_test_pred += pred\n",
    "\n",
    "    del X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred /= (K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSLE XGB Regressor, full validtion, fold  ' + str(RMSLE(np.log1p(test['visitors']).values, y_test_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
