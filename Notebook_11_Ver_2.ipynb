{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parag/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/parag/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "/home/parag/anaconda3/lib/python3.6/site-packages/sklearn/learning_curve.py:22: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n",
      "/home/parag/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import glob, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "import datetime as dt\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, PReLU, BatchNormalization\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/train_proc5.csv\")\n",
    "test = pd.read_csv(\"../input/test_proc5.csv\")\n",
    "submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "air_store = pd.read_csv(\"../input/allstore_info_proc1.csv\").rename(columns={'store_id':'air_store_id'})\n",
    "date_info = pd.read_csv(\"../input/date_info.csv\").rename(columns={\"calendar_date\" : 'visit_date'})\n",
    "train_weather = pd.read_csv(\"../input/train_weather_01.csv\")\n",
    "test_weather = pd.read_csv(\"../input/test_weather_01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['visit_date'] = submission['id'].map(lambda x: str(x).split('_')[2])\n",
    "submission['air_store_id'] = submission['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "submission['visit_date'] = pd.to_datetime(submission['visit_date'])\n",
    "submission['visitors'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weather = pd.concat([train_weather, test_weather])\n",
    "all_weather['visit_date'] = pd.to_datetime(all_weather['visit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['visit_date'] = pd.to_datetime(train['visit_date'])\n",
    "test['visit_date'] = pd.to_datetime(test['visit_date'])\n",
    "#test['visitors'] = -1\n",
    "test['visitors'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_info['visit_date'] = pd.to_datetime(date_info['visit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_info['date_seq_id'] = date_info['visit_date'].dt.date - date_info['visit_date'].dt.date.min() + dt.timedelta(1)\n",
    "date_info['date_seq_id'] = date_info['date_seq_id']/dt.timedelta(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_info['week_seq_id'] = ((date_info['date_seq_id']+4)/7).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_to_drop = list(set(train['air_store_id']) - set(test['air_store_id']))\n",
    "train = train.where(~(train['air_store_id'].isin(stores_to_drop)))\n",
    "train = train.dropna(axis=0,subset=['air_store_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([train,test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train; del test;\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.merge(all_data,air_store,how='left',on='air_store_id',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.merge(all_data,date_info,how='left',on='visit_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['visit_month'] = all_data['visit_date'].dt.month\n",
    "all_data['visit_year'] = all_data['visit_date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['diff_max_lat'] = all_data['latitude'].max() - all_data['latitude']\n",
    "all_data['diff_min_lat'] = all_data['latitude'].min() - all_data['latitude']\n",
    "all_data['diff_max_long'] = all_data['longitude'].max() - all_data['longitude']\n",
    "all_data['diff_min_long'] = all_data['longitude'].min() - all_data['longitude']\n",
    "all_data['lat_plus_long'] = all_data['latitude'] + all_data['longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_to_drop = ['air_b2d8bc9c88b85f96',\n",
    " 'air_cf22e368c1a71d53',\n",
    " 'air_229d7e508d9f1b5e',\n",
    " 'air_d0a7bd3339c3d12a',\n",
    " 'air_cb083b4789a8d3a2',\n",
    " 'air_2703dcb33192b181',\n",
    " 'air_0ead98dd07e7a82a',\n",
    " 'air_d63cfa6d6ab78446']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.where(~(all_data['air_store_id'].isin(stores_to_drop)))\n",
    "all_data = all_data.dropna(axis=0,subset=['air_store_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.merge(all_data,all_weather, on=['air_store_id','visit_date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(282487, 73)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del air_store;del date_info;gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['visit_date'] = pd.to_datetime(all_data['visit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-01-01 00:00:00\n",
      "2017-05-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "#train.sort_values('visit_date').iloc[-1]['visit_date']-dt.timedelta(days=90) #38days validation set\n",
    "print(all_data['visit_date'].min())\n",
    "print(all_data['visit_date'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_data = all_data.sort_values(['air_store_id','visit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = all_data[:train_len]\n",
    "test = all_data[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250468, 73)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reset_index().drop('index',axis=1)\n",
    "test = test.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32019, 73)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2017, 5, 31)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.date(2017, 4, 22) + dt.timedelta(days=39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Asian', 'Bar/Cocktail', 'Cafe/Sweets', 'Creative cuisine',\n",
      "       'Dining bar', 'International cuisine', 'Italian/French', 'Izakaya',\n",
      "       'Japanese food', 'Karaoke/Party', 'Okonomiyaki/Monja/Teppanyaki',\n",
      "       'Other', 'Western food', 'Yakiniku/Korean food'],\n",
      "      dtype='object') \n",
      "\n",
      "\n",
      "Index(['Fukuoka-ken Fukuoka-shi Daimyō',\n",
      "       'Fukuoka-ken Fukuoka-shi Hakata Ekimae',\n",
      "       'Fukuoka-ken Fukuoka-shi Imaizumi', 'Fukuoka-ken Fukuoka-shi Momochi',\n",
      "       'Fukuoka-ken Fukuoka-shi Shiobaru', 'Fukuoka-ken Fukuoka-shi Takatori',\n",
      "       'Fukuoka-ken Fukuoka-shi Tenjin', 'Fukuoka-ken Fukuoka-shi Torikai',\n",
      "       'Fukuoka-ken Itoshima-shi Maebarunishi',\n",
      "       'Fukuoka-ken Kitakyūshū-shi Konyamachi',\n",
      "       ...\n",
      "       'Ōsaka-fu Sakai-shi Minamikawaramachi', 'Ōsaka-fu Suita-shi Izumichō',\n",
      "       'Ōsaka-fu Ōsaka-shi Fuminosato', 'Ōsaka-fu Ōsaka-shi Kyōmachibori',\n",
      "       'Ōsaka-fu Ōsaka-shi Kyūtarōmachi', 'Ōsaka-fu Ōsaka-shi Nakanochō',\n",
      "       'Ōsaka-fu Ōsaka-shi Nanbasennichimae', 'Ōsaka-fu Ōsaka-shi Shinmachi',\n",
      "       'Ōsaka-fu Ōsaka-shi Ōgimachi', 'Ōsaka-fu Ōsaka-shi Ōhiraki'],\n",
      "      dtype='object', length=103) \n",
      "\n",
      "\n",
      "Index(['Fukuoka-ken', 'Hiroshima-ken', 'Hokkaidō', 'Hyōgo-ken', 'Miyagi-ken',\n",
      "       'Niigata-ken', 'Shizuoka-ken', 'Tōkyō-to', 'Ōsaka-fu'],\n",
      "      dtype='object') \n",
      "\n",
      "\n",
      "Index(['Abashiri-shi', 'Adachi-ku', 'Amagasaki-shi', 'Asahikawa-shi',\n",
      "       'Bunkyō-ku', 'Chiyoda-ku', 'Chūō-ku', 'Edogawa-ku', 'Fuchū-shi',\n",
      "       'Fukuoka-shi', 'Fukuyama-shi', 'Hamamatsu-shi', 'Higashiōsaka-shi',\n",
      "       'Himeji-shi', 'Hiroshima-shi', 'Itabashi-ku', 'Itoshima-shi',\n",
      "       'Kakogawa-shi', 'Kashiwazaki-shi', 'Katsushika-ku', 'Katō-gun',\n",
      "       'Kita-ku', 'Kitakyūshū-shi', 'Koganei-shi', 'Kurume-shi', 'Kōbe-shi',\n",
      "       'Kōtō-ku', 'Machida-shi', 'Meguro-ku', 'Minato-ku', 'Musashino-shi',\n",
      "       'Nagaoka-shi', 'Nakano-ku', 'Nerima-ku', 'Neyagawa-shi', 'Niigata-shi',\n",
      "       'Nishinomiya-shi', 'Numazu-shi', 'Sakai-shi', 'Sapporo-shi',\n",
      "       'Sendai-shi', 'Setagaya-ku', 'Shibuya-ku', 'Shinagawa-ku',\n",
      "       'Shinjuku-ku', 'Shizuoka-shi', 'Suginami-ku', 'Suita-shi',\n",
      "       'Tachikawa-shi', 'Taitō-ku', 'Takarazuka-shi', 'Toshima-ku', 'Yame-shi',\n",
      "       'Ōsaka-shi', 'Ōta-ku'],\n",
      "      dtype='object') \n",
      "\n",
      "\n",
      "Index(['Friday', 'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday',\n",
      "       'Wednesday'],\n",
      "      dtype='object') \n",
      "\n",
      "\n",
      "Index(['fukuoka__fukuoka-kana__fukuoka', 'fukuoka__hakata-kana__hakata',\n",
      "       'fukuoka__kuroki-kana__kuroigi', 'fukuoka__kurume-kana__kurume',\n",
      "       'fukuoka__maehara-kana__maebaru', 'fukuoka__yawata-kana__yahata',\n",
      "       'hiroshima__fukuyama-kana__fukuyama',\n",
      "       'hiroshima__hiroshima-kana__hiroshima',\n",
      "       'hokkaido_ishikari__sapporo-katakana__satporo',\n",
      "       'hokkaido_kamikawa__asahikawa-kana__asahikawa',\n",
      "       'hokkaido_okhotsk__abashiri-kana__abashiri',\n",
      "       'hokkaido_tokachi__komaba-kana__NONE', 'hyogo__akashi-kana__akashi',\n",
      "       'hyogo__himeji-kana__gimpe', 'hyogo__kobe-kana__koube',\n",
      "       'hyogo__nishinomiya-kana__nishinomiya',\n",
      "       'kanagawa__hiyoshi-kana__hiyoshi',\n",
      "       'kanagawa__sagamihara-central-kana__sagamiharasuki',\n",
      "       'miyagi__sendai-kana__sendai', 'niigata__kashiwazaki-kana__kashiwazaki',\n",
      "       'niigata__nagaoka-kana__nagaoka', 'niigata__niigata-kana__niigata',\n",
      "       'osaka__hirakata-kana__hirakata', 'osaka__ikomayama-kana__ikomayama',\n",
      "       'osaka__osaka-kana__osaka', 'osaka__sakai-kana__sakai',\n",
      "       'osaka__toyonaka-kana__toyonaka',\n",
      "       'saitama__tokorozawa-kana__tokorozawa',\n",
      "       'shizuoka__hamamatsu-kana__hamamatsu',\n",
      "       'shizuoka__mishima-kana__mishima', 'shizuoka__shizuoka-kana__shizuoka',\n",
      "       'tokyo__edogawa-seaside-kana__edgawawinkai', 'tokyo__fuchu-kana__fuku',\n",
      "       'tokyo__haneda-kana__haneda', 'tokyo__nerima-kana__nerima',\n",
      "       'tokyo__setagaya-kana__setagaya', 'tokyo__tokyo-kana__tonokyo',\n",
      "       'yamaguchi__shimonoseki-kana__shimonoseki'],\n",
      "      dtype='object') \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols_to_encode = ([i for i,j in zip(all_data.dtypes.index,all_data.dtypes.values) if j == 'object'])\n",
    "cols_to_encode.remove('air_store_id')\n",
    "#cols_to_encode.remove('visit_date')\n",
    "#lbl = preprocessing.LabelEncoder()\n",
    "for i in cols_to_encode:\n",
    "    #all_data[i] = lbl.fit_transform(all_data[i])\n",
    "    all_data = pd.concat([all_data, pd.get_dummies(all_data[i])],axis=1)\n",
    "    if i not in [\"day_of_week\", 'genre_name']:\n",
    "        all_data.drop(i, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['genre_name', 'area_name', 'prefecture', 'city', 'day_of_week', 'station_id']\n"
     ]
    }
   ],
   "source": [
    "print(cols_to_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_shifted_ewm(series, alpha, adjust=True, days=0): #\n",
    "    return series.shift(periods=days).ewm(alpha=alpha, adjust=adjust).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = (train.groupby(['air_store_id', 'day_of_week'])\n",
    "                  .apply(lambda g: calc_shifted_ewm(g['visitors'], 0.1, days=39))\n",
    "                  .reset_index().set_index('level_2').sort_index()\n",
    "                  .rename(columns={'visitors' : 'ewm'})\n",
    "                  .drop(['air_store_id','day_of_week'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.concat([train, tmp], axis=1)[['air_store_id', 'visit_date', 'ewm']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_store_id</th>\n",
       "      <th>visit_date</th>\n",
       "      <th>ewm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2016-07-02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2016-07-04</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2016-07-05</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2016-07-06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           air_store_id visit_date  ewm\n",
       "0  air_00a91d42b08b08d9 2016-07-01  NaN\n",
       "1  air_00a91d42b08b08d9 2016-07-02  NaN\n",
       "2  air_00a91d42b08b08d9 2016-07-04  NaN\n",
       "3  air_00a91d42b08b08d9 2016-07-05  NaN\n",
       "4  air_00a91d42b08b08d9 2016-07-06  NaN"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['visit_date'] = pd.to_datetime(all_data['visit_date'])\n",
    "tmp['visit_date'] = pd.to_datetime(tmp['visit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_data['visit_date'] = all_data['visit_date'].dt.date\n",
    "#tmp['visit_date'] = tmp['visit_date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reset_index().drop('index',axis=1)\n",
    "test = test.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train min: 2016-01-01 00:00:00\n",
      "Train max:2017-04-22 00:00:00\n",
      "Test min: 2017-04-23 00:00:00\n",
      "Test max:2017-05-31 00:00:00\n",
      "tmp min: 2016-01-01 00:00:00\n",
      "tmp max:2017-04-22 00:00:00\n",
      "Difference: 39 days 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print(\"Train min: \" + str(train['visit_date'].min()))\n",
    "print(\"Train max:\" + str(train['visit_date'].max()))\n",
    "print(\"Test min: \" + str(test['visit_date'].min()))\n",
    "print(\"Test max:\" + str(test['visit_date'].max()))\n",
    "print(\"tmp min: \" + str(tmp['visit_date'].min()))\n",
    "print(\"tmp max:\" + str(tmp['visit_date'].max()))\n",
    "print(\"Difference: \" + str(test['visit_date'].max() - train['visit_date'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp['visit_date'] += dt.timedelta(days=39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 282487 entries, 0 to 282486\n",
      "Columns: 295 entries, air_store_id to yamaguchi__shimonoseki-kana__shimonoseki\n",
      "dtypes: datetime64[ns](1), float64(53), int64(12), object(3), uint8(226)\n",
      "memory usage: 211.7+ MB\n"
     ]
    }
   ],
   "source": [
    "all_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250468, 3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_data = pd.merge(all_data, tmp, on=['air_store_id', 'visit_date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = all_data[:train_len]\n",
    "test = all_data[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reset_index().drop('index',axis=1)\n",
    "test = test.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['visit_date'] = pd.to_datetime(all_data['visit_date'])\n",
    "#all_data['visit_date'] = all_data['visit_date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_data; gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diff_8_7                 13\n",
       "diff_9_8                  6\n",
       "eight_weeks_ago        4377\n",
       "eleven_weeks_ago       4589\n",
       "nine_weeks_ago         4439\n",
       "priorMax               1632\n",
       "priorMean              1632\n",
       "priorMin               1632\n",
       "seven_weeks_ago        4395\n",
       "six_weeks_ago          4349\n",
       "ten_weeks_ago          4545\n",
       "visitors              32019\n",
       "vmax_6weekago_7          13\n",
       "vmax_6weekago_8           6\n",
       "vmean_6weekago_7         13\n",
       "vmean_6weekago_8          6\n",
       "vmedian_6weekago_7       13\n",
       "vmedian_6weekago_8        6\n",
       "vmin_6weekago_7          13\n",
       "vmin_6weekago_8           6\n",
       "avg_temperature        3276\n",
       "high_temperature       3276\n",
       "low_temperature        3276\n",
       "precipitation         12730\n",
       "hours_sunlight         4641\n",
       "dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().sum()[test.isnull().sum()>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['air_store_id', 'diff_10_9', 'diff_11_10', 'diff_12_11', 'diff_8_7',\n",
       "       'diff_9_8', 'eight_weeks_ago', 'eleven_weeks_ago', 'nine_weeks_ago',\n",
       "       'priorMax',\n",
       "       ...\n",
       "       'shizuoka__hamamatsu-kana__hamamatsu',\n",
       "       'shizuoka__mishima-kana__mishima', 'shizuoka__shizuoka-kana__shizuoka',\n",
       "       'tokyo__edogawa-seaside-kana__edgawawinkai', 'tokyo__fuchu-kana__fuku',\n",
       "       'tokyo__haneda-kana__haneda', 'tokyo__nerima-kana__nerima',\n",
       "       'tokyo__setagaya-kana__setagaya', 'tokyo__tokyo-kana__tonokyo',\n",
       "       'yamaguchi__shimonoseki-kana__shimonoseki'],\n",
       "      dtype='object', length=295)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['air_store_id', 'diff_10_9', 'diff_11_10', 'diff_12_11', 'diff_8_7',\n",
       "       'diff_9_8', 'eight_weeks_ago', 'eleven_weeks_ago', 'nine_weeks_ago',\n",
       "       'priorMax',\n",
       "       ...\n",
       "       'shizuoka__hamamatsu-kana__hamamatsu',\n",
       "       'shizuoka__mishima-kana__mishima', 'shizuoka__shizuoka-kana__shizuoka',\n",
       "       'tokyo__edogawa-seaside-kana__edgawawinkai', 'tokyo__fuchu-kana__fuku',\n",
       "       'tokyo__haneda-kana__haneda', 'tokyo__nerima-kana__nerima',\n",
       "       'tokyo__setagaya-kana__setagaya', 'tokyo__tokyo-kana__tonokyo',\n",
       "       'yamaguchi__shimonoseki-kana__shimonoseki'],\n",
       "      dtype='object', length=295)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = train.fillna(0) ##0.5171185851031399\n",
    "#test = test.fillna(0)\n",
    "train = train.fillna(np.nan) ##0.5185937979429444\n",
    "test = test.fillna(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load prepareData.py\n",
    "def prepareData(trainIn, valIn, testIn = None):\n",
    "    stat1 = trainIn.groupby([\"air_store_id\",\"day_of_week\"],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_store_day'})\n",
    "    stat2 = trainIn.groupby([\"air_store_id\",\"day_of_week\"],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_store_day'})\n",
    "        \n",
    "    stat3 = trainIn.groupby([\"area_id\",\"day_of_week\"],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_area'})\n",
    "    stat4 = trainIn.groupby([\"area_id\",\"day_of_week\"],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_area'})\n",
    "\n",
    "    stat5 = trainIn.groupby([\"n200mt_cluster_id\",\"day_of_week\"],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_200mt'})\n",
    "    stat6 = trainIn.groupby([\"n200mt_cluster_id\",\"day_of_week\"],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_200mt'})\n",
    "\n",
    "    stat7 = trainIn.groupby([\"n400mt_cluster_id\",\"day_of_week\"],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_400mt'})\n",
    "    stat8 = trainIn.groupby([\"n400mt_cluster_id\",\"day_of_week\"],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_400mt'})\n",
    "    \n",
    "    stat9 = trainIn.groupby([\"n1000mt_cluster_id\",\"day_of_week\"],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_1000mt'})\n",
    "    stat10 = trainIn.groupby([\"n1000mt_cluster_id\",\"day_of_week\"],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_1000mt'})\n",
    "    \n",
    "    stat11 = trainIn.groupby([\"n200mt_cluster_id\",\"genre_name\"],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_genre_200mt'})\n",
    "    stat12 = trainIn.groupby([\"n200mt_cluster_id\",\"genre_name\"],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_genre_200mt'})\n",
    "    \n",
    "    stat13 = trainIn.groupby([\"n400mt_cluster_id\",\"genre_name\"],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_genre_400mt'})\n",
    "    stat14 = trainIn.groupby([\"n400mt_cluster_id\",\"genre_name\"],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_genre_400mt'})\n",
    "    \n",
    "    stat15 = trainIn.groupby([\"n1000mt_cluster_id\",\"genre_name\"],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_genre_1000mt'})\n",
    "    stat16 = trainIn.groupby([\"n1000mt_cluster_id\",\"genre_name\"],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_genre_1000mt'})\n",
    "    \n",
    "    #####################################\n",
    "    stat17 = trainIn.groupby([\"air_store_id\",\"day_of_week\",'week_seq_id'],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_store_day'})\n",
    "    #stat18 = trainIn.groupby([\"air_store_id\",\"day_of_week\",'week_seq_id'],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_store_day'})\n",
    "    #stat17 = pd.merge(stat17, stat18, on=['air_store_id','day_of_week','week_seq_id'])\n",
    "    #del stat18; gc.collect()\n",
    "    \n",
    "    stat_6wago_day = stat17.copy()\n",
    "    stat_6wago_day.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_6wago_day.loc[:,'week_seq_id'] = stat17.loc[:,'week_seq_id'] + 6\n",
    "    stat_6wago_day = stat_6wago_day.rename(columns={'visitors_sum_store_day' : 'visitors_6wks_wago'})\n",
    "    \n",
    "    stat_7wago_day = stat17.copy()\n",
    "    stat_7wago_day.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_7wago_day.loc[:,'week_seq_id'] = stat17.loc[:,'week_seq_id'] + 7\n",
    "    stat_7wago_day = stat_7wago_day.rename(columns={'visitors_sum_store_day' : 'visitors_7wks_wago'})\n",
    "   \n",
    "    stat_8wago_day = stat17.copy()\n",
    "    stat_8wago_day.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_8wago_day.loc[:,'week_seq_id'] = stat17.loc[:,'week_seq_id'] + 8\n",
    "    stat_8wago_day = stat_8wago_day.rename(columns={'visitors_sum_store_day' : 'visitors_8wks_wago'})\n",
    "\n",
    "    stat_9wago_day = stat17.copy()\n",
    "    stat_9wago_day.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_9wago_day.loc[:,'week_seq_id'] = stat17.loc[:,'week_seq_id'] + 9\n",
    "    stat_9wago_day = stat_9wago_day.rename(columns={'visitors_sum_store_day' : 'visitors_9wks_wago'})\n",
    "    \n",
    "    stat_10wago_day = stat17.copy()\n",
    "    stat_10wago_day.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_10wago_day.loc[:,'week_seq_id'] = stat17.loc[:,'week_seq_id'] + 10\n",
    "    stat_10wago_day = stat_10wago_day.rename(columns={'visitors_sum_store_day' : 'visitors_10wks_wago'})\n",
    "    \n",
    "    #################################\n",
    "    stat18 = trainIn.groupby([\"air_store_id\",\"week_seq_id\"],as_index=False)['visitors'].sum().rename(columns={'visitors':'visitors_sum_store_week'})\n",
    "    stat19 = trainIn.groupby([\"air_store_id\",\"week_seq_id\"],as_index=False)['visitors'].mean().rename(columns={'visitors':'visitors_mean_store_week'})                  \n",
    "    stat18 = pd.merge(stat18, stat19, on=['air_store_id','week_seq_id'])\n",
    "    del stat19;\n",
    "    \n",
    "    stat_6wago = stat18.copy()\n",
    "    stat_6wago.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_6wago.loc[:,'week_seq_id'] = stat18.loc[:,'week_seq_id'] + 6\n",
    "    stat_6wago = stat_6wago.rename(columns={'visitors_sum_store_week' : 'visitors_sum_store_6wago'})\n",
    "    stat_6wago = stat_6wago.rename(columns={'visitors_mean_store_week' : 'visitors_mean_store_6wago'})\n",
    "    \n",
    "    stat_7wago = stat18.copy()\n",
    "    stat_7wago.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_7wago.loc[:,'week_seq_id'] = stat18.loc[:,'week_seq_id'] + 7\n",
    "    stat_7wago = stat_7wago.rename(columns={'visitors_sum_store_week' : 'visitors_sum_store_7wago'})\n",
    "    stat_7wago = stat_7wago.rename(columns={'visitors_mean_store_week' : 'visitors_mean_store_8wago'})\n",
    "    \n",
    "    stat_8wago = stat18.copy()\n",
    "    stat_8wago.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_8wago.loc[:,'week_seq_id'] = stat18.loc[:,'week_seq_id'] + 8\n",
    "    stat_8wago = stat_8wago.rename(columns={'visitors_sum_store_week' : 'visitors_sum_store_day_6wago'})\n",
    "    stat_8wago = stat_8wago.rename(columns={'visitors_mean_store_week' : 'visitors_mean_store_day_6wago'}) \n",
    "    \n",
    "    stat_9wago = stat18.copy()\n",
    "    stat_9wago.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_9wago.loc[:,'week_seq_id'] = stat18.loc[:,'week_seq_id'] + 9\n",
    "    stat_9wago = stat_9wago.rename(columns={'visitors_sum_store_week' : 'visitors_sum_store_9wago'})\n",
    "    stat_9wago = stat_9wago.rename(columns={'visitors_mean_store_week' : 'visitors_mean_store_9wago'})\n",
    "    \n",
    "    stat_10wago = stat18.copy()\n",
    "    stat_10wago.loc[:,'week_seq_id'] = np.nan\n",
    "    stat_10wago.loc[:,'week_seq_id'] = stat18.loc[:,'week_seq_id'] + 10\n",
    "    stat_10wago = stat_10wago.rename(columns={'visitors_sum_store_week' : 'visitors_sum_store_10wago'})\n",
    "    stat_10wago = stat_10wago.rename(columns={'visitors_mean_store_week' : 'visitors_mean_store_10wago'}) \n",
    "    \n",
    "    ##############################\n",
    "    trainIn = pd.merge(trainIn, stat1, on = [\"air_store_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat2, on = [\"air_store_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat3, on = [\"area_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat4, on = [\"area_id\", \"day_of_week\"], how='left')\n",
    "    \n",
    "    trainIn = pd.merge(trainIn, stat_6wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_7wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_8wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_9wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_10wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    \n",
    "    trainIn = pd.merge(trainIn, stat_6wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_7wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_8wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_9wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat_10wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    \n",
    "    trainIn = pd.merge(trainIn, stat5, on = [\"n200mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat6, on = [\"n200mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat7, on = [\"n400mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat8, on = [\"n400mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat9, on = [\"n1000mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat10, on = [\"n1000mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    \n",
    "    trainIn = pd.merge(trainIn, stat11, on = [\"n200mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat12, on = [\"n200mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat13, on = [\"n400mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat14, on = [\"n400mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat15, on = [\"n1000mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    trainIn = pd.merge(trainIn, stat16, on = [\"n1000mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    \n",
    "    ############################\n",
    "    testIn = pd.merge(testIn, stat1, on = [\"air_store_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat2, on = [\"air_store_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat3, on = [\"area_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat4, on = [\"area_id\", \"day_of_week\"], how='left')\n",
    "\n",
    "    testIn = pd.merge(testIn, stat_6wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_7wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_8wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_9wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_10wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "\n",
    "    testIn = pd.merge(testIn, stat_6wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_7wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_8wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_9wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat_10wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "\n",
    "    testIn = pd.merge(testIn, stat5, on = [\"n200mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat6, on = [\"n200mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat7, on = [\"n400mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat8, on = [\"n400mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat9, on = [\"n1000mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat10, on = [\"n1000mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "\n",
    "    testIn = pd.merge(testIn, stat11, on = [\"n200mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat12, on = [\"n200mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat13, on = [\"n400mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat14, on = [\"n400mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat15, on = [\"n1000mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    testIn = pd.merge(testIn, stat16, on = [\"n1000mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    \n",
    "    ##################################\n",
    "    valIn = pd.merge(valIn, stat1, on = [\"air_store_id\", \"day_of_week\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat2, on = [\"air_store_id\", \"day_of_week\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat3, on = [\"area_id\", \"day_of_week\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat4, on = [\"area_id\", \"day_of_week\"], how='left')\n",
    "    \n",
    "    valIn = pd.merge(valIn, stat_6wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat_7wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat_8wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat_9wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat_10wago_day, on = [\"air_store_id\", \"day_of_week\", \"week_seq_id\"], how='left')\n",
    "\n",
    "    valIn = pd.merge(valIn, stat_6wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat_7wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat_8wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat_9wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat_10wago, on = [\"air_store_id\", \"week_seq_id\"], how='left')\n",
    "    \n",
    "    valIn = pd.merge(valIn, stat5, on = [\"n200mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat6, on = [\"n200mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat7, on = [\"n400mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat8, on = [\"n400mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat9, on = [\"n1000mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat10, on = [\"n1000mt_cluster_id\", \"day_of_week\"], how='left')\n",
    "    \n",
    "    valIn = pd.merge(valIn, stat11, on = [\"n200mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat12, on = [\"n200mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat13, on = [\"n400mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat14, on = [\"n400mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat15, on = [\"n1000mt_cluster_id\", \"genre_name\"], how='left')\n",
    "    valIn = pd.merge(valIn, stat16, on = [\"n1000mt_cluster_id\", \"genre_name\"], how='left')\n",
    " \n",
    "    return (trainIn, valIn, testIn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [c for c in train if c not in ['id', 'air_store_id', 'visit_date','visitors','visit_date', 'day_of_week', 'genre_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.sort_values(['air_store_id', 'visit_date']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = X[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.log1p(X['visitors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test = test[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "kf = model_selection.KFold(n_splits = K, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSLE(y, pred):\n",
    "    return metrics.mean_squared_error(np.log1p(y), np.log1p(pred))**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = 0\n",
    "y_train_pred = np.zeros(len(X))\n",
    "#y_train_pred = X['visitors'].copy\n",
    "y_train_pred\n",
    "bestIters = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0., ...,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225421, 326)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250468,)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 250468 entries, 0 to 250467\n",
      "Columns: 295 entries, air_store_id to yamaguchi__shimonoseki-kana__shimonoseki\n",
      "dtypes: datetime64[ns](1), float64(53), int64(12), object(3), uint8(226)\n",
      "memory usage: 187.7+ MB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold  0\n",
      "Train on 225421 samples, validate on 25047 samples\n",
      "Epoch 1/100\n",
      "224768/225421 [============================>.] - ETA: 0s - loss: 0.5069 - mean_squared_error: 0.5069Epoch 00001: val_loss improved from inf to 0.28659, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 14s 63us/step - loss: 0.5063 - mean_squared_error: 0.5063 - val_loss: 0.2866 - val_mean_squared_error: 0.2866\n",
      "Epoch 2/100\n",
      "225280/225421 [============================>.] - ETA: 0s - loss: 0.3044 - mean_squared_error: 0.3044Epoch 00002: val_loss improved from 0.28659 to 0.28328, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 11s 50us/step - loss: 0.3045 - mean_squared_error: 0.3045 - val_loss: 0.2833 - val_mean_squared_error: 0.2833\n",
      "Epoch 3/100\n",
      "225024/225421 [============================>.] - ETA: 0s - loss: 0.2935 - mean_squared_error: 0.2935Epoch 00003: val_loss improved from 0.28328 to 0.27506, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 11s 49us/step - loss: 0.2935 - mean_squared_error: 0.2935 - val_loss: 0.2751 - val_mean_squared_error: 0.2751\n",
      "Epoch 4/100\n",
      "225024/225421 [============================>.] - ETA: 0s - loss: 0.2851 - mean_squared_error: 0.2851Epoch 00004: val_loss did not improve\n",
      "225421/225421 [==============================] - 11s 47us/step - loss: 0.2852 - mean_squared_error: 0.2852 - val_loss: 0.2775 - val_mean_squared_error: 0.2775\n",
      "Epoch 5/100\n",
      "224512/225421 [============================>.] - ETA: 0s - loss: 0.2786 - mean_squared_error: 0.2786Epoch 00005: val_loss improved from 0.27506 to 0.27046, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 11s 49us/step - loss: 0.2786 - mean_squared_error: 0.2786 - val_loss: 0.2705 - val_mean_squared_error: 0.2705\n",
      "Epoch 6/100\n",
      "224512/225421 [============================>.] - ETA: 0s - loss: 0.2740 - mean_squared_error: 0.2740Epoch 00006: val_loss did not improve\n",
      "225421/225421 [==============================] - 11s 50us/step - loss: 0.2739 - mean_squared_error: 0.2739 - val_loss: 0.2722 - val_mean_squared_error: 0.2722\n",
      "Epoch 7/100\n",
      "225280/225421 [============================>.] - ETA: 0s - loss: 0.2709 - mean_squared_error: 0.2709Epoch 00007: val_loss improved from 0.27046 to 0.26411, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 11s 51us/step - loss: 0.2710 - mean_squared_error: 0.2710 - val_loss: 0.2641 - val_mean_squared_error: 0.2641\n",
      "Epoch 8/100\n",
      "224512/225421 [============================>.] - ETA: 0s - loss: 0.2693 - mean_squared_error: 0.2693Epoch 00008: val_loss did not improve\n",
      "225421/225421 [==============================] - 11s 51us/step - loss: 0.2693 - mean_squared_error: 0.2693 - val_loss: 0.2670 - val_mean_squared_error: 0.2670\n",
      "Epoch 9/100\n",
      "224512/225421 [============================>.] - ETA: 0s - loss: 0.2670 - mean_squared_error: 0.2670Epoch 00009: val_loss did not improve\n",
      "225421/225421 [==============================] - 12s 53us/step - loss: 0.2670 - mean_squared_error: 0.2670 - val_loss: 0.2654 - val_mean_squared_error: 0.2654\n",
      "Epoch 00009: early stopping\n",
      "RMSLE Keras Regressor, validation set, fold  0 :  0.515184612558\n",
      "\n",
      "Fold  1\n",
      "Train on 225421 samples, validate on 25047 samples\n",
      "Epoch 1/100\n",
      "225280/225421 [============================>.] - ETA: 0s - loss: 0.5100 - mean_squared_error: 0.5100Epoch 00001: val_loss improved from inf to 0.28223, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 14s 63us/step - loss: 0.5099 - mean_squared_error: 0.5099 - val_loss: 0.2822 - val_mean_squared_error: 0.2822\n",
      "Epoch 2/100\n",
      "224768/225421 [============================>.] - ETA: 0s - loss: 0.3058 - mean_squared_error: 0.3058Epoch 00002: val_loss improved from 0.28223 to 0.26887, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 11s 51us/step - loss: 0.3058 - mean_squared_error: 0.3058 - val_loss: 0.2689 - val_mean_squared_error: 0.2689\n",
      "Epoch 3/100\n",
      "224768/225421 [============================>.] - ETA: 0s - loss: 0.2957 - mean_squared_error: 0.2957Epoch 00003: val_loss improved from 0.26887 to 0.26477, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 12s 51us/step - loss: 0.2957 - mean_squared_error: 0.2957 - val_loss: 0.2648 - val_mean_squared_error: 0.2648\n",
      "Epoch 4/100\n",
      "224512/225421 [============================>.] - ETA: 0s - loss: 0.2865 - mean_squared_error: 0.2865Epoch 00004: val_loss did not improve\n",
      "225421/225421 [==============================] - 12s 53us/step - loss: 0.2864 - mean_squared_error: 0.2864 - val_loss: 0.2749 - val_mean_squared_error: 0.2749\n",
      "Epoch 5/100\n",
      "224512/225421 [============================>.] - ETA: 0s - loss: 0.2798 - mean_squared_error: 0.2798Epoch 00005: val_loss improved from 0.26477 to 0.26232, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 12s 53us/step - loss: 0.2797 - mean_squared_error: 0.2797 - val_loss: 0.2623 - val_mean_squared_error: 0.2623\n",
      "Epoch 6/100\n",
      "225280/225421 [============================>.] - ETA: 0s - loss: 0.2750 - mean_squared_error: 0.2750Epoch 00006: val_loss improved from 0.26232 to 0.25920, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 12s 54us/step - loss: 0.2750 - mean_squared_error: 0.2750 - val_loss: 0.2592 - val_mean_squared_error: 0.2592\n",
      "Epoch 7/100\n",
      "224512/225421 [============================>.] - ETA: 0s - loss: 0.2721 - mean_squared_error: 0.2721Epoch 00007: val_loss did not improve\n",
      "225421/225421 [==============================] - 12s 51us/step - loss: 0.2721 - mean_squared_error: 0.2721 - val_loss: 0.2611 - val_mean_squared_error: 0.2611\n",
      "Epoch 8/100\n",
      "224512/225421 [============================>.] - ETA: 0s - loss: 0.2700 - mean_squared_error: 0.2700Epoch 00008: val_loss improved from 0.25920 to 0.25693, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 12s 53us/step - loss: 0.2702 - mean_squared_error: 0.2702 - val_loss: 0.2569 - val_mean_squared_error: 0.2569\n",
      "Epoch 9/100\n",
      "225280/225421 [============================>.] - ETA: 0s - loss: 0.2684 - mean_squared_error: 0.2684Epoch 00009: val_loss improved from 0.25693 to 0.25671, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 12s 53us/step - loss: 0.2684 - mean_squared_error: 0.2684 - val_loss: 0.2567 - val_mean_squared_error: 0.2567\n",
      "Epoch 10/100\n",
      "224768/225421 [============================>.] - ETA: 0s - loss: 0.2667 - mean_squared_error: 0.2667Epoch 00010: val_loss improved from 0.25671 to 0.25486, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 11s 51us/step - loss: 0.2669 - mean_squared_error: 0.2669 - val_loss: 0.2549 - val_mean_squared_error: 0.2549\n",
      "Epoch 11/100\n",
      "224768/225421 [============================>.] - ETA: 0s - loss: 0.2654 - mean_squared_error: 0.2654Epoch 00011: val_loss did not improve\n",
      "225421/225421 [==============================] - 12s 51us/step - loss: 0.2653 - mean_squared_error: 0.2653 - val_loss: 0.2553 - val_mean_squared_error: 0.2553\n",
      "Epoch 12/100\n",
      "224768/225421 [============================>.] - ETA: 0s - loss: 0.2646 - mean_squared_error: 0.2646Epoch 00012: val_loss improved from 0.25486 to 0.25464, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 12s 52us/step - loss: 0.2647 - mean_squared_error: 0.2647 - val_loss: 0.2546 - val_mean_squared_error: 0.2546\n",
      "Epoch 13/100\n",
      "224256/225421 [============================>.] - ETA: 0s - loss: 0.2636 - mean_squared_error: 0.2636Epoch 00013: val_loss improved from 0.25464 to 0.25315, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 11s 49us/step - loss: 0.2635 - mean_squared_error: 0.2635 - val_loss: 0.2531 - val_mean_squared_error: 0.2531\n",
      "Epoch 14/100\n",
      "224512/225421 [============================>.] - ETA: 0s - loss: 0.2619 - mean_squared_error: 0.2619Epoch 00014: val_loss did not improve\n",
      "225421/225421 [==============================] - 10s 45us/step - loss: 0.2619 - mean_squared_error: 0.2619 - val_loss: 0.2551 - val_mean_squared_error: 0.2551\n",
      "Epoch 15/100\n",
      "224768/225421 [============================>.] - ETA: 0s - loss: 0.2620 - mean_squared_error: 0.2620Epoch 00015: val_loss improved from 0.25315 to 0.25285, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 10s 45us/step - loss: 0.2620 - mean_squared_error: 0.2620 - val_loss: 0.2529 - val_mean_squared_error: 0.2529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100\n",
      "225280/225421 [============================>.] - ETA: 0s - loss: 0.2610 - mean_squared_error: 0.2610Epoch 00016: val_loss improved from 0.25285 to 0.25193, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 10s 45us/step - loss: 0.2609 - mean_squared_error: 0.2609 - val_loss: 0.2519 - val_mean_squared_error: 0.2519\n",
      "Epoch 17/100\n",
      "224256/225421 [============================>.] - ETA: 0s - loss: 0.2594 - mean_squared_error: 0.2594Epoch 00017: val_loss did not improve\n",
      "225421/225421 [==============================] - 10s 46us/step - loss: 0.2593 - mean_squared_error: 0.2593 - val_loss: 0.2521 - val_mean_squared_error: 0.2521\n",
      "Epoch 18/100\n",
      "224256/225421 [============================>.] - ETA: 0s - loss: 0.2593 - mean_squared_error: 0.2593Epoch 00018: val_loss improved from 0.25193 to 0.25133, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 10s 45us/step - loss: 0.2594 - mean_squared_error: 0.2594 - val_loss: 0.2513 - val_mean_squared_error: 0.2513\n",
      "Epoch 19/100\n",
      "224768/225421 [============================>.] - ETA: 0s - loss: 0.2585 - mean_squared_error: 0.2585Epoch 00019: val_loss improved from 0.25133 to 0.25121, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 10s 45us/step - loss: 0.2585 - mean_squared_error: 0.2585 - val_loss: 0.2512 - val_mean_squared_error: 0.2512\n",
      "Epoch 20/100\n",
      "225024/225421 [============================>.] - ETA: 0s - loss: 0.2577 - mean_squared_error: 0.2577Epoch 00020: val_loss improved from 0.25121 to 0.25112, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 10s 45us/step - loss: 0.2577 - mean_squared_error: 0.2577 - val_loss: 0.2511 - val_mean_squared_error: 0.2511\n",
      "Epoch 21/100\n",
      "224256/225421 [============================>.] - ETA: 0s - loss: 0.2571 - mean_squared_error: 0.2571Epoch 00021: val_loss improved from 0.25112 to 0.25075, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 10s 45us/step - loss: 0.2571 - mean_squared_error: 0.2571 - val_loss: 0.2507 - val_mean_squared_error: 0.2507\n",
      "Epoch 22/100\n",
      "224768/225421 [============================>.] - ETA: 0s - loss: 0.2561 - mean_squared_error: 0.2561Epoch 00022: val_loss improved from 0.25075 to 0.25004, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 10s 45us/step - loss: 0.2560 - mean_squared_error: 0.2560 - val_loss: 0.2500 - val_mean_squared_error: 0.2500\n",
      "Epoch 23/100\n",
      "224256/225421 [============================>.] - ETA: 0s - loss: 0.2561 - mean_squared_error: 0.2561Epoch 00023: val_loss improved from 0.25004 to 0.24973, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 10s 45us/step - loss: 0.2561 - mean_squared_error: 0.2561 - val_loss: 0.2497 - val_mean_squared_error: 0.2497\n",
      "Epoch 24/100\n",
      "224512/225421 [============================>.] - ETA: 0s - loss: 0.2553 - mean_squared_error: 0.2553Epoch 00024: val_loss improved from 0.24973 to 0.24913, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 10s 45us/step - loss: 0.2552 - mean_squared_error: 0.2552 - val_loss: 0.2491 - val_mean_squared_error: 0.2491\n",
      "Epoch 25/100\n",
      "225024/225421 [============================>.] - ETA: 0s - loss: 0.2551 - mean_squared_error: 0.2551Epoch 00025: val_loss did not improve\n",
      "225421/225421 [==============================] - 10s 45us/step - loss: 0.2551 - mean_squared_error: 0.2551 - val_loss: 0.2506 - val_mean_squared_error: 0.2506\n",
      "Epoch 26/100\n",
      "225024/225421 [============================>.] - ETA: 0s - loss: 0.2549 - mean_squared_error: 0.2549Epoch 00026: val_loss did not improve\n",
      "225421/225421 [==============================] - 10s 45us/step - loss: 0.2550 - mean_squared_error: 0.2550 - val_loss: 0.2515 - val_mean_squared_error: 0.2515\n",
      "Epoch 00026: early stopping\n",
      "RMSLE Keras Regressor, validation set, fold  1 :  0.501484674359\n",
      "\n",
      "Fold  2\n",
      "Train on 225421 samples, validate on 25047 samples\n",
      "Epoch 1/100\n",
      "225024/225421 [============================>.] - ETA: 0s - loss: 0.5046 - mean_squared_error: 0.5046Epoch 00001: val_loss improved from inf to 0.28265, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 13s 59us/step - loss: 0.5042 - mean_squared_error: 0.5042 - val_loss: 0.2826 - val_mean_squared_error: 0.2826\n",
      "Epoch 2/100\n",
      "224768/225421 [============================>.] - ETA: 0s - loss: 0.3063 - mean_squared_error: 0.3063Epoch 00002: val_loss improved from 0.28265 to 0.27367, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 10s 45us/step - loss: 0.3062 - mean_squared_error: 0.3062 - val_loss: 0.2737 - val_mean_squared_error: 0.2737\n",
      "Epoch 3/100\n",
      "225024/225421 [============================>.] - ETA: 0s - loss: 0.2938 - mean_squared_error: 0.2938Epoch 00003: val_loss improved from 0.27367 to 0.26941, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 10s 46us/step - loss: 0.2939 - mean_squared_error: 0.2939 - val_loss: 0.2694 - val_mean_squared_error: 0.2694\n",
      "Epoch 4/100\n",
      "225024/225421 [============================>.] - ETA: 0s - loss: 0.2848 - mean_squared_error: 0.2848Epoch 00004: val_loss improved from 0.26941 to 0.26900, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 10s 46us/step - loss: 0.2847 - mean_squared_error: 0.2847 - val_loss: 0.2690 - val_mean_squared_error: 0.2690\n",
      "Epoch 5/100\n",
      "224768/225421 [============================>.] - ETA: 0s - loss: 0.2786 - mean_squared_error: 0.2786Epoch 00005: val_loss did not improve\n",
      "225421/225421 [==============================] - 10s 46us/step - loss: 0.2787 - mean_squared_error: 0.2787 - val_loss: 0.2723 - val_mean_squared_error: 0.2723\n",
      "Epoch 6/100\n",
      "225280/225421 [============================>.] - ETA: 0s - loss: 0.2741 - mean_squared_error: 0.2741Epoch 00006: val_loss improved from 0.26900 to 0.26181, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 10s 46us/step - loss: 0.2741 - mean_squared_error: 0.2741 - val_loss: 0.2618 - val_mean_squared_error: 0.2618\n",
      "Epoch 7/100\n",
      "225024/225421 [============================>.] - ETA: 0s - loss: 0.2718 - mean_squared_error: 0.2718Epoch 00007: val_loss did not improve\n",
      "225421/225421 [==============================] - 10s 46us/step - loss: 0.2718 - mean_squared_error: 0.2718 - val_loss: 0.2648 - val_mean_squared_error: 0.2648\n",
      "Epoch 8/100\n",
      "224768/225421 [============================>.] - ETA: 0s - loss: 0.2696 - mean_squared_error: 0.2696Epoch 00008: val_loss improved from 0.26181 to 0.26176, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 10s 46us/step - loss: 0.2696 - mean_squared_error: 0.2696 - val_loss: 0.2618 - val_mean_squared_error: 0.2618\n",
      "Epoch 9/100\n",
      "224256/225421 [============================>.] - ETA: 0s - loss: 0.2680 - mean_squared_error: 0.2680Epoch 00009: val_loss improved from 0.26176 to 0.25990, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 10s 46us/step - loss: 0.2681 - mean_squared_error: 0.2681 - val_loss: 0.2599 - val_mean_squared_error: 0.2599\n",
      "Epoch 10/100\n",
      "224768/225421 [============================>.] - ETA: 0s - loss: 0.2666 - mean_squared_error: 0.2666Epoch 00010: val_loss did not improve\n",
      "225421/225421 [==============================] - 10s 46us/step - loss: 0.2666 - mean_squared_error: 0.2666 - val_loss: 0.2599 - val_mean_squared_error: 0.2599\n",
      "Epoch 11/100\n",
      "225280/225421 [============================>.] - ETA: 0s - loss: 0.2652 - mean_squared_error: 0.2652Epoch 00011: val_loss improved from 0.25990 to 0.25920, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 10s 46us/step - loss: 0.2652 - mean_squared_error: 0.2652 - val_loss: 0.2592 - val_mean_squared_error: 0.2592\n",
      "Epoch 12/100\n",
      "224256/225421 [============================>.] - ETA: 0s - loss: 0.2637 - mean_squared_error: 0.2637Epoch 00012: val_loss improved from 0.25920 to 0.25763, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 10s 46us/step - loss: 0.2639 - mean_squared_error: 0.2639 - val_loss: 0.2576 - val_mean_squared_error: 0.2576\n",
      "Epoch 13/100\n",
      "224512/225421 [============================>.] - ETA: 0s - loss: 0.2635 - mean_squared_error: 0.2635Epoch 00013: val_loss did not improve\n",
      "225421/225421 [==============================] - 10s 46us/step - loss: 0.2635 - mean_squared_error: 0.2635 - val_loss: 0.2580 - val_mean_squared_error: 0.2580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100\n",
      "224512/225421 [============================>.] - ETA: 0s - loss: 0.2616 - mean_squared_error: 0.2616Epoch 00014: val_loss did not improve\n",
      "225421/225421 [==============================] - 10s 46us/step - loss: 0.2617 - mean_squared_error: 0.2617 - val_loss: 0.2636 - val_mean_squared_error: 0.2636\n",
      "Epoch 00014: early stopping\n",
      "RMSLE Keras Regressor, validation set, fold  2 :  0.513437786821\n",
      "\n",
      "Fold  3\n",
      "Train on 225421 samples, validate on 25047 samples\n",
      "Epoch 1/100\n",
      "224512/225421 [============================>.] - ETA: 0s - loss: 0.5159 - mean_squared_error: 0.5159Epoch 00001: val_loss improved from inf to 0.27976, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 13s 60us/step - loss: 0.5150 - mean_squared_error: 0.5150 - val_loss: 0.2798 - val_mean_squared_error: 0.2798\n",
      "Epoch 2/100\n",
      "225280/225421 [============================>.] - ETA: 0s - loss: 0.3057 - mean_squared_error: 0.3057Epoch 00002: val_loss improved from 0.27976 to 0.27507, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 10s 46us/step - loss: 0.3057 - mean_squared_error: 0.3057 - val_loss: 0.2751 - val_mean_squared_error: 0.2751\n",
      "Epoch 3/100\n",
      "224512/225421 [============================>.] - ETA: 0s - loss: 0.2949 - mean_squared_error: 0.2949Epoch 00003: val_loss improved from 0.27507 to 0.27159, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 10s 46us/step - loss: 0.2947 - mean_squared_error: 0.2947 - val_loss: 0.2716 - val_mean_squared_error: 0.2716\n",
      "Epoch 4/100\n",
      "224768/225421 [============================>.] - ETA: 0s - loss: 0.2854 - mean_squared_error: 0.2854Epoch 00004: val_loss improved from 0.27159 to 0.26670, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 10s 45us/step - loss: 0.2853 - mean_squared_error: 0.2853 - val_loss: 0.2667 - val_mean_squared_error: 0.2667\n",
      "Epoch 5/100\n",
      "224256/225421 [============================>.] - ETA: 0s - loss: 0.2787 - mean_squared_error: 0.2787Epoch 00005: val_loss did not improve\n",
      "225421/225421 [==============================] - 10s 45us/step - loss: 0.2788 - mean_squared_error: 0.2788 - val_loss: 0.2693 - val_mean_squared_error: 0.2693\n",
      "Epoch 6/100\n",
      "225280/225421 [============================>.] - ETA: 0s - loss: 0.2747 - mean_squared_error: 0.2747Epoch 00006: val_loss did not improve\n",
      "225421/225421 [==============================] - 10s 45us/step - loss: 0.2747 - mean_squared_error: 0.2747 - val_loss: 0.2684 - val_mean_squared_error: 0.2684\n",
      "Epoch 00006: early stopping\n",
      "RMSLE Keras Regressor, validation set, fold  3 :  0.518064099167\n",
      "\n",
      "Fold  4\n",
      "Train on 225421 samples, validate on 25047 samples\n",
      "Epoch 1/100\n",
      "225280/225421 [============================>.] - ETA: 0s - loss: 0.5049 - mean_squared_error: 0.5049Epoch 00001: val_loss improved from inf to 0.28236, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 14s 61us/step - loss: 0.5048 - mean_squared_error: 0.5048 - val_loss: 0.2824 - val_mean_squared_error: 0.2824\n",
      "Epoch 2/100\n",
      "224256/225421 [============================>.] - ETA: 0s - loss: 0.3067 - mean_squared_error: 0.3067Epoch 00002: val_loss improved from 0.28236 to 0.27153, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 10s 45us/step - loss: 0.3067 - mean_squared_error: 0.3067 - val_loss: 0.2715 - val_mean_squared_error: 0.2715\n",
      "Epoch 3/100\n",
      "224768/225421 [============================>.] - ETA: 0s - loss: 0.2941 - mean_squared_error: 0.2941Epoch 00003: val_loss improved from 0.27153 to 0.26970, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 10s 45us/step - loss: 0.2941 - mean_squared_error: 0.2941 - val_loss: 0.2697 - val_mean_squared_error: 0.2697\n",
      "Epoch 4/100\n",
      "225024/225421 [============================>.] - ETA: 0s - loss: 0.2852 - mean_squared_error: 0.2852Epoch 00004: val_loss did not improve\n",
      "225421/225421 [==============================] - 10s 45us/step - loss: 0.2851 - mean_squared_error: 0.2851 - val_loss: 0.2711 - val_mean_squared_error: 0.2711\n",
      "Epoch 5/100\n",
      "224256/225421 [============================>.] - ETA: 0s - loss: 0.2793 - mean_squared_error: 0.2793Epoch 00005: val_loss improved from 0.26970 to 0.26217, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 10s 46us/step - loss: 0.2792 - mean_squared_error: 0.2792 - val_loss: 0.2622 - val_mean_squared_error: 0.2622\n",
      "Epoch 6/100\n",
      "224256/225421 [============================>.] - ETA: 0s - loss: 0.2742 - mean_squared_error: 0.2742Epoch 00006: val_loss improved from 0.26217 to 0.26169, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 10s 45us/step - loss: 0.2742 - mean_squared_error: 0.2742 - val_loss: 0.2617 - val_mean_squared_error: 0.2617\n",
      "Epoch 7/100\n",
      "225024/225421 [============================>.] - ETA: 0s - loss: 0.2715 - mean_squared_error: 0.2715Epoch 00007: val_loss improved from 0.26169 to 0.25875, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 10s 45us/step - loss: 0.2715 - mean_squared_error: 0.2715 - val_loss: 0.2588 - val_mean_squared_error: 0.2588\n",
      "Epoch 8/100\n",
      "225024/225421 [============================>.] - ETA: 0s - loss: 0.2698 - mean_squared_error: 0.2698Epoch 00008: val_loss improved from 0.25875 to 0.25729, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 10s 45us/step - loss: 0.2697 - mean_squared_error: 0.2697 - val_loss: 0.2573 - val_mean_squared_error: 0.2573\n",
      "Epoch 9/100\n",
      "225024/225421 [============================>.] - ETA: 0s - loss: 0.2679 - mean_squared_error: 0.2679Epoch 00009: val_loss did not improve\n",
      "225421/225421 [==============================] - 10s 45us/step - loss: 0.2678 - mean_squared_error: 0.2678 - val_loss: 0.2580 - val_mean_squared_error: 0.2580\n",
      "Epoch 10/100\n",
      "224512/225421 [============================>.] - ETA: 0s - loss: 0.2664 - mean_squared_error: 0.2664Epoch 00010: val_loss did not improve\n",
      "225421/225421 [==============================] - 10s 45us/step - loss: 0.2663 - mean_squared_error: 0.2663 - val_loss: 0.2590 - val_mean_squared_error: 0.2590\n",
      "Epoch 00010: early stopping\n",
      "RMSLE Keras Regressor, validation set, fold  4 :  0.508904645884\n",
      "\n",
      "Fold  5\n",
      "Train on 225421 samples, validate on 25047 samples\n",
      "Epoch 1/100\n",
      "225280/225421 [============================>.] - ETA: 0s - loss: 0.5106 - mean_squared_error: 0.5106Epoch 00001: val_loss improved from inf to 0.29234, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 14s 62us/step - loss: 0.5105 - mean_squared_error: 0.5105 - val_loss: 0.2923 - val_mean_squared_error: 0.2923\n",
      "Epoch 2/100\n",
      "224256/225421 [============================>.] - ETA: 0s - loss: 0.3051 - mean_squared_error: 0.3051Epoch 00002: val_loss improved from 0.29234 to 0.27787, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 10s 45us/step - loss: 0.3051 - mean_squared_error: 0.3051 - val_loss: 0.2779 - val_mean_squared_error: 0.2779\n",
      "Epoch 3/100\n",
      "225024/225421 [============================>.] - ETA: 0s - loss: 0.2940 - mean_squared_error: 0.2940Epoch 00003: val_loss improved from 0.27787 to 0.27041, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 10s 46us/step - loss: 0.2940 - mean_squared_error: 0.2940 - val_loss: 0.2704 - val_mean_squared_error: 0.2704\n",
      "Epoch 4/100\n",
      "225024/225421 [============================>.] - ETA: 0s - loss: 0.2855 - mean_squared_error: 0.2855Epoch 00004: val_loss did not improve\n",
      "225421/225421 [==============================] - 10s 46us/step - loss: 0.2856 - mean_squared_error: 0.2856 - val_loss: 0.2716 - val_mean_squared_error: 0.2716\n",
      "Epoch 5/100\n",
      "225280/225421 [============================>.] - ETA: 0s - loss: 0.2792 - mean_squared_error: 0.2792Epoch 00005: val_loss improved from 0.27041 to 0.26842, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 10s 46us/step - loss: 0.2792 - mean_squared_error: 0.2792 - val_loss: 0.2684 - val_mean_squared_error: 0.2684\n",
      "Epoch 6/100\n",
      "225280/225421 [============================>.] - ETA: 0s - loss: 0.2741 - mean_squared_error: 0.2741Epoch 00006: val_loss improved from 0.26842 to 0.26480, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 11s 47us/step - loss: 0.2741 - mean_squared_error: 0.2741 - val_loss: 0.2648 - val_mean_squared_error: 0.2648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n",
      "224768/225421 [============================>.] - ETA: 0s - loss: 0.2714 - mean_squared_error: 0.2714Epoch 00007: val_loss improved from 0.26480 to 0.26186, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 10s 46us/step - loss: 0.2714 - mean_squared_error: 0.2714 - val_loss: 0.2619 - val_mean_squared_error: 0.2619\n",
      "Epoch 8/100\n",
      "225024/225421 [============================>.] - ETA: 0s - loss: 0.2693 - mean_squared_error: 0.2693Epoch 00008: val_loss did not improve\n",
      "225421/225421 [==============================] - 10s 46us/step - loss: 0.2693 - mean_squared_error: 0.2693 - val_loss: 0.2656 - val_mean_squared_error: 0.2656\n",
      "Epoch 9/100\n",
      "224768/225421 [============================>.] - ETA: 0s - loss: 0.2675 - mean_squared_error: 0.2675Epoch 00009: val_loss did not improve\n",
      "225421/225421 [==============================] - 10s 46us/step - loss: 0.2676 - mean_squared_error: 0.2676 - val_loss: 0.2717 - val_mean_squared_error: 0.2717\n",
      "Epoch 00009: early stopping\n",
      "RMSLE Keras Regressor, validation set, fold  5 :  0.52118786753\n",
      "\n",
      "Fold  6\n",
      "Train on 225421 samples, validate on 25047 samples\n",
      "Epoch 1/100\n",
      "224512/225421 [============================>.] - ETA: 0s - loss: 0.5094 - mean_squared_error: 0.5094Epoch 00001: val_loss improved from inf to 0.29394, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 14s 62us/step - loss: 0.5088 - mean_squared_error: 0.5088 - val_loss: 0.2939 - val_mean_squared_error: 0.2939\n",
      "Epoch 2/100\n",
      "225280/225421 [============================>.] - ETA: 0s - loss: 0.3062 - mean_squared_error: 0.3062Epoch 00002: val_loss improved from 0.29394 to 0.27742, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 10s 46us/step - loss: 0.3062 - mean_squared_error: 0.3062 - val_loss: 0.2774 - val_mean_squared_error: 0.2774\n",
      "Epoch 3/100\n",
      "224256/225421 [============================>.] - ETA: 0s - loss: 0.2941 - mean_squared_error: 0.2941Epoch 00003: val_loss improved from 0.27742 to 0.27021, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 10s 46us/step - loss: 0.2941 - mean_squared_error: 0.2941 - val_loss: 0.2702 - val_mean_squared_error: 0.2702\n",
      "Epoch 4/100\n",
      "225024/225421 [============================>.] - ETA: 0s - loss: 0.2861 - mean_squared_error: 0.2861Epoch 00004: val_loss improved from 0.27021 to 0.26707, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 10s 46us/step - loss: 0.2861 - mean_squared_error: 0.2861 - val_loss: 0.2671 - val_mean_squared_error: 0.2671\n",
      "Epoch 5/100\n",
      "225280/225421 [============================>.] - ETA: 0s - loss: 0.2791 - mean_squared_error: 0.2791Epoch 00005: val_loss did not improve\n",
      "225421/225421 [==============================] - 10s 46us/step - loss: 0.2791 - mean_squared_error: 0.2791 - val_loss: 0.2713 - val_mean_squared_error: 0.2713\n",
      "Epoch 6/100\n",
      "225024/225421 [============================>.] - ETA: 0s - loss: 0.2746 - mean_squared_error: 0.2746Epoch 00006: val_loss did not improve\n",
      "225421/225421 [==============================] - 11s 47us/step - loss: 0.2747 - mean_squared_error: 0.2747 - val_loss: 0.2702 - val_mean_squared_error: 0.2702\n",
      "Epoch 00006: early stopping\n",
      "RMSLE Keras Regressor, validation set, fold  6 :  0.51975082507\n",
      "\n",
      "Fold  7\n",
      "Train on 225421 samples, validate on 25047 samples\n",
      "Epoch 1/100\n",
      "225024/225421 [============================>.] - ETA: 0s - loss: 0.5168 - mean_squared_error: 0.5168Epoch 00001: val_loss improved from inf to 0.28038, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 14s 64us/step - loss: 0.5164 - mean_squared_error: 0.5164 - val_loss: 0.2804 - val_mean_squared_error: 0.2804\n",
      "Epoch 2/100\n",
      "225024/225421 [============================>.] - ETA: 0s - loss: 0.3066 - mean_squared_error: 0.3066Epoch 00002: val_loss improved from 0.28038 to 0.27875, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 10s 45us/step - loss: 0.3067 - mean_squared_error: 0.3067 - val_loss: 0.2787 - val_mean_squared_error: 0.2787\n",
      "Epoch 3/100\n",
      "224256/225421 [============================>.] - ETA: 0s - loss: 0.2951 - mean_squared_error: 0.2951Epoch 00003: val_loss improved from 0.27875 to 0.27262, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 10s 45us/step - loss: 0.2951 - mean_squared_error: 0.2951 - val_loss: 0.2726 - val_mean_squared_error: 0.2726\n",
      "Epoch 4/100\n",
      "225280/225421 [============================>.] - ETA: 0s - loss: 0.2865 - mean_squared_error: 0.2865Epoch 00004: val_loss did not improve\n",
      "225421/225421 [==============================] - 10s 45us/step - loss: 0.2865 - mean_squared_error: 0.2865 - val_loss: 0.2733 - val_mean_squared_error: 0.2733\n",
      "Epoch 5/100\n",
      "225024/225421 [============================>.] - ETA: 0s - loss: 0.2802 - mean_squared_error: 0.2802Epoch 00005: val_loss improved from 0.27262 to 0.26124, saving model to weights.hdf5\n",
      "225421/225421 [==============================] - 10s 46us/step - loss: 0.2802 - mean_squared_error: 0.2802 - val_loss: 0.2612 - val_mean_squared_error: 0.2612\n",
      "Epoch 6/100\n",
      "225024/225421 [============================>.] - ETA: 0s - loss: 0.2755 - mean_squared_error: 0.2755Epoch 00006: val_loss did not improve\n",
      "225421/225421 [==============================] - 10s 45us/step - loss: 0.2755 - mean_squared_error: 0.2755 - val_loss: 0.2644 - val_mean_squared_error: 0.2644\n",
      "Epoch 7/100\n",
      "224512/225421 [============================>.] - ETA: 0s - loss: 0.2721 - mean_squared_error: 0.2721Epoch 00007: val_loss did not improve\n",
      "225421/225421 [==============================] - 10s 45us/step - loss: 0.2721 - mean_squared_error: 0.2721 - val_loss: 0.2624 - val_mean_squared_error: 0.2624\n",
      "Epoch 00007: early stopping\n",
      "RMSLE Keras Regressor, validation set, fold  7 :  0.512232349283\n",
      "\n",
      "Fold  8\n",
      "Train on 225422 samples, validate on 25046 samples\n",
      "Epoch 1/100\n",
      "224768/225422 [============================>.] - ETA: 0s - loss: 0.5176 - mean_squared_error: 0.5176Epoch 00001: val_loss improved from inf to 0.28151, saving model to weights.hdf5\n",
      "225422/225422 [==============================] - 14s 64us/step - loss: 0.5170 - mean_squared_error: 0.5170 - val_loss: 0.2815 - val_mean_squared_error: 0.2815\n",
      "Epoch 2/100\n",
      "224768/225422 [============================>.] - ETA: 0s - loss: 0.3072 - mean_squared_error: 0.3072Epoch 00002: val_loss improved from 0.28151 to 0.27225, saving model to weights.hdf5\n",
      "225422/225422 [==============================] - 10s 46us/step - loss: 0.3071 - mean_squared_error: 0.3071 - val_loss: 0.2722 - val_mean_squared_error: 0.2722\n",
      "Epoch 3/100\n",
      "224768/225422 [============================>.] - ETA: 0s - loss: 0.2954 - mean_squared_error: 0.2954Epoch 00003: val_loss did not improve\n",
      "225422/225422 [==============================] - 10s 46us/step - loss: 0.2953 - mean_squared_error: 0.2953 - val_loss: 0.2788 - val_mean_squared_error: 0.2788\n",
      "Epoch 4/100\n",
      "225280/225422 [============================>.] - ETA: 0s - loss: 0.2855 - mean_squared_error: 0.2855Epoch 00004: val_loss improved from 0.27225 to 0.26750, saving model to weights.hdf5\n",
      "225422/225422 [==============================] - 10s 46us/step - loss: 0.2855 - mean_squared_error: 0.2855 - val_loss: 0.2675 - val_mean_squared_error: 0.2675\n",
      "Epoch 5/100\n",
      "224512/225422 [============================>.] - ETA: 0s - loss: 0.2793 - mean_squared_error: 0.2793Epoch 00005: val_loss did not improve\n",
      "225422/225422 [==============================] - 10s 46us/step - loss: 0.2793 - mean_squared_error: 0.2793 - val_loss: 0.2730 - val_mean_squared_error: 0.2730\n",
      "Epoch 6/100\n",
      "224768/225422 [============================>.] - ETA: 0s - loss: 0.2751 - mean_squared_error: 0.2751Epoch 00006: val_loss improved from 0.26750 to 0.26326, saving model to weights.hdf5\n",
      "225422/225422 [==============================] - 10s 46us/step - loss: 0.2750 - mean_squared_error: 0.2750 - val_loss: 0.2633 - val_mean_squared_error: 0.2633\n",
      "Epoch 7/100\n",
      "224512/225422 [============================>.] - ETA: 0s - loss: 0.2719 - mean_squared_error: 0.2719Epoch 00007: val_loss did not improve\n",
      "225422/225422 [==============================] - 10s 46us/step - loss: 0.2719 - mean_squared_error: 0.2719 - val_loss: 0.2638 - val_mean_squared_error: 0.2638\n",
      "Epoch 8/100\n",
      "224512/225422 [============================>.] - ETA: 0s - loss: 0.2702 - mean_squared_error: 0.2702Epoch 00008: val_loss did not improve\n",
      "225422/225422 [==============================] - 10s 46us/step - loss: 0.2703 - mean_squared_error: 0.2703 - val_loss: 0.2674 - val_mean_squared_error: 0.2674\n",
      "Epoch 00008: early stopping\n",
      "RMSLE Keras Regressor, validation set, fold  8 :  0.517096721609\n",
      "\n",
      "Fold  9\n",
      "Train on 225422 samples, validate on 25046 samples\n",
      "Epoch 1/100\n",
      "225280/225422 [============================>.] - ETA: 0s - loss: 0.5169 - mean_squared_error: 0.5169Epoch 00001: val_loss improved from inf to 0.28394, saving model to weights.hdf5\n",
      "225422/225422 [==============================] - 15s 65us/step - loss: 0.5168 - mean_squared_error: 0.5168 - val_loss: 0.2839 - val_mean_squared_error: 0.2839\n",
      "Epoch 2/100\n",
      "224256/225422 [============================>.] - ETA: 0s - loss: 0.3069 - mean_squared_error: 0.3069Epoch 00002: val_loss improved from 0.28394 to 0.27928, saving model to weights.hdf5\n",
      "225422/225422 [==============================] - 10s 46us/step - loss: 0.3068 - mean_squared_error: 0.3068 - val_loss: 0.2793 - val_mean_squared_error: 0.2793\n",
      "Epoch 3/100\n",
      "224768/225422 [============================>.] - ETA: 0s - loss: 0.2942 - mean_squared_error: 0.2942Epoch 00003: val_loss did not improve\n",
      "225422/225422 [==============================] - 10s 46us/step - loss: 0.2941 - mean_squared_error: 0.2941 - val_loss: 0.2796 - val_mean_squared_error: 0.2796\n",
      "Epoch 4/100\n",
      "225280/225422 [============================>.] - ETA: 0s - loss: 0.2859 - mean_squared_error: 0.2859Epoch 00004: val_loss improved from 0.27928 to 0.27236, saving model to weights.hdf5\n",
      "225422/225422 [==============================] - 11s 47us/step - loss: 0.2859 - mean_squared_error: 0.2859 - val_loss: 0.2724 - val_mean_squared_error: 0.2724\n",
      "Epoch 5/100\n",
      "225280/225422 [============================>.] - ETA: 0s - loss: 0.2792 - mean_squared_error: 0.2792Epoch 00005: val_loss improved from 0.27236 to 0.26477, saving model to weights.hdf5\n",
      "225422/225422 [==============================] - 10s 46us/step - loss: 0.2792 - mean_squared_error: 0.2792 - val_loss: 0.2648 - val_mean_squared_error: 0.2648\n",
      "Epoch 6/100\n",
      "225280/225422 [============================>.] - ETA: 0s - loss: 0.2748 - mean_squared_error: 0.2748Epoch 00006: val_loss did not improve\n",
      "225422/225422 [==============================] - 10s 46us/step - loss: 0.2748 - mean_squared_error: 0.2748 - val_loss: 0.2691 - val_mean_squared_error: 0.2691\n",
      "Epoch 7/100\n",
      "224768/225422 [============================>.] - ETA: 0s - loss: 0.2716 - mean_squared_error: 0.2716Epoch 00007: val_loss did not improve\n",
      "225422/225422 [==============================] - 10s 46us/step - loss: 0.2716 - mean_squared_error: 0.2716 - val_loss: 0.2654 - val_mean_squared_error: 0.2654\n",
      "Epoch 00007: early stopping\n",
      "RMSLE Keras Regressor, validation set, fold  9 :  0.515172922748\n"
     ]
    }
   ],
   "source": [
    "for i, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "    train, val, test2 = prepareData(X.iloc[train_index, :].copy(), X.iloc[val_index, :].copy(), test.copy())\n",
    "    train = train.sort_values(['air_store_id', 'visit_date'])\n",
    "    val = val.sort_values(['air_store_id', 'visit_date'])\n",
    "    test2 = test2.sort_values(['air_store_id', 'visit_date'])\n",
    "    train = train.fillna(0)\n",
    "    val = val.fillna(0)\n",
    "    test2 = test.fillna(0)\n",
    "    \n",
    "    X_train, y_train = train[col], np.log1p(train['visitors'])\n",
    "    X_valid, y_valid = val[col], np.log1p(val['visitors'])\n",
    "    print(\"\\nFold \", i)\n",
    "    \n",
    "    sc = preprocessing.StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    test2 = sc.transform(test2[col])\n",
    "    X_valid = sc.fit_transform(X_valid)\n",
    "    #x_val = np.array(x_valid)\n",
    "    #y_val = np.array(y_valid)\n",
    "    \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units = 160 , kernel_initializer = 'normal', input_dim = X_train.shape[1]))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(.2))\n",
    "    model.add(Dense(units = 100 , kernel_initializer = 'normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(.2))\n",
    "    model.add(Dense(units = 64 , kernel_initializer = 'normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(.1))\n",
    "    model.add(Dense(units = 26, kernel_initializer = 'normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(.1))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(lr=1e-2,decay=1e-4), metrics=['mean_squared_error'])#lr=0.1,decay=1e-4\n",
    "\n",
    "    \n",
    "    wtpath = 'weights.hdf5'\n",
    "    bestepoch = ModelCheckpoint( filepath=wtpath, verbose=1, save_best_only=True )\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=2, verbose=1) \n",
    "    \n",
    "    network_history = (model.fit(X_train, y_train, validation_data = (X_valid, y_valid), epochs=100, \n",
    "          batch_size=256, verbose=True, callbacks=[bestepoch, early_stop])) \n",
    "    \n",
    "    val_pred = model.predict(X_valid)\n",
    "    val_pred = [item for sublist in val_pred for item in sublist]\n",
    "    val_pred = np.array(val_pred)\n",
    "    val_pred[val_pred < 0] = 0\n",
    "    val_pred = np.expm1(val_pred)\n",
    "    val_pred[val_pred < 1] = 1\n",
    "    y_train_pred[val_index] = np.array(val_pred) \n",
    "    print('RMSLE Keras Regressor, validation set, fold ', i, ': ', RMSLE(val['visitors'], val_pred))\n",
    "\n",
    "    test_pred = model.predict(test2)\n",
    "    test_pred = [item for sublist in test_pred for item in sublist]\n",
    "    test_pred = np.array(test_pred)\n",
    "    test_pred[test_pred < 0] = 0\n",
    "    test_pred = np.expm1(test_pred)\n",
    "    test_pred[test_pred < 1] = 1\n",
    "    y_test_pred += test_pred\n",
    "\n",
    "    del X_train, X_valid, y_train, y_valid, train, test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred /= K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSLE Keras, full validtion, fold  0.51428070883\n"
     ]
    }
   ],
   "source": [
    "print('RMSLE Keras, full validtion, fold  ' + str(RMSLE(X['visitors'].values, y_train_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  7.52490377  25.76529884  28.42699623 ...,   3.06510997   3.83794928\n",
      "   3.81537247]\n"
     ]
    }
   ],
   "source": [
    "print(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] = test['air_store_id'] + \"_\" + test['visit_date'].dt.date.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['visitors'] = y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('../submissions/submission11_2.csv', float_format='%.6f', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "valdf = pd.DataFrame()\n",
    "valdf['air_store_id'] = X['air_store_id']\n",
    "valdf['visit_date'] = X['visit_date']\n",
    "valdf['visitors'] = X['visitors']\n",
    "valdf['prediction'] = y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "valdf.to_csv('../submissions/val11_2.csv', float_format='%.6f', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 43.31747055,  17.19405937,  14.3493681 , ...,   3.91078472,\n",
       "         4.99315643,   6.24098969])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
